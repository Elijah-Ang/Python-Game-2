{
  "1": {
    "id": 1,
    "title": "What is a Variable?",
    "content": "# üì¶ What is a Variable?\n\n## Definition\nA **variable** is a named container that stores data in your computer's memory. Think of it like a labeled box where you can put things and retrieve them later.\n\n## Why Do We Need Variables?\n\nImagine you're calculating someone's age. Without variables, you'd have to remember the number (like 25) and type it every time. With a variable, you give it a name (like `age`) and the computer remembers it for you!\n\n## Real-World Analogy\n\nThink of a variable like a **sticky note on a box**:\n- The **box** holds something (a piece of data)\n- The **sticky note** has a name written on it (the variable name)\n- When you need what's inside, you just look for the note!\n\n![Variable Memory](/assets/python-diagrams/variable_box_age.png)\n\n## How to Create a Variable\n\nIn Python, creating a variable is simple:\n\n```python\n# variable_name = value\nage = 25\nname = \"Alice\"\nprice = 19.99\n```\n\nThe `=` sign means \"store this value in this variable name.\"\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Variable** | A named container for data |\n| **Value** | The actual data stored (like 25 or \"Alice\") |\n| **Assignment** | The act of storing a value in a variable using `=` |\n\n---\n\n## üéØ Your Task\n\nCreate a variable called `student` and set it to `\"Alice\"`. Then print it!\n",
    "starter_code": "# Create a variable called student with value \"Alice\"\n\n\n# Print the variable\n",
    "solution_code": "# Create a variable called student with value \"Alice\"\nstudent = \"Alice\"\n\n# Print the variable\nprint(student)",
    "expected_output": "Alice",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "2": {
    "id": 2,
    "title": "Naming Variables",
    "content": "# üè∑Ô∏è How to Name Variables\n\n## Why Does Naming Matter?\n\nGood variable names make your code **readable**. Compare:\n\n```python\n# Bad - What is x? What is y?\nx = 25\ny = 75000\n\n# Good - Clear what each variable represents!\nage = 25\nsalary = 75000\n```\n\nWhen you (or someone else) read the code later, good names save time and prevent confusion.\n\n## Python's Naming Rules\n\nThese are rules you MUST follow or Python will give an error:\n\n| Rule | ‚úÖ Valid | ‚ùå Invalid |\n| --- | --- | --- |\n| Can start with letter or _ | `name`, `_count` | ‚Äî |\n| Cannot start with number | ‚Äî | `2name` |\n| No spaces allowed | `my_name` | `my name` |\n| Only letters, numbers, _ | `user_1` | `user-name` |\n| Case sensitive | `Age` ‚â† `age` | ‚Äî |\n\n## Naming Conventions (Best Practices)\n\nThese aren't required, but make your code professional:\n\n```python\n# ‚úÖ Use snake_case (lowercase with underscores)\nuser_name = \"Alice\"\ntotal_price = 99.99\n\n# ‚ùå Avoid starting with uppercase (reserved for classes)\nUserName = \"Alice\"  # Works but not conventional\n\n# ‚úÖ Be descriptive\ncustomer_email = \"alice@email.com\"\n\n# ‚ùå Avoid single letters (except for loops)\ne = \"alice@email.com\"  # What is 'e'?\n```\n\n---\n\n## üéØ Your Task\n\nCreate these two properly named variables:\n- `first_name` = `\"John\"`\n- `last_name` = `\"Doe\"`\n\nThen print both.\n",
    "starter_code": "# Create first_name variable\n\n\n# Create last_name variable\n\n\n# Print both\n",
    "solution_code": "# Create first_name variable\nfirst_name = \"John\"\n\n# Create last_name variable\nlast_name = \"Doe\"\n\n# Print both\nprint(first_name)\nprint(last_name)",
    "expected_output": "John\nDoe",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "3": {
    "id": 3,
    "title": "Reassigning Variables",
    "content": "# üîÑ Changing Variable Values\n\n## What is Reassignment?\n\nVariables can change! You can update a variable by assigning a new value:\n\n```python\nscore = 0          # Start at 0\nprint(score)       # Output: 0\n\nscore = 100        # Now it's 100!\nprint(score)       # Output: 100\n```\n\n## Why Does This Matter?\n\nIn real programs, values change constantly:\n- A player's score goes up\n- A shopping cart total increases\n- A countdown timer decreases\n\n## How It Works in Memory\n\nWhen you reassign:\n1. Python finds the variable name\n2. Throws away the old value\n3. Stores the new value\n\n```python\ntemperature = 72   # Box now holds 72\ntemperature = 85   # Box now holds 85 (72 is gone!)\n```\n\n## Using the Current Value\n\nYou can use a variable's current value to calculate a new one:\n\n```python\ncount = 5\ncount = count + 1  # Take current (5), add 1, store result (6)\nprint(count)       # Output: 6\n```\n\nShorthand version:\n```python\ncount += 1  # Same as: count = count + 1\n```\n\n---\n\n## üéØ Your Task\n\n1. Start with `points = 0`\n2. Add 10 to points using `+=`\n3. Add 5 more to points using `+=`\n4. Print the final value (should be 15)\n",
    "starter_code": "# Start with 0 points\npoints = 0\n\n# Add 10 points\n\n\n# Add 5 more points\n\n\n# Print final value\n",
    "solution_code": "# Start with 0 points\npoints = 0\n\n# Add 10 points\npoints += 10\n\n# Add 5 more points\npoints += 5\n\n# Print final value\nprint(points)",
    "expected_output": "15",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "4": {
    "id": 4,
    "title": "Multiple Variables",
    "content": "# üìã Working with Multiple Variables\n\n## Assigning Multiple Variables\n\nYou can create several variables at once:\n\n```python\n# One per line (most clear)\nname = \"Alice\"\nage = 25\ncity = \"New York\"\n\n# All on one line (for related values)\nx, y, z = 10, 20, 30\n```\n\n## Swapping Variables\n\nSometimes you need to swap two values. Python makes this easy:\n\n```python\na = 5\nb = 10\n\n# Swap them!\na, b = b, a\n\nprint(a)  # 10\nprint(b)  # 5\n```\n\nIn other languages, you'd need a temporary variable. Python handles it elegantly!\n\n## Using Variables Together\n\nVariables can reference each other:\n\n```python\nprice = 20\nquantity = 3\ntotal = price * quantity  # total is now 60\n\nprint(f\"Total: ${total}\")\n```\n\n---\n\n## üéØ Your Task\n\nCreate variables for a product's name, price, and quantity:\n- `product` = `\"Laptop\"`\n- `price` = `999`\n- `quantity` = `2`\n\nCalculate `total` as price √ó quantity, then print:\n```\nProduct: Laptop\nTotal: 1998\n```\n",
    "starter_code": "# Product information\nproduct = \"Laptop\"\nprice = 999\nquantity = 2\n\n# Calculate total\n\n\n# Print product and total\n",
    "solution_code": "# Product information\nproduct = \"Laptop\"\nprice = 999\nquantity = 2\n\n# Calculate total\ntotal = price * quantity\n\n# Print product and total\nprint(f\"Product: {product}\")\nprint(f\"Total: {total}\")",
    "expected_output": "Product: Laptop\nTotal: 1998",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "5": {
    "id": 5,
    "title": "What are Strings?",
    "content": "# üìù What is a String?\n\n## Definition\n\nA **string** is a sequence of characters (letters, numbers, symbols, spaces) surrounded by quotes. It's how we represent text in programming.\n\n![Variable Memory](/assets/python-diagrams/variable_box_string.png)\n\n```python\nmessage = \"Hello, World!\"\nname = 'Alice'  # Single or double quotes both work\n```\n\n## Why Do We Need Strings?\n\nNearly every program works with text:\n- User names, emails, addresses\n- Messages and notifications\n- File paths and URLs\n- Any data that isn't purely numeric\n\n## Types of Quotes\n\nPython accepts three types:\n\n```python\n# Single quotes\ngreeting = 'Hello'\n\n# Double quotes (same as single)\ngreeting = \"Hello\"\n\n# Triple quotes (for multi-line text)\npoem = \"\"\"Roses are red,\nViolets are blue,\nPython is awesome,\nAnd so are you!\"\"\"\n```\n\n## When to Use Which?\n\n```python\n# Use double quotes if string contains single quote\nsentence = \"It's a beautiful day\"\n\n# Use single quotes if string contains double quote\nhtml = '<div class=\"container\">'\n```\n\n---\n\n## üéØ Your Task\n\nCreate these strings:\n- `greeting` = `\"Hello\"`\n- `name` = `\"Python\"`\n\nPrint: `Hello, Python!`\n",
    "starter_code": "# Create greeting\ngreeting = \"Hello\"\n\n# Create name\nname = \"Python\"\n\n# Print greeting, name!\n",
    "solution_code": "# Create greeting\ngreeting = \"Hello\"\n\n# Create name\nname = \"Python\"\n\n# Print greeting, name!\nprint(greeting + \", \" + name + \"!\")",
    "expected_output": "Hello, Python!",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "6": {
    "id": 6,
    "title": "String Concatenation",
    "content": "# üîó Joining Strings Together\n\n## What is Concatenation?\n\n**Concatenation** means joining strings end-to-end. Use the `+` operator:\n\n```python\nfirst = \"Hello\"\nsecond = \"World\"\ncombined = first + second\nprint(combined)  # HelloWorld\n```\n\n## Adding Spaces\n\nNotice there's no automatic space! You must add it:\n\n```python\ncombined = first + \" \" + second\nprint(combined)  # Hello World\n```\n\n## Why Use Concatenation?\n\nBuilding messages with dynamic data:\n\n```python\nname = \"Alice\"\nmessage = \"Welcome, \" + name + \"!\"\nprint(message)  # Welcome, Alice!\n```\n\n## Repeating Strings\n\nUse `*` to repeat a string:\n\n```python\nline = \"-\" * 20\nprint(line)  # --------------------\n\ncheer = \"Hip \" * 2 + \"Hooray!\"\nprint(cheer)  # Hip Hip Hooray!\n```\n\n---\n\n## üéØ Your Task\n\nGiven:\n- `first_name` = `\"Jane\"`\n- `last_name` = `\"Smith\"`\n\nCreate `full_name` by joining them with a space.\nThen print: `Welcome, Jane Smith!`\n",
    "starter_code": "# Given names\nfirst_name = \"Jane\"\nlast_name = \"Smith\"\n\n# Join them into full_name\n\n\n# Print welcome message\n",
    "solution_code": "# Given names\nfirst_name = \"Jane\"\nlast_name = \"Smith\"\n\n# Join them into full_name\nfull_name = first_name + \" \" + last_name\n\n# Print welcome message\nprint(\"Welcome, \" + full_name + \"!\")",
    "expected_output": "Welcome, Jane Smith!",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "7": {
    "id": 7,
    "title": "F-Strings (Formatted Strings)",
    "content": "# ‚ú® F-Strings: The Modern Way\n\n## What are F-Strings?\n\nF-strings (formatted string literals) let you embed variables directly in text. Just add `f` before the quote and put variables in `{}`:\n\n```python\nname = \"Alice\"\nage = 25\nmessage = f\"My name is {name} and I'm {age} years old.\"\nprint(message)\n# Output: My name is Alice and I'm 25 years old.\n```\n\n## Why F-Strings are Better\n\nCompare concatenation vs f-strings:\n\n```python\n# Old way (messy)\nmessage = \"Hello, \" + name + \"! You have \" + str(score) + \" points.\"\n\n# F-string way (clean!)\nmessage = f\"Hello, {name}! You have {score} points.\"\n```\n\nF-strings are:\n- Easier to read\n- Less error-prone\n- No need to convert numbers to strings\n\n## Expressions Inside F-Strings\n\nYou can put any expression in `{}`:\n\n```python\nprice = 19.99\nquantity = 3\nprint(f\"Total: ${price * quantity}\")  # Total: $59.97\n\n# Formatting numbers\nprint(f\"Price: ${price:.2f}\")  # Price: $19.99 (2 decimal places)\n```\n\n---\n\n## üéØ Your Task\n\nGiven:\n- `item` = `\"Coffee\"`\n- `price` = `4.50`\n\nUse an f-string to print: `Coffee costs $4.5`\n",
    "starter_code": "# Given data\nitem = \"Coffee\"\nprice = 4.50\n\n# Print using f-string\n",
    "solution_code": "# Given data\nitem = \"Coffee\"\nprice = 4.50\n\n# Print using f-string\nprint(f\"{item} costs ${price}\")",
    "expected_output": "Coffee costs $4.5",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "8": {
    "id": 8,
    "title": "String Methods",
    "content": "# üõ†Ô∏è String Methods\n\n## What are Methods?\n\nMethods are **actions** you can perform on strings. Use dot notation: `string.method()`\n\n## Common String Methods\n\n```python\ntext = \"Hello World\"\n\ntext.upper()      # \"HELLO WORLD\"\ntext.lower()      # \"hello world\"\ntext.title()      # \"Hello World\"\ntext.strip()      # Removes whitespace from ends\ntext.replace(\"Hello\", \"Hi\")  # \"Hi World\"\ntext.split(\" \")   # [\"Hello\", \"World\"]\nlen(text)         # 11 (length - not a method, but useful!)\n```\n\n## Why These Are Useful\n\n| Method | Use Case |\n| --- | --- |\n| `.upper()/.lower()` | Case-insensitive comparison |\n| `.strip()` | Clean user input |\n| `.replace()` | Find and replace text |\n| `.split()` | Break text into parts |\n\n## Example: Cleaning User Input\n\n```python\nuser_input = \"  Alice  \"\nclean_name = user_input.strip().title()\nprint(clean_name)  # \"Alice\"\n```\n\n## Chaining Methods\n\nMethods return new strings, so you can chain them:\n\n```python\nmessy = \"   hELLo wORLD   \"\nclean = messy.strip().lower().title()\nprint(clean)  # \"Hello World\"\n```\n\n---\n\n## üéØ Your Task\n\nGiven: `messy_email = \"  JOHN@EMAIL.COM  \"`\n\nClean it up:\n1. Remove extra spaces with `.strip()`\n2. Convert to lowercase with `.lower()`\n3. Print the result: `john@email.com`\n",
    "starter_code": "# Messy email\nmessy_email = \"  JOHN@EMAIL.COM  \"\n\n# Clean it: strip and lowercase\n\n\n# Print cleaned email\n",
    "solution_code": "# Messy email\nmessy_email = \"  JOHN@EMAIL.COM  \"\n\n# Clean it: strip and lowercase\nclean_email = messy_email.strip().lower()\n\n# Print cleaned email\nprint(clean_email)",
    "expected_output": "john@email.com",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "9": {
    "id": 9,
    "title": "Numbers: Integers and Floats",
    "content": "# üî¢ Numbers in Python\n\n## Two Main Types of Numbers\n\nPython has two types of numbers:\n\n| Type | Definition | Examples |\n| --- | --- | --- |\n| **Integer (int)** | Whole numbers, no decimal | `10`, `-5`, `0`, `1000` |\n| **Float** | Numbers with decimals | `3.14`, `-2.5`, `0.0` |\n\n## Why Two Types?\n\n- **Integers** are precise and faster (good for counting)\n- **Floats** are needed for measurements, science, money\n\n```python\ncount = 42       # Integer - exact count\nprice = 19.99    # Float - needs decimals\ntemperature = 98.6\n```\n\n## Python Auto-Detects the Type\n\n```python\nwhole = 10      # Python sees int\ndecimal = 10.0  # Python sees float\n\n# Check the type\nprint(type(whole))    # <class 'int'>\nprint(type(decimal))  # <class 'float'>\n```\n\n## Converting Between Types\n\n```python\nx = 10\ny = float(x)    # 10.0 (now a float)\n\nz = 10.7\nw = int(z)      # 10 (decimals cut off, not rounded!)\n```\n\n---\n\n## üéØ Your Task\n\nCreate:\n- `quantity` = `5` (integer)\n- `unit_price` = `12.50` (float)\n- `total` = quantity √ó unit_price\n\nPrint: `Total: 62.5`\n",
    "starter_code": "# Create quantity (integer)\nquantity = 5\n\n# Create unit_price (float)\nunit_price = 12.50\n\n# Calculate total\n\n\n# Print total\n",
    "solution_code": "# Create quantity (integer)\nquantity = 5\n\n# Create unit_price (float)\nunit_price = 12.50\n\n# Calculate total\ntotal = quantity * unit_price\n\n# Print total\nprint(f\"Total: {total}\")",
    "expected_output": "Total: 62.5",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "10": {
    "id": 10,
    "title": "Math Operations",
    "content": "# ‚ûï Math in Python\n\n## Basic Operations\n\nPython supports all standard math:\n\n| Operator | Name | Example | Result |\n| --- | --- | --- | --- |\n| `+` | Addition | `5 + 3` | `8` |\n| `-` | Subtraction | `5 - 3` | `2` |\n| `*` | Multiplication | `5 * 3` | `15` |\n| `/` | Division | `5 / 2` | `2.5` |\n| `**` | Power | `5 ** 2` | `25` |\n| `//` | Floor Division | `5 // 2` | `2` |\n| `%` | Modulo (remainder) | `5 % 2` | `1` |\n\n## Understanding Floor Division and Modulo\n\n- `//` gives the **whole number** part of division\n- `%` gives the **remainder**\n\n```python\n17 // 5  # 3 (17 goes into 5 three times)\n17 % 5   # 2 (with 2 left over)\n```\n\n## Order of Operations (PEMDAS)\n\nPython follows standard math order:\n1. **P**arentheses `()`\n2. **E**xponents `**`\n3. **M**ultiplication & **D**ivision `* / // %`\n4. **A**ddition & **S**ubtraction `+ -`\n\n```python\nresult = 2 + 3 * 4      # 14 (not 20!)\nresult = (2 + 3) * 4    # 20 (parentheses first)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate a restaurant tip:\n- `bill` = `80.00`\n- `tip_percent` = `20`\n- Calculate `tip` as 20% of bill\n- Calculate `total` as bill + tip\n\nPrint: `Tip: 16.0` and `Total: 96.0`\n",
    "starter_code": "# Bill amount\nbill = 80.00\ntip_percent = 20\n\n# Calculate tip (20% of bill)\n\n\n# Calculate total\n\n\n# Print tip and total\n",
    "solution_code": "# Bill amount\nbill = 80.00\ntip_percent = 20\n\n# Calculate tip (20% of bill)\ntip = bill * (tip_percent / 100)\n\n# Calculate total\ntotal = bill + tip\n\n# Print tip and total\nprint(f\"Tip: {tip}\")\nprint(f\"Total: {total}\")",
    "expected_output": "Tip: 16.0\nTotal: 96.0",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "11": {
    "id": 11,
    "title": "Compound Assignment",
    "content": "# üìù Shorthand Math Operations\n\n## What is Compound Assignment?\n\nInstead of writing `x = x + 5`, Python has shorter versions:\n\n| Long Form | Shorthand | Meaning |\n| --- | --- | --- |\n| `x = x + 5` | `x += 5` | Add 5 to x |\n| `x = x - 5` | `x -= 5` | Subtract 5 from x |\n| `x = x * 5` | `x *= 5` | Multiply x by 5 |\n| `x = x / 5` | `x /= 5` | Divide x by 5 |\n| `x = x ** 2` | `x **= 2` | Square x |\n\n## Why Use Shorthand?\n\n1. Less typing\n2. Clearer intent (you're modifying, not replacing)\n3. Industry standard practice\n\n```python\nscore = 0\nscore += 10   # Player earns 10 points\nscore += 25   # Player earns 25 more\nscore -= 5    # Player loses 5 points\nprint(score)  # 30\n```\n\n## Common Use: Counters and Accumulators\n\n```python\n# Counting\ncount = 0\ncount += 1\ncount += 1\ncount += 1\nprint(count)  # 3\n\n# Running total\ntotal = 0\ntotal += 100\ntotal += 50\ntotal += 25\nprint(total)  # 175\n```\n\n---\n\n## üéØ Your Task\n\nSimulate a game:\n1. Start `health = 100`\n2. Take 25 damage (use `-=`)\n3. Heal 10 health (use `+=`)\n4. Print final health (should be 85)\n",
    "starter_code": "# Start with full health\nhealth = 100\n\n# Take 25 damage\n\n\n# Heal 10 health\n\n\n# Print final health\n",
    "solution_code": "# Start with full health\nhealth = 100\n\n# Take 25 damage\nhealth -= 25\n\n# Heal 10 health\nhealth += 10\n\n# Print final health\nprint(health)",
    "expected_output": "85",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "12": {
    "id": 12,
    "title": "Booleans and Type Conversion",
    "content": "# ‚≠ï Booleans: True or False\n\n## What is a Boolean?\n\nA **Boolean** (named after mathematician George Boole) can only be one of two values:\n- `True`\n- `False`\n\n```python\nis_sunny = True\nis_raining = False\n```\n\n## Why Are Booleans Important?\n\nBooleans are the foundation of **decision making** in code:\n\n```python\nis_adult = age >= 18\nis_logged_in = True\nhas_permission = user_role == \"admin\"\n```\n\nWe'll use these extensively in the Logic chapter!\n\n## Type Conversion\n\nYou can convert between types:\n\n```python\n# To integer\nint(\"42\")       # 42\nint(3.7)        # 3 (truncates, doesn't round!)\n\n# To float\nfloat(\"3.14\")   # 3.14\nfloat(5)        # 5.0\n\n# To string\nstr(42)         # \"42\"\nstr(3.14)       # \"3.14\"\n\n# To boolean\nbool(1)         # True\nbool(0)         # False\nbool(\"\")        # False (empty string)\nbool(\"hello\")   # True (non-empty string)\n```\n\n## Common Use: User Input\n\n```python\n# input() always returns a string!\nage_text = \"25\"\nage = int(age_text)  # Convert to number for math\n```\n\n---\n\n## üéØ Your Task\n\nGiven: `age_text = \"25\"`\n\n1. Convert it to an integer using `int()`\n2. Add 5 to get age in 5 years\n3. Print the future age (should be 30)\n",
    "starter_code": "# Age as a string\nage_text = \"25\"\n\n# Convert to integer\n\n\n# Add 5 years\n\n\n# Print future age\n",
    "solution_code": "# Age as a string\nage_text = \"25\"\n\n# Convert to integer\nage = int(age_text)\n\n# Add 5 years\nfuture_age = age + 5\n\n# Print future age\nprint(future_age)",
    "expected_output": "30",
    "chapter_id": 1,
    "chapter_title": "Variables, Types & Memory"
  },
  "13": {
    "id": 13,
    "title": "For Loop Basics",
    "content": "# üîÅ For Loops: Repeat Actions\n\n## What is a Loop?\n\nA **loop** lets you repeat code multiple times without writing it over and over. It's one of the most powerful tools in programming!\n\n## Real-World Analogy\n\nImagine you need to greet 100 guests at a party. Instead of saying \"Hello\" 100 times manually, you'd use a loop: \"For each guest, say Hello.\"\n\n## The For Loop\n\nA `for` loop repeats code for each item in a sequence:\n\n```python\n# For each item in the sequence...\nfor item in sequence:\n    # Do something with item\n    print(item)\n```\n\n## Example: Looping Through a List\n\n```python\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n```\n\nOutput:\n```\napple\nbanana\ncherry\n```\n\n## How It Works Step by Step\n\n1. Python takes the first item (\"apple\") and stores it in `fruit`\n2. Runs the indented code (prints \"apple\")\n3. Takes the next item (\"banana\") and stores it in `fruit`\n4. Runs the indented code (prints \"banana\")\n5. Continues until no items left\n\n---\n\n## üéØ Your Task\n\nGiven this list:\n```python\ncolors = [\"red\", \"green\", \"blue\"]\n```\n\nUse a for loop to print each color on a new line.\n",
    "starter_code": "colors = [\"red\", \"green\", \"blue\"]\n\n# Print each color\n",
    "solution_code": "colors = [\"red\", \"green\", \"blue\"]\n\n# Print each color\nfor color in colors:\n    print(color)",
    "expected_output": "red\ngreen\nblue",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "14": {
    "id": 14,
    "title": "Looping Through Strings",
    "content": "# üìù Looping Through Strings\n\n## Strings Are Sequences Too!\n\nA string is actually a sequence of characters. You can loop through it just like a list:\n\n```python\nword = \"Hello\"\nfor letter in word:\n    print(letter)\n```\n\nOutput:\n```\nH\ne\nl\nl\no\n```\n\n## Why Loop Through Strings?\n\nCommon use cases:\n- Counting specific characters\n- Checking each character for validity\n- Transforming characters one by one\n- Finding patterns\n\n## Example: Count Vowels\n\n```python\ntext = \"hello world\"\nvowel_count = 0\nfor char in text:\n    if char in \"aeiou\":\n        vowel_count += 1\nprint(f\"Vowels: {vowel_count}\")  # Vowels: 3\n```\n\n## Understanding Characters\n\nEach loop iteration gives you ONE character:\n\n```python\nfor char in \"ABC\":\n    print(f\"Character: '{char}'\")\n# Output:\n# Character: 'A'\n# Character: 'B'\n# Character: 'C'\n```\n\n---\n\n## üéØ Your Task\n\nLoop through the word `\"Python\"` and print each letter on a separate line.\n",
    "starter_code": "word = \"Python\"\n\n# Print each letter\n",
    "solution_code": "word = \"Python\"\n\n# Print each letter\nfor letter in word:\n    print(letter)",
    "expected_output": "P\ny\nt\nh\no\nn",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "15": {
    "id": 15,
    "title": "Accumulating Values",
    "chapter_title": "Loops (Iteration)",
    "content": "# ‚ûï Accumulating Values in Loops\n\n## The Accumulator Pattern\n\nOne of the most common loop patterns: start with a value, then update it each iteration.\n\n```python\n# Start with an initial value\ntotal = 0\n\n# Loop and accumulate (sum 0 to 4)\nfor num in range(5):\n    total += num\n\nprint(total)  # 10\n```\n\n## How It Works Step by Step\n\n| Iteration | `num` | `total` before | Action | `total` after |\n| --- | --- | --- | --- | --- |\n| 1 | 0 | 0 | 0 + 0 | 0 |\n| 2 | 1 | 0 | 0 + 1 | 1 |\n| 3 | 2 | 1 | 1 + 2 | 3 |\n| 4 | 3 | 3 | 3 + 3 | 6 |\n| 5 | 4 | 6 | 6 + 4 | 10 |\n\n## Other Accumulator Examples\n\n```python\n# Product (Factorial of 5)\nproduct = 1\nfor num in range(1, 6):\n    product *= num\nprint(product)  # 120 (1*2*3*4*5)\n\n# String building\nsentence = \"\"\nfor char in \"Hello\":\n    sentence += char + \"-\"\nprint(sentence)  # \"H-e-l-l-o-\"\n```\n\n---\n\n## üéØ Your Task\n\nCalculate the sum of numbers from 10 to 40 (inclusive), stepping by 10 (10, 20, 30, 40).",
    "starter_code": "total = 0\n\n# Loop through 10, 20, 30, 40\n# HINT: range(start, stop, step)\n\n\n# Print the sum\n",
    "solution_code": "total = 0\nfor num in range(10, 41, 10):\n    total += num\nprint(total)",
    "expected_output": "100",
    "chapter_id": 2
  },
  "16": {
    "id": 16,
    "title": "Using range()",
    "content": "# üìä The range() Function\n\n## What is range()?\n\n`range()` generates a sequence of numbers. It's perfect when you need to repeat something a specific number of times.\n\n```python\nfor i in range(5):\n    print(i)\n# Output: 0, 1, 2, 3, 4\n```\n\n## Why Start at 0?\n\nPython (like most languages) uses **zero-based indexing**. This means counting starts at 0, not 1.\n\n```python\nrange(5)  # Generates: 0, 1, 2, 3, 4 (that's 5 numbers!)\n```\n\n## Common Pattern: Repeat N Times\n\nIf you just want to repeat something:\n\n```python\nfor i in range(3):\n    print(\"Hello!\")\n# Output: Hello! Hello! Hello!\n```\n\n## range() Returns a Special Object\n\nNote: `range()` doesn't create a list immediately (to save memory). But it works in for loops!\n\n```python\nprint(range(5))       # range(0, 5) - the object\nprint(list(range(5))) # [0, 1, 2, 3, 4] - converted to list\n```\n\n---\n\n## üéØ Your Task\n\nUse `range(5)` to print numbers 0 through 4, each on a new line.\n",
    "starter_code": "# Print 0 through 4 using range\n",
    "solution_code": "# Print 0 through 4 using range\nfor i in range(5):\n    print(i)",
    "expected_output": "0\n1\n2\n3\n4",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "17": {
    "title": "range() with Start and End",
    "chapter_title": "Loops",
    "content": "# üî¢ range() with Start and End: Control Your Sequence\n\n## Beyond Just Stop\n\nBy default, `range(n)` starts at 0. But you can specify a starting point:\n\n```python\nrange(5)       # 0, 1, 2, 3, 4\nrange(1, 6)    # 1, 2, 3, 4, 5\nrange(10, 15)  # 10, 11, 12, 13, 14\n```\n\n## The Pattern\n\n```python\nrange(start, stop)\n      ‚Üë       ‚Üë\n   Included  Excluded\n```\n\n## Why Start at Different Numbers?\n\n### Human-Readable Counts\n```python\n# Loop 1 to 10 (not 0 to 9)\nfor i in range(1, 11):\n    print(f\"Item {i}\")\n```\n\n### Skip Initial Values\n```python\n# Process items 100-199\nfor i in range(100, 200):\n    process(i)\n```\n\n### Page Numbers\n```python\n# Generate page numbers\npages = list(range(1, 21))  # Pages 1-20\n```\n\n## Common Uses\n\n```python\n# Multiplication table (2 to 10)\nfor i in range(2, 11):\n    print(f\"{i} x 5 = {i*5}\")\n\n# Process specific range\nfor i in range(start_idx, end_idx):\n    items[i] = transform(items[i])\n```\n\n---\n\n## üéØ Your Task\n\nUse range() with start and end to generate specific sequences.",
    "starter_code": "# Range 1 to 10 (human-readable counting)\nprint(\"Counting 1 to 10:\")\nfor i in range(1, 11):\n    print(i, end=\" \")\nprint()\n\n# Range 5 to 10\nprint(\"\\nRange 5 to 10:\", list(range(5, 11)))\n\n# Page numbers\npages = list(range(1, 11))\nprint(f\"\\nPages: {pages}\")\n\n# Multiplication table for 7 (1-10)\nprint(\"\\n7x table:\")\nfor i in range(1, 11):\n    print(f\"  7 x {i:2} = {7*i:2}\")",
    "solution_code": "print(list(range(1, 6)))   # [1, 2, 3, 4, 5]\nprint(list(range(10, 15)))  # [10, 11, 12, 13, 14]",
    "expected_output": "Counting 1 to 10:\n1 2 3 4 5 6 7 8 9 10 \n\nRange 5 to 10: [5, 6, 7, 8, 9, 10]"
  },
  "18": {
    "title": "range() with Step",
    "chapter_title": "Loops",
    "content": "# ‚è≠Ô∏è range() with Step: Skip Numbers\n\n## The Step Parameter\n\n`range(start, stop, step)` lets you skip numbers:\n\n```python\nrange(0, 10, 2)  # 0, 2, 4, 6, 8 (every 2nd)\nrange(0, 10, 3)  # 0, 3, 6, 9 (every 3rd)\n```\n\n## Common Patterns\n\n### Even Numbers\n```python\nevens = list(range(0, 11, 2))  # [0, 2, 4, 6, 8, 10]\n```\n\n### Odd Numbers\n```python\nodds = list(range(1, 11, 2))  # [1, 3, 5, 7, 9]\n```\n\n### Countdown (Negative Step)\n```python\ncountdown = list(range(10, 0, -1))  # [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n```\n\n### Every 5th\n```python\nlist(range(0, 51, 5))  # [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n```\n\n## Practical Uses\n\n```python\n# Process every 10th item\nfor i in range(0, len(data), 10):\n    process(data[i])\n\n# Skip items in reverse\nfor i in range(len(data)-1, -1, -2):\n    print(data[i])\n```\n\n## Step with Enumerate\n\n```python\n# Get index and value every 2 items\nfor i in range(0, len(items), 2):\n    print(f\"Index {i}: {items[i]}\")\n```\n\n---\n\n## üéØ Your Task\n\nUse step parameter to generate various number sequences.",
    "starter_code": "# Even numbers 0-20\nevens = list(range(0, 21, 2))\nprint(f\"Evens (0-20): {evens}\")\n\n# Odd numbers 1-19\nodds = list(range(1, 20, 2))\nprint(f\"Odds (1-19): {odds}\")\n\n# Countdown from 10\ncountdown = list(range(10, 0, -1))\nprint(f\"Countdown: {countdown}\")\n\n# Every 5th from 0 to 50\nfives = list(range(0, 51, 5))\nprint(f\"By 5s: {fives}\")\n\n# Reverse by 2s\nreverse_by_2 = list(range(10, 0, -2))\nprint(f\"Reverse by 2s: {reverse_by_2}\")",
    "solution_code": "print(list(range(0, 11, 2)))  # [0, 2, 4, 6, 8, 10]\nprint(list(range(10, 0, -1)))  # [10, 9, 8, ..., 1]",
    "expected_output": "Evens (0-20): [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\nOdds (1-19): [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
  },
  "19": {
    "id": 19,
    "title": "While Loop Basics",
    "content": "# ‚è≥ While Loops\n\n## For vs While\n\n| Loop Type | Use Case |\n| --- | --- |\n| `for` | When you know how many times to loop |\n| `while` | When you loop until a condition changes |\n\n## While Loop Syntax\n\n```python\nwhile condition:\n    # Do something\n    # Update something (to eventually stop!)\n```\n\n## Example\n\n```python\ncount = 0\nwhile count < 5:\n    print(count)\n    count += 1\n# Output: 0, 1, 2, 3, 4\n```\n\n## ‚ö†Ô∏è Warning: Infinite Loops!\n\nIf the condition never becomes False, the loop runs forever!\n\n```python\n# DANGER! Never stops!\nwhile True:\n    print(\"Forever...\")\n\n# SAFE: Will eventually stop\nx = 0\nwhile x < 10:\n    print(x)\n    x += 1  # This makes x eventually reach 10\n```\n\n## When to Use While\n\n- User input validation (keep asking until valid)\n- Game loops (run until game over)\n- Processing data until a condition is met\n\n---\n\n## üéØ Your Task\n\nStart with `x = 1`. \nUse a while loop to print x and double it (`x *= 2`) while x <= 16.\n",
    "starter_code": "x = 1\n\n# While x <= 16, print x and double it\n",
    "solution_code": "x = 1\n\n# While x <= 16, print x and double it\nwhile x <= 16:\n    print(x)\n    x *= 2",
    "expected_output": "1\n2\n4\n8\n16",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "20": {
    "id": 20,
    "title": "Loop Control: break",
    "content": "# üõë Breaking Out of Loops\n\n## The break Statement\n\n`break` immediately exits the loop, even if there are more items:\n\n```python\nfor i in range(10):\n    if i == 5:\n        break  # Exit NOW!\n    print(i)\n# Output: 0, 1, 2, 3, 4\n```\n\n## Why Use break?\n\n- **Early exit**: Stop when you find what you're looking for\n- **Performance**: Don't process unnecessary items\n- **Error handling**: Exit if something goes wrong\n\n## Example: Find First Match\n\n```python\nnames = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"]\ntarget = \"Charlie\"\n\nfor name in names:\n    if name == target:\n        print(f\"Found {target}!\")\n        break\n    print(f\"Checking {name}...\")\n\n# Output:\n# Checking Alice...\n# Checking Bob...\n# Found Charlie!\n```\n\nNotice: We never check Diana because we broke out early!\n\n---\n\n## üéØ Your Task\n\nLoop through `[1, 2, 3, 4, 5, 6, 7]`.\nPrint each number, but use `break` to exit when you reach 5.\n",
    "starter_code": "numbers = [1, 2, 3, 4, 5, 6, 7]\n\n# Print each, break at 5\n",
    "solution_code": "numbers = [1, 2, 3, 4, 5, 6, 7]\n\n# Print each, break at 5\nfor num in numbers:\n    if num == 5:\n        break\n    print(num)",
    "expected_output": "1\n2\n3\n4",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "21": {
    "id": 21,
    "title": "Loop Control: continue",
    "content": "# ‚è≠Ô∏è Skipping with continue\n\n## The continue Statement\n\n`continue` skips the rest of the current iteration and moves to the next one:\n\n```python\nfor i in range(5):\n    if i == 2:\n        continue  # Skip 2\n    print(i)\n# Output: 0, 1, 3, 4\n```\n\n## break vs continue\n\n| Statement | What It Does |\n| --- | --- |\n| `break` | **EXIT** the entire loop |\n| `continue` | **SKIP** to next iteration |\n\n## Example: Skip Negative Numbers\n\n```python\nnumbers = [1, -2, 3, -4, 5]\nfor num in numbers:\n    if num < 0:\n        continue  # Skip negatives\n    print(num)\n# Output: 1, 3, 5\n```\n\n## When to Use continue\n\n- Skip invalid data\n- Filter out unwanted items\n- Process only items that meet criteria\n\n```python\n# Process only adults\nfor person in people:\n    if person.age < 18:\n        continue\n    # ... process adult ...\n```\n\n---\n\n## üéØ Your Task\n\nPrint numbers 1-5, but skip 3 using `continue`.\n",
    "starter_code": "# Print 1-5, skip 3\n",
    "solution_code": "# Print 1-5, skip 3\nfor i in range(1, 6):\n    if i == 3:\n        continue\n    print(i)",
    "expected_output": "1\n2\n4\n5",
    "chapter_id": 2,
    "chapter_title": "Loops (Iteration)"
  },
  "22": {
    "id": 22,
    "title": "If Statements",
    "content": "# üéØ If Statements: Making Decisions\n\n## What is an If Statement?\n\nAn `if` statement lets your code make decisions. It runs code ONLY when a condition is True.\n\n## Real-World Analogy\n\nThink of a bouncer at a club:\n- **IF** you're 21 or older ‚Üí you can enter\n- Otherwise ‚Üí you're turned away\n\nIn Python:\n```python\nage = 25\nif age >= 21:\n    print(\"Welcome!\")\n```\n\n## The Basic Syntax\n\n```python\nif condition:\n    # This code runs if condition is True\n    # Notice the indentation!\n```\n\n![If Flowchart](/assets/python-diagrams/flowchart_if.png)\n\n**Important**: The colon `:` and indentation are required!\n\n## How Python Evaluates Conditions\n\nPython checks if the condition is `True` or `False`:\n\n```python\nage = 20\nif age >= 18:  # 20 >= 18 is True\n    print(\"You're an adult!\")  # This runs!\n\nif age >= 21:  # 20 >= 21 is False\n    print(\"You can drink!\")  # This does NOT run\n```\n\n---\n\n## üéØ Your Task\n\nGiven `score = 85`:\n- If score >= 70, print `\"Pass\"`\n",
    "starter_code": "score = 85\n\n# If score >= 70, print Pass\n",
    "solution_code": "score = 85\n\n# If score >= 70, print Pass\nif score >= 70:\n    print(\"Pass\")",
    "expected_output": "Pass",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "23": {
    "id": 23,
    "title": "If-Else",
    "content": "# ‚öñÔ∏è If-Else: Two Paths\n\n## Adding an Alternative\n\nWhat if you want to do something when the condition is False? Use `else`:\n\n```python\nif condition:\n    # Runs if True\nelse:\n    # Runs if False\n```\n\n## Example\n\n```python\nage = 15\nif age >= 18:\n    print(\"You can vote!\")\nelse:\n    print(\"Too young to vote\")\n# Output: Too young to vote\n```\n\n## Only ONE Path Runs\n\nWith if-else, exactly ONE block runs - never both, never neither:\n\n```python\ntemperature = 75\nif temperature > 80:\n    print(\"Hot!\")\nelse:\n    print(\"Nice weather!\")\n# Output: Nice weather! (only this one runs)\n```\n\n## Common Mistake\n\nDon't use two separate `if` statements when you want if-else:\n\n```python\n# WRONG - both might run!\nif x > 0:\n    print(\"Positive\")\nif x <= 0:\n    print(\"Non-positive\")\n\n# RIGHT - only one runs\nif x > 0:\n    print(\"Positive\")\nelse:\n    print(\"Non-positive\")\n```\n\n---\n\n## üéØ Your Task\n\nGiven `temperature = 35`:\n- If temperature > 30, print `\"Hot\"`\n- Else print `\"Nice\"`\n",
    "starter_code": "temperature = 35\n\n# Check if hot or nice\n",
    "solution_code": "temperature = 35\n\n# Check if hot or nice\nif temperature > 30:\n    print(\"Hot\")\nelse:\n    print(\"Nice\")",
    "expected_output": "Hot",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "24": {
    "id": 24,
    "title": "If-Elif-Else",
    "content": "# üìä Multiple Conditions with Elif\n\n## When You Have More Than Two Options\n\n`elif` (short for \"else if\") lets you check multiple conditions:\n\n```python\nif condition1:\n    # First choice\nelif condition2:\n    # Second choice\nelif condition3:\n    # Third choice\nelse:\n    # Default (if nothing else matches)\n```\n\n## Example: Letter Grades\n\n```python\nscore = 85\n\nif score >= 90:\n    grade = \"A\"\nelif score >= 80:\n    grade = \"B\"\nelif score >= 70:\n    grade = \"C\"\nelif score >= 60:\n    grade = \"D\"\nelse:\n    grade = \"F\"\n\nprint(grade)  # B\n```\n\n## Order Matters!\n\nPython checks conditions from top to bottom and stops at the first True:\n\n```python\nscore = 95\n\n# WRONG order - 95 >= 60 is True, so prints D!\nif score >= 60: print(\"D\")\nelif score >= 70: print(\"C\")\nelif score >= 80: print(\"B\")\nelif score >= 90: print(\"A\")\n\n# CORRECT order - checks highest first\nif score >= 90: print(\"A\")\nelif score >= 80: print(\"B\")\nelif score >= 70: print(\"C\")\nelif score >= 60: print(\"D\")\n```\n\n---\n\n## üéØ Your Task\n\nGiven `score = 75`:\n- >= 90: print `\"A\"`\n- >= 80: print `\"B\"`\n- >= 70: print `\"C\"`\n- else: print `\"F\"`\n",
    "starter_code": "score = 75\n\n# Determine grade\n",
    "solution_code": "score = 75\n\n# Determine grade\nif score >= 90:\n    print(\"A\")\nelif score >= 80:\n    print(\"B\")\nelif score >= 70:\n    print(\"C\")\nelse:\n    print(\"F\")",
    "expected_output": "C",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "25": {
    "id": 25,
    "title": "Comparison Operators",
    "content": "# ‚öñÔ∏è Comparison Operators\n\n## Comparing Values\n\nComparison operators compare two values and return `True` or `False`:\n\n| Operator | Meaning | Example | Result |\n| --- | --- | --- | --- |\n| `==` | Equal to | `5 == 5` | `True` |\n| `!=` | Not equal to | `5 != 3` | `True` |\n| `>` | Greater than | `5 > 3` | `True` |\n| `<` | Less than | `5 < 3` | `False` |\n| `>=` | Greater or equal | `5 >= 5` | `True` |\n| `<=` | Less or equal | `5 <= 3` | `False` |\n\n## Common Mistake: = vs ==\n\n```python\n# = is ASSIGNMENT (storing a value)\nx = 5\n\n# == is COMPARISON (checking equality)\nif x == 5:\n    print(\"x is five!\")\n```\n\n## Comparing Strings\n\nYou can compare strings too:\n\n```python\nname = \"Alice\"\nif name == \"Alice\":\n    print(\"Hello, Alice!\")\n\n# Alphabetical comparison\n\"apple\" < \"banana\"  # True (a comes before b)\n```\n\n---\n\n## üéØ Your Task\n\nGiven `a = 10` and `b = 10`:\nCheck if they are equal and print: `Equal: True`\n",
    "starter_code": "a = 10\nb = 10\n\n# Check if equal\nresult = a == b\nprint(f\"Equal: {result}\")",
    "solution_code": "a = 10\nb = 10\n\n# Check if equal\nresult = a == b\nprint(f\"Equal: {result}\")",
    "expected_output": "Equal: True",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "26": {
    "id": 26,
    "title": "Logical AND",
    "content": "# üîó Logical AND\n\n## Combining Conditions\n\n`and` requires BOTH conditions to be True:\n\n```python\nif condition1 and condition2:\n    # Runs only if BOTH are True\n```\n\n## Real-World Example\n\nTo enter a bar, you need to be 21+ AND have ID:\n\n```python\nage = 25\nhas_id = True\n\nif age >= 21 and has_id:\n    print(\"Welcome!\")\nelse:\n    print(\"Sorry, can't enter\")\n```\n\n## Truth Table for AND\n\n| A | B | A and B |\n| --- | --- | --- |\n| True | True | **True** |\n| True | False | False |\n| False | True | False |\n| False | False | False |\n\nOnly True if BOTH are True!\n\n## Multiple ANDs\n\nYou can chain multiple conditions:\n\n```python\nif age >= 18 and has_license and not is_suspended:\n    print(\"You can drive!\")\n```\n\n---\n\n## üéØ Your Task\n\nGiven:\n- `age = 25`\n- `has_ticket = True`\n\nIf age >= 18 AND has_ticket, print `\"Can enter\"`\n",
    "starter_code": "age = 25\nhas_ticket = True\n\n# Check both conditions\n",
    "solution_code": "age = 25\nhas_ticket = True\n\n# Check both conditions\nif age >= 18 and has_ticket:\n    print(\"Can enter\")",
    "expected_output": "Can enter",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "27": {
    "id": 27,
    "title": "Logical OR",
    "content": "# üîÄ Logical OR\n\n## Either Condition\n\n`or` requires AT LEAST ONE condition to be True:\n\n```python\nif condition1 or condition2:\n    # Runs if EITHER (or both) is True\n```\n\n## Real-World Example\n\nFree shipping if order is $50+ OR member is premium:\n\n```python\norder_total = 35\nis_premium = True\n\nif order_total >= 50 or is_premium:\n    print(\"Free shipping!\")\nelse:\n    print(\"Shipping: $5\")\n```\n\n## Truth Table for OR\n\n| A | B | A or B |\n| --- | --- | --- |\n| True | True | True |\n| True | False | True |\n| False | True | True |\n| False | False | **False** |\n\nOnly False if BOTH are False!\n\n## Combining AND and OR\n\nUse parentheses for clarity:\n\n```python\nif (is_weekend or is_holiday) and not is_working:\n    print(\"Day off!\")\n```\n\n---\n\n## üéØ Your Task\n\nGiven:\n- `is_member = False`\n- `has_coupon = True`\n\nIf is_member OR has_coupon, print `\"Discount applied\"`\n",
    "starter_code": "is_member = False\nhas_coupon = True\n\n# Check if either is true\n",
    "solution_code": "is_member = False\nhas_coupon = True\n\n# Check if either is true\nif is_member or has_coupon:\n    print(\"Discount applied\")",
    "expected_output": "Discount applied",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "28": {
    "title": "Logical NOT",
    "chapter_title": "Logic",
    "content": "# ‚ùå Logical NOT: Flip True to False\n\n## What Does NOT Do?\n\nThe `not` operator inverts boolean values:\n- `not True` ‚Üí `False`\n- `not False` ‚Üí `True`\n\n## Basic Usage\n\n```python\nis_admin = False\nif not is_admin:\n    print(\"Access denied\")  # This runs!\n```\n\n## Common Patterns\n\n### Check if NOT in collection\n```python\nif item not in my_list:\n    print(\"Item not found\")\n```\n\n### Check if NOT equal\n```python\nif status != 'complete':\n    print(\"Still processing\")\n```\n\n### Double negative\n```python\nnot not True  # True (double negation)\nnot not False  # False\n```\n\n## With Truthiness\n\n```python\nmy_list = []\nif not my_list:  # Empty list is falsy\n    print(\"List is empty!\")\n\nusername = \"\"\nif not username:  # Empty string is falsy\n    print(\"Please enter username\")\n```\n\n## Combining with AND/OR\n\n```python\n# Not admin AND not owner\nif not is_admin and not is_owner:\n    print(\"No privileges\")\n\n# Same thing with De Morgan's law\nif not (is_admin or is_owner):\n    print(\"No privileges\")\n```\n\n## Readability Tip\n\n```python\n# Hard to read\nif not x != 5:\n    ...\n\n# Clearer\nif x == 5:\n    ...\n```\n\n---\n\n## üéØ Your Task\n\nUse the `not` operator for boolean logic and condition checking.",
    "starter_code": "# Basic NOT\nprint(f\"not True = {not True}\")\nprint(f\"not False = {not False}\")\n\n# With variables\nis_logged_in = False\nif not is_logged_in:\n    print(\"\\nPlease log in!\")\n\n# Checking empty collections\nitems = []\nif not items:\n    print(\"Cart is empty\")\n\n# not in\nbanned_users = ['spam_bot', 'trouble_maker']\nuser = 'alice'\nif user not in banned_users:\n    print(f\"{user} is allowed\")\n\n# Combined conditions\nis_admin = False\nis_verified = True\nif not is_admin and is_verified:\n    print(\"\\nRegular verified user\")",
    "solution_code": "print(not True)   # False\nprint(not False)  # True\n\nitems = []\nif not items:\n    print(\"Empty!\")",
    "expected_output": "not True = False\nnot False = True\n\nPlease log in!\nCart is empty\nalice is allowed"
  },
  "29": {
    "id": 29,
    "title": "Nested Conditionals",
    "content": "# ü™Ü Nested If Statements\n\n## If Inside If\n\nYou can put if statements inside other if statements:\n\n```python\nif has_account:\n    if is_verified:\n        print(\"Full access\")\n    else:\n        print(\"Please verify your email\")\nelse:\n    print(\"Please create an account\")\n```\n\n## When to Nest\n\nUseful when second check only makes sense if first is True:\n\n```python\nif user_input:  # First check: did they enter anything?\n    if user_input.isdigit():  # Only check this if there's input\n        print(\"Valid number!\")\n    else:\n        print(\"Not a number\")\nelse:\n    print(\"No input provided\")\n```\n\n## Avoid Deep Nesting\n\nToo many levels becomes hard to read:\n\n```python\n# BAD - too nested!\nif a:\n    if b:\n        if c:\n            if d:\n                do_something()\n\n# BETTER - use AND\nif a and b and c and d:\n    do_something()\n```\n\n---\n\n## üéØ Your Task\n\nGiven:\n- `logged_in = True`\n- `is_admin = True`\n\nIf logged_in, check if is_admin:\n- If admin: print `\"Admin panel\"`\n- Else: print `\"User dashboard\"`\n",
    "starter_code": "logged_in = True\nis_admin = True\n\n# Nested check\n",
    "solution_code": "logged_in = True\nis_admin = True\n\n# Nested check\nif logged_in:\n    if is_admin:\n        print(\"Admin panel\")\n    else:\n        print(\"User dashboard\")",
    "expected_output": "Admin panel",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "30": {
    "id": 30,
    "title": "Ternary Operator",
    "content": "# ‚ö° One-Line Conditionals\n\n## The Ternary Operator\n\nPython has a shorthand for simple if-else:\n\n```python\nvalue_if_true if condition else value_if_false\n```\n\n## Example\n\n```python\nage = 20\nstatus = \"Adult\" if age >= 18 else \"Minor\"\nprint(status)  # Adult\n```\n\nThis is equivalent to:\n```python\nif age >= 18:\n    status = \"Adult\"\nelse:\n    status = \"Minor\"\n```\n\n## When to Use\n\n‚úÖ Good for simple, short conditions:\n```python\nresult = \"Pass\" if score >= 70 else \"Fail\"\nmessage = f\"Welcome, {name}!\" if name else \"Welcome, Guest!\"\n```\n\n‚ùå Avoid for complex logic:\n```python\n# Too complex for ternary - use regular if-else\ngrade = \"A\" if score >= 90 else \"B\" if score >= 80 else \"C\" if score >= 70 else \"F\"\n```\n\n---\n\n## üéØ Your Task\n\nGiven `points = 150`:\nSet `level` to `\"Gold\"` if points >= 100, else `\"Silver\"`.\nPrint the level.\n",
    "starter_code": "points = 150\n\n# Set level using ternary\n\n\n# Print level\n",
    "solution_code": "points = 150\n\n# Set level using ternary\nlevel = \"Gold\" if points >= 100 else \"Silver\"\n\n# Print level\nprint(level)",
    "expected_output": "Gold",
    "chapter_id": 3,
    "chapter_title": "Logic & Control Flow"
  },
  "31": {
    "id": 31,
    "title": "Defining Functions",
    "content": "# üîß Creating Functions\n\n## What is a Function?\n\nA **function** is a reusable block of code that performs a specific task. Instead of writing the same code over and over, you define it once and call it whenever needed.\n\n## Real-World Analogy\n\nThink of a function like a **recipe**:\n- You write the recipe once\n- Every time you want that dish, you follow the same recipe\n- You don't have to figure it out from scratch each time!\n\n## Why Use Functions?\n\n1. **Reusability**: Write once, use many times\n2. **Organization**: Break complex code into manageable pieces\n3. **Readability**: Give meaningful names to code blocks\n4. **Maintenance**: Fix bugs in one place\n\n## Defining a Function\n\n![Function Machine](/assets/python-diagrams/function_machine.png)\n\n```python\ndef function_name():\n    # Code inside the function\n    print(\"Hello from the function!\")\n```\n\n- `def` keyword starts the definition\n- `function_name` is what you call it\n- `():` parentheses and colon are required\n- Indented code is the function body\n\n## Calling a Function\n\nThe function doesn't run until you **call** it:\n\n```python\ndef greet():\n    print(\"Hello!\")\n\ngreet()  # Call the function - output: Hello!\ngreet()  # Call it again!\n```\n\n---\n\n## üéØ Your Task\n\nDefine a function called `say_hello` that prints `\"Hello, World!\"`.\nThen call it.\n",
    "starter_code": "# Define the function\n\n\n# Call the function\n",
    "solution_code": "# Define the function\ndef say_hello():\n    print(\"Hello, World!\")\n\n# Call the function\nsay_hello()",
    "expected_output": "Hello, World!",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "32": {
    "id": 32,
    "title": "Function Parameters",
    "content": "# üì• Parameters: Passing Data to Functions\n\n## What are Parameters?\n\nParameters let you pass data INTO a function. They're like ingredients for a recipe - different ingredients, different results!\n\n```python\ndef greet(name):  # 'name' is a parameter\n    print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")  # Output: Hello, Alice!\ngreet(\"Bob\")    # Output: Hello, Bob!\n```\n\n## Parameters vs Arguments\n\n| Term | Definition |\n| --- | --- |\n| **Parameter** | Variable in function definition |\n| **Argument** | Actual value passed when calling |\n\n```python\ndef greet(name):    # 'name' is the PARAMETER\n    print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")      # \"Alice\" is the ARGUMENT\n```\n\n## Multiple Parameters\n\nFunctions can have multiple parameters:\n\n```python\ndef add(a, b):\n    result = a + b\n    print(result)\n\nadd(5, 3)  # Output: 8\nadd(10, 20)  # Output: 30\n```\n\n---\n\n## üéØ Your Task\n\nDefine `greet(name)` that prints `\"Welcome, {name}!\"`.\nCall it with `\"Python\"`.\n",
    "starter_code": "# Define greet with name parameter\n\n\n# Call with \"Python\"\n",
    "solution_code": "# Define greet with name parameter\ndef greet(name):\n    print(f\"Welcome, {name}!\")\n\n# Call with \"Python\"\ngreet(\"Python\")",
    "expected_output": "Welcome, Python!",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "33": {
    "id": 33,
    "title": "Return Values",
    "content": "# üì§ Return Values: Getting Data Back\n\n## What is Return?\n\n`return` sends a value back to where the function was called. This value can be stored, used in calculations, or printed.\n\n```python\ndef add(a, b):\n    return a + b  # Send the result back\n\nresult = add(5, 3)  # result now equals 8\nprint(result)       # Output: 8\n```\n\n## Return vs Print\n\n| `print()` | `return` |\n| --- | --- |\n| Shows output to screen | Sends value back to caller |\n| Value is lost after printing | Value can be used further |\n| Debugging/user display | Building blocks for programs |\n\n```python\n# With print - can't use the result\ndef add_print(a, b):\n    print(a + b)\n\nx = add_print(5, 3)  # Prints 8, but x is None!\n\n# With return - can use the result\ndef add_return(a, b):\n    return a + b\n\ny = add_return(5, 3)  # y equals 8\nz = y * 2             # z equals 16\n```\n\n## Return Ends the Function\n\nCode after `return` doesn't run:\n\n```python\ndef example():\n    return \"Done\"\n    print(\"This never runs!\")  # Unreachable!\n```\n\n---\n\n## üéØ Your Task\n\nDefine `double(n)` that returns `n * 2`.\nCall it with `7` and print the result.\n",
    "starter_code": "# Define double function\n\n\n# Call with 7 and print\n",
    "solution_code": "# Define double function\ndef double(n):\n    return n * 2\n\n# Call with 7 and print\nresult = double(7)\nprint(result)",
    "expected_output": "14",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "34": {
    "id": 34,
    "title": "Multiple Parameters",
    "content": "# üìä Working with Multiple Parameters\n\n## Functions with Several Inputs\n\nFunctions often need multiple pieces of data:\n\n```python\ndef calculate_area(width, height):\n    return width * height\n\narea = calculate_area(5, 3)\nprint(area)  # 15\n```\n\n## Order Matters (Positional Arguments)\n\nArguments are matched to parameters by position:\n\n```python\ndef greet(first_name, last_name):\n    print(f\"Hello, {first_name} {last_name}!\")\n\ngreet(\"John\", \"Doe\")    # Hello, John Doe!\ngreet(\"Doe\", \"John\")    # Hello, Doe John! (wrong order!)\n```\n\n## Practical Example\n\n```python\ndef create_email(username, domain):\n    return f\"{username}@{domain}\"\n\nemail = create_email(\"alice\", \"gmail.com\")\nprint(email)  # alice@gmail.com\n```\n\n---\n\n## üéØ Your Task\n\nDefine `calculate_area(width, height)` that returns `width * height`.\nCall it with `5` and `3`, print the result.\n",
    "starter_code": "# Define calculate_area\n\n\n# Call with 5, 3 and print\n",
    "solution_code": "# Define calculate_area\ndef calculate_area(width, height):\n    return width * height\n\n# Call with 5, 3 and print\narea = calculate_area(5, 3)\nprint(area)",
    "expected_output": "15",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "35": {
    "id": 35,
    "title": "Default Parameters",
    "content": "# ‚öôÔ∏è Default Parameter Values\n\n## What are Default Values?\n\nYou can give parameters default values. If no argument is passed, the default is used:\n\n```python\ndef greet(name=\"Guest\"):\n    print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")  # Hello, Alice!\ngreet()         # Hello, Guest! (uses default)\n```\n\n## Why Use Defaults?\n\n- Make functions more flexible\n- Reduce required arguments\n- Provide sensible fallbacks\n\n## Rules for Default Parameters\n\nDefault parameters must come AFTER non-default ones:\n\n```python\n# CORRECT - default at the end\ndef greet(name, greeting=\"Hello\"):\n    print(f\"{greeting}, {name}!\")\n\n# ERROR - default before non-default\ndef greet(greeting=\"Hello\", name):  # SyntaxError!\n    print(f\"{greeting}, {name}!\")\n```\n\n---\n\n## üéØ Your Task\n\nDefine `power(base, exp=2)` that returns `base ** exp`.\nCall it with just `4` (should return 16 since exp defaults to 2).\n",
    "starter_code": "# Define power with default exp=2\n\n\n# Call with just 4\n",
    "solution_code": "# Define power with default exp=2\ndef power(base, exp=2):\n    return base ** exp\n\n# Call with just 4\nresult = power(4)\nprint(result)",
    "expected_output": "16",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "36": {
    "id": 36,
    "title": "Keyword Arguments",
    "content": "# üè∑Ô∏è Keyword Arguments\n\n## What are Keyword Arguments?\n\nYou can specify arguments by name, not just position:\n\n```python\ndef greet(name, greeting):\n    print(f\"{greeting}, {name}!\")\n\n# Using keyword arguments\ngreet(name=\"Alice\", greeting=\"Hi\")\ngreet(greeting=\"Hello\", name=\"Bob\")  # Order doesn't matter!\n```\n\n## Why Use Keyword Arguments?\n\n1. **Clarity**: Makes code more readable\n2. **Flexibility**: Call in any order\n3. **Skip defaults**: Override only specific defaults\n\n```python\ndef create_user(name, age, email, is_admin=False):\n    # ...\n    \n# Skip to the argument you need\ncreate_user(\"Alice\", 25, \"a@b.com\", is_admin=True)\n```\n\n## Mixing Positional and Keyword\n\nPositional arguments must come before keyword arguments:\n\n```python\ndef func(a, b, c):\n    print(a, b, c)\n\nfunc(1, 2, c=3)      # OK\nfunc(1, b=2, c=3)    # OK\nfunc(a=1, 2, 3)      # ERROR!\n```\n\n---\n\n## üéØ Your Task\n\nDefine `describe(item, price)` that prints `\"{item}: ${price}\"`.\nCall it with keyword arguments: `price=9.99, item=\"Book\"`.\n",
    "starter_code": "# Define describe\n\n\n# Call with keyword arguments\n",
    "solution_code": "# Define describe\ndef describe(item, price):\n    print(f\"{item}: ${price}\")\n\n# Call with keyword arguments\ndescribe(price=9.99, item=\"Book\")",
    "expected_output": "Book: $9.99",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "37": {
    "id": 37,
    "title": "Lambda Functions",
    "content": "# ‚ö° Lambda Functions: One-Line Functions\n\n## What is a Lambda?\n\nA **lambda** is a small anonymous function defined in one line:\n\n```python\n# Regular function\ndef square(x):\n    return x ** 2\n\n# Lambda equivalent\nsquare = lambda x: x ** 2\n```\n\n## Lambda Syntax\n\n```python\nlambda arguments: expression\n```\n\n- No `def` or `return` keywords\n- Expression is automatically returned\n- Can have multiple arguments\n\n## Examples\n\n```python\n# One argument\ndouble = lambda x: x * 2\nprint(double(5))  # 10\n\n# Two arguments\nadd = lambda a, b: a + b\nprint(add(3, 4))  # 7\n\n# Conditional\nis_adult = lambda age: \"Adult\" if age >= 18 else \"Minor\"\nprint(is_adult(20))  # Adult\n```\n\n## When to Use Lambda\n\n- Short, simple operations\n- Passing to other functions (like `sort`, `map`, `filter`)\n- One-time use functions\n\n---\n\n## üéØ Your Task\n\nCreate a lambda function `triple` that multiplies by 3.\nCall it with `10` and print the result.\n",
    "starter_code": "# Create lambda triple\n\n\n# Call with 10\n",
    "solution_code": "# Create lambda triple\ntriple = lambda x: x * 3\n\n# Call with 10\nprint(triple(10))",
    "expected_output": "30",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "38": {
    "id": 38,
    "title": "Docstrings",
    "content": "# üìù Documenting Functions with Docstrings\n\n## What is a Docstring?\n\nA **docstring** is a string that describes what your function does. It goes right after the function definition:\n\n```python\ndef add(a, b):\n    \"\"\"Returns the sum of a and b.\"\"\"\n    return a + b\n```\n\n## Why Write Docstrings?\n\n1. Help other developers understand your code\n2. Remind yourself what the function does\n3. Tools can auto-generate documentation\n4. Shows up in `help()` function!\n\n```python\nhelp(add)\n# Output:\n# add(a, b)\n#     Returns the sum of a and b.\n```\n\n## Multi-line Docstrings\n\nFor more complex functions:\n\n```python\ndef calculate_tip(bill, tip_percent=15):\n    \"\"\"\n    Calculate the tip amount for a bill.\n    \n    Args:\n        bill: Total bill amount\n        tip_percent: Tip percentage (default 15)\n    \n    Returns:\n        The tip amount as a float\n    \"\"\"\n    return bill * (tip_percent / 100)\n```\n\n---\n\n## üéØ Your Task\n\nDefine `multiply(a, b)` with a docstring.\nReturn `a * b` and call with `6, 7`.\n",
    "starter_code": "# Define multiply with docstring\n\n\n# Call with 6, 7\n",
    "solution_code": "# Define multiply with docstring\ndef multiply(a, b):\n    \"\"\"Returns the product of a and b.\"\"\"\n    return a * b\n\n# Call with 6, 7\nprint(multiply(6, 7))",
    "expected_output": "42",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "39": {
    "id": 39,
    "title": "Variable Scope",
    "content": "# üî≠ Variable Scope\n\n## What is Scope?\n\n**Scope** determines where a variable can be accessed. Variables created inside a function only exist inside that function!\n\n```python\ndef my_function():\n    x = 10  # Local variable - only exists inside function\n    print(x)\n\nmy_function()  # Output: 10\nprint(x)       # ERROR! x doesn't exist here\n```\n\n## Local vs Global\n\n| Type | Where Created | Where Accessible |\n| --- | --- | --- |\n| **Local** | Inside a function | Only that function |\n| **Global** | Outside all functions | Everywhere |\n\n```python\ny = 20  # Global variable\n\ndef show():\n    print(y)  # Can read global variable\n\nshow()  # Output: 20\n```\n\n## Shadowing\n\nA local variable can have the same name as a global (but it's a different variable!):\n\n```python\nx = 100  # Global\n\ndef test():\n    x = 5  # Local - different variable!\n    print(x)  # 5\n\ntest()\nprint(x)  # 100 (global unchanged)\n```\n\n---\n\n## üéØ Your Task\n\nDefine a function `show_secret()` that:\n1. Creates a local variable `secret = \"Python rocks!\"`\n2. Prints it\n\nCall the function.\n",
    "starter_code": "# Define show_secret\n\n\n# Call it\n",
    "solution_code": "# Define show_secret\ndef show_secret():\n    secret = \"Python rocks!\"\n    print(secret)\n\n# Call it\nshow_secret()",
    "expected_output": "Python rocks!",
    "chapter_id": 4,
    "chapter_title": "Functions"
  },
  "40": {
    "title": "üêç Sand Sphinx Challenge",
    "chapter_title": "Logic",
    "content": "# üêç Sand Sphinx Challenge: Master Logic Gates\n\n## The Challenge\n\nThe Sand Sphinx guards ancient Python wisdom. To pass, you must prove your mastery of logical operations!\n\n## Logic Gate Review\n\n### AND Gate\nBoth inputs must be True:\n```python\nTrue and True   # True\nTrue and False  # False\nFalse and True  # False\nFalse and False # False\n```\n\n### OR Gate\nEither input can be True:\n```python\nTrue or True    # True\nTrue or False   # True\nFalse or True   # True\nFalse or False  # False\n```\n\n### NOT Gate\nFlip the value:\n```python\nnot True   # False\nnot False  # True\n```\n\n## Complex Combinations\n\n```python\n# Precedence: NOT > AND > OR\nnot True or False and True\n# = (not True) or (False and True)\n# = False or False\n# = False\n\n# Use parentheses for clarity!\n(not True) or (False and True)\n```\n\n## Short-Circuit Evaluation\n\nPython stops early when it knows the result:\n```python\nFalse and expensive_function()  # Never calls function!\nTrue or expensive_function()    # Never calls function!\n```\n\n---\n\n## üéØ Your Task\n\nSolve the Sphinx's logic puzzles to prove your worth!",
    "starter_code": "# Sphinx's Logic Challenges\n\nprint(\"=== AND Gate ===\")\nprint(f\"True and True = {True and True}\")\nprint(f\"True and False = {True and False}\")\n\nprint(\"\\n=== OR Gate ===\")\nprint(f\"True or False = {True or False}\")\nprint(f\"False or False = {False or False}\")\n\nprint(\"\\n=== NOT Gate ===\")\nprint(f\"not True = {not True}\")\nprint(f\"not False = {not False}\")\n\nprint(\"\\n=== Complex Expression ===\")\nresult = (True or False) and (not False)\nprint(f\"(True or False) and (not False) = {result}\")\n\n# Final challenge\na, b, c = True, False, True\nsphinx_puzzle = (a and b) or (not b and c)\nprint(f\"\\nSphinx's puzzle: (T and F) or (not F and T) = {sphinx_puzzle}\")\nprint(\"You have proven worthy! üèÜ\")",
    "solution_code": "print(True and False)  # False\nprint(True or False)   # True\nprint(not True)        # False",
    "expected_output": "=== AND Gate ===\nTrue and True = True\nTrue and False = False\n\n=== Complex Expression ===\n(True or False) and (not False) = True\n\nYou have proven worthy! üèÜ"
  },
  "41": {
    "id": 41,
    "title": "Lists Basics",
    "content": "# üìã Lists: Collections of Items\n\n## What is a List?\n\nA **list** is an ordered collection that can hold multiple items. Think of it like a shopping list or a playlist.\n\n```python\nfruits = [\"apple\", \"banana\", \"cherry\"]\nnumbers = [1, 2, 3, 4, 5]\nmixed = [1, \"hello\", 3.14, True]  # Can mix types!\n```\n\n## Why Use Lists?\n\n- Store multiple related items together\n- Access items by position (index)\n- Add, remove, or modify items\n- Loop through all items\n\n## Accessing Items by Index\n\n![List Indices](/assets/python-diagrams/list_indices.png)\n\nLists use **zero-based indexing** (counting starts at 0):\n\n```python\nfruits = [\"apple\", \"banana\", \"cherry\"]\n#          [0]       [1]       [2]\n\nprint(fruits[0])  # apple\nprint(fruits[1])  # banana\nprint(fruits[2])  # cherry\nprint(fruits[-1]) # cherry (last item)\n```\n\n## List Length\n\n```python\nprint(len(fruits))  # 3\n```\n\n---\n\n## üéØ Your Task\n\nCreate a list `colors = [\"red\", \"green\", \"blue\"]`.\nPrint the second item (index 1).\n",
    "starter_code": "# Create colors list\n\n\n# Print second item\n",
    "solution_code": "# Create colors list\ncolors = [\"red\", \"green\", \"blue\"]\n\n# Print second item\nprint(colors[1])",
    "expected_output": "green",
    "chapter_id": 7,
    "chapter_title": "Data Structures"
  },
  "42": {
    "id": 42,
    "title": "List Methods",
    "content": "# üìù Modifying Lists\n\n## Common List Methods\n\n| Method | What It Does | Example |\n| --- | --- | --- |\n| `.append(x)` | Add to end | `list.append(4)` |\n| `.insert(i, x)` | Insert at index | `list.insert(0, \"first\")` |\n| `.remove(x)` | Remove first occurrence | `list.remove(\"apple\")` |\n| `.pop()` | Remove and return last | `last = list.pop()` |\n| `.pop(i)` | Remove at index | `list.pop(0)` |\n| `.sort()` | Sort in place | `list.sort()` |\n| `.reverse()` | Reverse in place | `list.reverse()` |\n\n## Examples\n\n```python\nfruits = [\"apple\", \"banana\"]\n\n# Add items\nfruits.append(\"cherry\")     # [\"apple\", \"banana\", \"cherry\"]\nfruits.insert(0, \"mango\")   # [\"mango\", \"apple\", \"banana\", \"cherry\"]\n\n# Remove items\nfruits.remove(\"banana\")     # [\"mango\", \"apple\", \"cherry\"]\nlast = fruits.pop()         # last = \"cherry\", list = [\"mango\", \"apple\"]\n```\n\n## Modifying Lists Changes the Original\n\nUnlike strings (which are immutable), lists can be changed:\n\n```python\nnums = [3, 1, 2]\nnums.sort()       # nums is now [1, 2, 3]\nnums.reverse()    # nums is now [3, 2, 1]\n```\n\n---\n\n## üéØ Your Task\n\nStart with: `numbers = [1, 2, 3]`\n1. Append `4`\n2. Print the list\n",
    "starter_code": "numbers = [1, 2, 3]\n\n# Append 4\n\n\n# Print list\n",
    "solution_code": "numbers = [1, 2, 3]\n\n# Append 4\nnumbers.append(4)\n\n# Print list\nprint(numbers)",
    "expected_output": "[1, 2, 3, 4]",
    "chapter_id": 7,
    "chapter_title": "Data Structures"
  },
  "43": {
    "id": 43,
    "title": "List Slicing",
    "content": "# ‚úÇÔ∏è Slicing Lists\n\n## What is Slicing?\n\n**Slicing** extracts a portion of a list:\n\n```python\nlist[start:stop]  # Elements from start up to (not including) stop\n```\n\n## Examples\n\n```python\nnums = [0, 1, 2, 3, 4, 5]\n\nnums[1:4]   # [1, 2, 3]     (index 1, 2, 3)\nnums[:3]    # [0, 1, 2]     (start to index 2)\nnums[3:]    # [3, 4, 5]     (index 3 to end)\nnums[:]     # [0, 1, 2, 3, 4, 5]  (copy entire list)\n```\n\n## Negative Indices\n\n```python\nnums[-3:]   # [3, 4, 5]     (last 3 elements)\nnums[:-2]   # [0, 1, 2, 3]  (all except last 2)\n```\n\n## Step in Slicing\n\n```python\nnums[::2]   # [0, 2, 4]     (every 2nd element)\nnums[::-1]  # [5, 4, 3, 2, 1, 0]  (reversed!)\n```\n\n---\n\n## üéØ Your Task\n\nGiven: `letters = [\"a\", \"b\", \"c\", \"d\", \"e\"]`\nPrint the slice from index 1 to 3 (should be `[\"b\", \"c\", \"d\"]`).\n",
    "starter_code": "letters = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n\n# Print slice [1:4]\n",
    "solution_code": "letters = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n\n# Print slice [1:4]\nprint(letters[1:4])",
    "expected_output": "['b', 'c', 'd']",
    "chapter_id": 7,
    "chapter_title": "Data Structures"
  },
  "44": {
    "id": 44,
    "title": "Dictionaries",
    "content": "# üìñ Dictionaries: Key-Value Pairs\n\n## What is a Dictionary?\n\nA **dictionary** stores data as key-value pairs. Instead of accessing by index, you access by key name.\n\n![Dictionary Mapping](/assets/python-diagrams/dict_mapping.png)\n\n```python\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 25,\n    \"city\": \"New York\"\n}\n```\n\n## Why Use Dictionaries?\n\n- Access data by meaningful names (not numbers)\n- Store related information together\n- Fast lookups\n- Real-world mapping (word ‚Üí definition, ID ‚Üí record)\n\n## Accessing Values\n\n```python\nprint(person[\"name\"])  # Alice\nprint(person[\"age\"])   # 25\n```\n\n## Adding/Modifying\n\n```python\nperson[\"email\"] = \"alice@email.com\"  # Add new key\nperson[\"age\"] = 26                    # Modify existing\n```\n\n## Safe Access with .get()\n\n```python\n# If key doesn't exist:\nperson[\"phone\"]           # KeyError!\nperson.get(\"phone\")       # None (no error)\nperson.get(\"phone\", \"N/A\")  # \"N/A\" (custom default)\n```\n\n---\n\n## üéØ Your Task\n\nCreate a dictionary:\n```python\nbook = {\"title\": \"Python 101\", \"author\": \"John Doe\", \"pages\": 300}\n```\nPrint the author.\n",
    "starter_code": "# Create book dictionary\n\n\n# Print author\n",
    "solution_code": "# Create book dictionary\nbook = {\"title\": \"Python 101\", \"author\": \"John Doe\", \"pages\": 300}\n\n# Print author\nprint(book[\"author\"])",
    "expected_output": "John Doe",
    "chapter_id": 7,
    "chapter_title": "Data Structures"
  },
  "45": {
    "title": "Dictionary Methods",
    "chapter_title": "Data Structures",
    "content": "# üìñ Dictionary Methods: Master Key-Value Operations\n\n## Creating Dictionaries\n\n```python\n# Literal syntax\nperson = {\"name\": \"Alice\", \"age\": 30}\n\n# From keys\nkeys = [\"a\", \"b\", \"c\"]\nd = dict.fromkeys(keys, 0)  # {'a': 0, 'b': 0, 'c': 0}\n```\n\n## Essential Methods\n\n### get() - Safe Access\n```python\nd = {\"name\": \"Alice\"}\nd.get(\"name\")     # \"Alice\"\nd.get(\"age\")      # None (no error!)\nd.get(\"age\", 25)  # 25 (custom default)\n```\n\n### keys(), values(), items()\n```python\nd = {\"a\": 1, \"b\": 2}\nd.keys()    # dict_keys(['a', 'b'])\nd.values()  # dict_values([1, 2])\nd.items()   # dict_items([('a', 1), ('b', 2)])\n```\n\n### update() - Merge Dicts\n```python\nd1 = {\"a\": 1}\nd1.update({\"b\": 2, \"c\": 3})\n# d1 is now {'a': 1, 'b': 2, 'c': 3}\n```\n\n### pop() - Remove and Return\n```python\nd = {\"a\": 1, \"b\": 2}\nvalue = d.pop(\"a\")  # Returns 1, removes key\n```\n\n## Iterating\n\n```python\nfor key in d:\n    print(key)\n    \nfor key, value in d.items():\n    print(f\"{key}: {value}\")\n```\n\n## Dict Comprehension\n\n```python\nsquares = {x: x**2 for x in range(5)}\n# {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n```\n\n---\n\n## üéØ Your Task\n\nPractice using dictionary methods to manipulate data.",
    "starter_code": "person = {\"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\"}\n\n# Safe access with get()\nemail = person.get(\"email\", \"not provided\")\nprint(f\"Email: {email}\")\n\n# Get all keys and values\nprint(f\"Keys: {list(person.keys())}\")\nprint(f\"Values: {list(person.values())}\")\n\n# Iterate over items\nprint(\"\\nAll info:\")\nfor key, value in person.items():\n    print(f\"  {key}: {value}\")\n\n# Update (add new keys)\nperson.update({\"job\": \"Engineer\", \"country\": \"USA\"})\nprint(f\"\\nUpdated: {person}\")\n\n# Dict comprehension\nnumbers = [1, 2, 3, 4, 5]\nsquares = {n: n**2 for n in numbers}\nprint(f\"\\nSquares: {squares}\")",
    "solution_code": "d = {\"name\": \"Alice\", \"age\": 30}\nprint(d.get(\"city\", \"Unknown\"))\nprint(list(d.keys()))",
    "expected_output": "Email: not provided\nKeys: ['name', 'age', 'city']"
  },
  "46": {
    "id": 46,
    "title": "Tuples",
    "content": "# üìå Tuples: Immutable Sequences\n\n## What is a Tuple?\n\nA **tuple** is like a list, but it **cannot be changed** (immutable):\n\n```python\npoint = (10, 20)\ncolors = (\"red\", \"green\", \"blue\")\n```\n\n## Tuples vs Lists\n\n| Feature | List `[]` | Tuple `()` |\n| --- | --- | --- |\n| Mutable | ‚úÖ Yes | ‚ùå No |\n| Use case | Data that changes | Data that shouldn't change |\n| Syntax | `[1, 2, 3]` | `(1, 2, 3)` |\n\n## Why Use Tuples?\n\n- Protect data from accidental changes\n- Dictionary keys (must be immutable)\n- Return multiple values from functions\n- Slightly faster than lists\n\n## Tuple Unpacking\n\nAssign tuple values to multiple variables:\n\n```python\npoint = (100, 200)\nx, y = point  # x=100, y=200\n\n# Swap variables!\na, b = b, a\n```\n\n---\n\n## üéØ Your Task\n\nCreate `coordinates = (100, 200)`.\nUnpack into `x` and `y`.\nPrint `x` and `y`.\n",
    "starter_code": "# Create coordinates tuple\n\n\n# Unpack\n\n\n# Print x and y\n",
    "solution_code": "# Create coordinates tuple\ncoordinates = (100, 200)\n\n# Unpack\nx, y = coordinates\n\n# Print x and y\nprint(x)\nprint(y)",
    "expected_output": "100\n200",
    "chapter_id": 7,
    "chapter_title": "Data Structures"
  },
  "47": {
    "title": "Sets",
    "chapter_title": "Data Structures",
    "content": "# üî∑ Sets: Unique Collections with Superpowers\n\n## What is a Set?\n\nA **set** is an unordered collection of unique elements‚Äîno duplicates allowed!\n\n```python\nmy_set = {1, 2, 3, 2, 1}\nprint(my_set)  # {1, 2, 3} - duplicates removed automatically!\n```\n\n## Creating Sets\n\n```python\n# From literals\ncolors = {\"red\", \"green\", \"blue\"}\n\n# From list (removes duplicates)\nunique = set([1, 2, 2, 3, 3, 3])  # {1, 2, 3}\n\n# Empty set (NOT {} - that's a dict!)\nempty = set()\n```\n\n## Basic Operations\n\n```python\ns = {1, 2, 3}\n\ns.add(4)      # Add element\ns.remove(2)   # Remove (error if not found)\ns.discard(5)  # Remove (no error if not found)\n3 in s        # Check membership: True\nlen(s)        # Size: 3\n```\n\n## Set Math Operations\n\n```python\na = {1, 2, 3}\nb = {2, 3, 4}\n\na | b  # Union: {1, 2, 3, 4}\na & b  # Intersection: {2, 3}\na - b  # Difference: {1}\na ^ b  # Symmetric difference: {1, 4}\n```\n\n## Real-World Uses\n\n```python\n# Find unique values\nunique_customers = set(order['customer'] for order in orders)\n\n# Find common items\ncommon_interests = user1_interests & user2_interests\n\n# Remove items seen before\nnew_items = all_items - already_processed\n```\n\n## Set vs List Lookup Speed\n\n```python\n# List lookup: O(n) - checks each element\n# Set lookup: O(1) - instant!\n\nif item in my_set:  # Fast!\n    process(item)\n```\n\n---\n\n## üéØ Your Task\n\nUse sets for unique collections and set operations.",
    "starter_code": "# Find unique items\npurchases = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"]\nunique_items = set(purchases)\nprint(f\"Unique items: {unique_items}\")\nprint(f\"Number of unique items: {len(unique_items)}\")\n\n# Set operations\npython_devs = {\"Alice\", \"Bob\", \"Charlie\", \"Diana\"}\njs_devs = {\"Bob\", \"Diana\", \"Eve\", \"Frank\"}\n\n# Who knows both?\nboth = python_devs & js_devs\nprint(f\"\\nKnow both Python and JS: {both}\")\n\n# Who knows Python but not JS?\npython_only = python_devs - js_devs\nprint(f\"Python only: {python_only}\")\n\n# All developers\nall_devs = python_devs | js_devs\nprint(f\"All developers: {all_devs}\")",
    "solution_code": "items = [1, 2, 2, 3, 3, 3]\nunique = set(items)\nprint(f\"Unique: {unique}\")\n\na = {1, 2, 3}\nb = {2, 3, 4}\nprint(f\"Union: {a | b}\")\nprint(f\"Intersection: {a & b}\")",
    "expected_output": "Unique items: {'apple', 'banana', 'cherry'}\nNumber of unique items: 3\n\nKnow both Python and JS: {'Bob', 'Diana'}"
  },
  "48": {
    "title": "List Comprehension",
    "chapter_title": "Data Structures",
    "content": "# üöÄ List Comprehension: Build Lists in One Line\n\n## The Transformation\n\nTraditional loop:\n```python\nsquares = []\nfor x in range(5):\n    squares.append(x ** 2)\n```\n\nList comprehension:\n```python\nsquares = [x ** 2 for x in range(5)]\n```\n\nSame result, one line!\n\n## The Pattern\n\n```python\n[expression for item in iterable]\n```\n\n## With Condition (Filtering)\n\n```python\n# Only even numbers\nevens = [x for x in range(10) if x % 2 == 0]\n# [0, 2, 4, 6, 8]\n```\n\nPattern:\n```python\n[expression for item in iterable if condition]\n```\n\n## Examples\n\n```python\n# Double each number\n[x * 2 for x in [1, 2, 3]]  # [2, 4, 6]\n\n# Uppercase strings\n[s.upper() for s in ['a', 'b', 'c']]  # ['A', 'B', 'C']\n\n# Filter and transform\n[x ** 2 for x in range(10) if x % 2 == 0]  # [0, 4, 16, 36, 64]\n```\n\n## With else (if-else expression)\n\n```python\n# Must use expression format, not condition format\n['even' if x % 2 == 0 else 'odd' for x in range(5)]\n# ['even', 'odd', 'even', 'odd', 'even']\n```\n\n## When to Use\n\n‚úÖ Simple transformations\n‚úÖ Simple filters\n‚ùå Complex logic (use regular loops)\n‚ùå Multiple statements (use regular loops)\n\n---\n\n## üéØ Your Task\n\nUse list comprehensions for concise data transformations.",
    "starter_code": "# Basic transformation\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x ** 2 for x in numbers]\nprint(f\"Squares: {squares}\")\n\n# With filtering\nevens = [x for x in range(10) if x % 2 == 0]\nprint(f\"Evens: {evens}\")\n\n# String processing\nwords = ['hello', 'world', 'python']\nuppercase = [w.upper() for w in words]\nprint(f\"Uppercase: {uppercase}\")\n\n# Conditional expression\nlabels = ['even' if x % 2 == 0 else 'odd' for x in range(5)]\nprint(f\"Labels: {labels}\")\n\n# From dict values\nprices = {'apple': 1.5, 'banana': 0.75, 'cherry': 3.0}\nexpensive = [item for item, price in prices.items() if price > 1]\nprint(f\"Expensive: {expensive}\")",
    "solution_code": "squares = [x ** 2 for x in range(5)]\nevens = [x for x in range(10) if x % 2 == 0]\nprint(squares, evens)",
    "expected_output": "Squares: [1, 4, 9, 16, 25]\nEvens: [0, 2, 4, 6, 8]\nUppercase: ['HELLO', 'WORLD', 'PYTHON']"
  },
  "49": {
    "title": "Flexible Greeting",
    "chapter_title": "Functions",
    "content": "# üëã Flexible Functions: Default Arguments\n\n## The Problem with Rigid Functions\n\nImagine you build a greeting function:\n```python\ndef greet(name, greeting, punctuation):\n    return f\"{greeting}, {name}{punctuation}\"\n```\n\nEvery time you call it, you MUST provide ALL three arguments:\n```python\ngreet(\"Alice\", \"Hello\", \"!\")\ngreet(\"Bob\", \"Hello\", \"!\")\ngreet(\"Charlie\", \"Hello\", \"!\")  # So repetitive!\n```\n\nMost of the time you just want \"Hello\" and \"!\"‚Äîshouldn't there be defaults?\n\n## The Solution: Default Arguments\n\n```python\ndef greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    return f\"{greeting}, {name}{punctuation}\"\n```\n\nNow you get FLEXIBILITY:\n```python\ngreet(\"Alice\")                      # \"Hello, Alice!\"\ngreet(\"Bob\", \"Hi\")                  # \"Hi, Bob!\"\ngreet(\"Charlie\", \"Hey\", \"?\")        # \"Hey, Charlie?\"\n```\n\n## Why This Matters\n\nDefault arguments make functions:\n- **Easier to use** (common case = fewer arguments)\n- **More flexible** (can still customize when needed)\n- **Cleaner code** (no repetition!)\n\n## Real-World Examples\n\n```python\n# File operations - most files are text, not binary\ndef read_file(path, mode=\"r\"):\n    ...\n\n# Web requests - most are GET, timeout of 30s is reasonable\ndef make_request(url, method=\"GET\", timeout=30):\n    ...\n\n# Data processing - usually want ascending order\ndef sort_data(data, ascending=True):\n    ...\n```\n\n## The Rule: Defaults Go at the End\n\n```python\n# ‚úÖ Correct - defaults at the end\ndef greet(name, greeting=\"Hello\", punctuation=\"!\"):\n\n# ‚ùå Wrong - default before non-default\ndef greet(greeting=\"Hello\", name, punctuation=\"!\"):  # ERROR!\n```\n\n---\n\n## üéØ Your Task\n\nCreate a flexible greeting function with defaults for `greeting` (default=\"Hello\") and `punctuation` (default=\"!\"). Then test it with different combinations!",
    "starter_code": "# Create a flexible greeting function\ndef greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    return f\"{greeting}, {name}{punctuation}\"\n\n# Test 1: Just a name (use all defaults)\nprint(greet(\"Alice\"))\n\n# Test 2: Custom greeting, default punctuation\nprint(greet(\"Bob\", \"Hi\", \".\"))",
    "solution_code": "def greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    return f\"{greeting}, {name}{punctuation}\"\n\nprint(greet(\"Alice\"))\nprint(greet(\"Bob\", \"Hi\", \".\"))",
    "expected_output": "Hello, Alice!\nHi, Bob."
  },
  "50": {
    "title": "Stats Calculator",
    "chapter_title": "Functions",
    "content": "# üßÆ Multiple Return Values: Getting More from Functions\n\n## The Problem\n\nSometimes one answer isn't enough! Imagine analyzing sales data:\n\n```python\ndef analyze_sales(data):\n    return sum(data)  # Only the total? What about average, max, min?\n```\n\nWhat if you need the total, average, max, AND min? Do you write 4 separate functions?\n\n## The Solution: Return Multiple Values\n\nPython functions can return **multiple values** using tuples:\n\n```python\ndef analyze_sales(data):\n    total = sum(data)\n    average = total / len(data)\n    highest = max(data)\n    lowest = min(data)\n    return total, average, highest, lowest  # Four values!\n```\n\n## How to Use Multiple Returns\n\n**Option 1: Unpack into separate variables**\n```python\ntotal, avg, high, low = analyze_sales(sales)\nprint(f\"Total: {total}, Average: {avg}\")\n```\n\n**Option 2: Store as a tuple**\n```python\nresults = analyze_sales(sales)\nprint(f\"Total: {results[0]}\")  # Access by index\n```\n\n## Why This Pattern Rocks\n\n**Before (messy):**\n```python\ntotal = get_total(data)\naverage = get_average(data)\nmaximum = get_maximum(data)  # 3 separate function calls!\n```\n\n**After (clean):**\n```python\ntotal, average, maximum = analyze(data)  # 1 call, all answers!\n```\n\n## Real-World Uses\n\n- **Statistics**: mean, median, mode, std all at once\n- **Coordinates**: x, y, z positions\n- **API responses**: data, status_code, headers\n- **File operations**: content, metadata, encoding\n\n---\n\n## üéØ Your Task\n\nCreate a `calc_stats` function that takes a list of numbers and returns:\n1. **min** - the smallest value\n2. **max** - the largest value  \n3. **mean** - the average\n\nThen **unpack** the results and print them!",
    "starter_code": "# Create a function that returns multiple statistics\ndef calc_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    mean = sum(numbers) / len(numbers)\n    return minimum, maximum, mean\n\n# Test data\ndata = [10, 20, 30, 40, 50]\n\n# Unpack the returned values\nmin_val, max_val, mean_val = calc_stats(data)\n\n# Print the results\nprint(f\"Min: {min_val}, Max: {max_val}, Mean: {mean_val}\")",
    "solution_code": "def calc_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    mean = sum(numbers) / len(numbers)\n    return minimum, maximum, mean\n\ndata = [10, 20, 30, 40, 50]\nmin_val, max_val, mean_val = calc_stats(data)\nprint(f\"Min: {min_val}, Max: {max_val}, Mean: {mean_val}\")",
    "expected_output": "Min: 10, Max: 50, Mean: 30.0"
  },
  "51": {
    "title": "Reading Lines",
    "chapter_title": "File Handling",
    "content": "# üìñ Reading Lines: Process Files Line by Line\n\n## Why Read Line by Line?\n\nFor large files, loading everything at once uses too much memory. Reading line by line is:\n- Memory efficient\n- Allows streaming processing\n- Works with files of any size\n\n## Basic Line Reading\n\n```python\nwith open('data.txt', 'r') as f:\n    for line in f:\n        print(line)\n```\n\n## The `with` Statement\n\nAlways use `with` for files‚Äîit automatically closes the file:\n\n```python\n# Good: File automatically closed\nwith open('file.txt', 'r') as f:\n    content = f.read()\n\n# Bad: Must remember to close\nf = open('file.txt', 'r')\ncontent = f.read()\nf.close()  # Easy to forget!\n```\n\n## Reading Methods\n\n```python\nwith open('data.txt', 'r') as f:\n    # Read entire file as string\n    content = f.read()\n    \n    # Read all lines as list\n    lines = f.readlines()\n    \n    # Read one line at a time\n    first_line = f.readline()\n```\n\n## Processing Lines\n\n```python\nwith open('data.txt', 'r') as f:\n    for line in f:\n        # Remove newline character\n        clean_line = line.strip()\n        \n        # Skip empty lines\n        if not clean_line:\n            continue\n            \n        # Process the line\n        process(clean_line)\n```\n\n## Line Numbers\n\n```python\nwith open('data.txt', 'r') as f:\n    for line_num, line in enumerate(f, 1):\n        print(f\"Line {line_num}: {line.strip()}\")\n```\n\n---\n\n## üéØ Your Task\n\nRead a file line by line and process its contents.",
    "starter_code": "# Create a sample file\ncontent = '''Line 1: Hello World\nLine 2: Python Programming\nLine 3: Data Science Rocks\n\nLine 5: After empty line'''\n\nwith open('sample.txt', 'w') as f:\n    f.write(content)\n\n# Read and process line by line\nprint(\"Reading file line by line:\")\nwith open('sample.txt', 'r') as f:\n    for line_num, line in enumerate(f, 1):\n        clean = line.strip()\n        if clean:  # Skip empty lines\n            print(f\"  {line_num}: {clean}\")\n\n# Count lines\nwith open('sample.txt', 'r') as f:\n    lines = f.readlines()\n    total = len(lines)\n    non_empty = sum(1 for line in lines if line.strip())\n    \nprint(f\"\\nTotal lines: {total}\")\nprint(f\"Non-empty lines: {non_empty}\")",
    "solution_code": "with open('sample.txt', 'w') as f:\n    f.write('Line 1\\nLine 2\\n\\nLine 4')\n\nwith open('sample.txt', 'r') as f:\n    for line_num, line in enumerate(f, 1):\n        print(f\"{line_num}: {line.strip()}\")",
    "expected_output": "Reading file line by line:\n  1: Line 1: Hello World\n  2: Line 2: Python Programming\n  3: Line 3: Data Science Rocks\n  5: Line 5: After empty line"
  },
  "52": {
    "title": "CSV Data",
    "chapter_title": "File Handling",
    "content": "# üìä CSV Files: Tabular Data Made Easy\n\n## What is CSV?\n\n**CSV** (Comma-Separated Values) is the universal format for tabular data‚Äîspreadsheets, databases, and data exports all use it!\n\n```csv\nname,age,city\nAlice,30,NYC\nBob,25,LA\n```\n\n## Reading CSV\n\n### Using csv module\n```python\nimport csv\n\nwith open('data.csv', 'r') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        print(row)  # Each row is a list\n```\n\n### As dictionaries (with headers)\n```python\nwith open('data.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        print(row['name'])  # Access by column name\n```\n\n## Writing CSV\n\n```python\nimport csv\n\ndata = [\n    ['name', 'age', 'city'],\n    ['Alice', 30, 'NYC'],\n    ['Bob', 25, 'LA']\n]\n\nwith open('output.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerows(data)\n```\n\n## With Pandas (Easier!)\n\n```python\nimport pandas as pd\n\n# Read\ndf = pd.read_csv('data.csv')\n\n# Write\ndf.to_csv('output.csv', index=False)\n```\n\n---\n\n## üéØ Your Task\n\nRead and write CSV files using the csv module.",
    "starter_code": "import csv\n\n# Create sample CSV\ndata = [\n    ['Name', 'Age', 'Role'],\n    ['Alice', 30, 'Engineer'],\n    ['Bob', 25, 'Designer'],\n    ['Charlie', 35, 'Manager']\n]\n\n# Write CSV\nwith open('employees.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerows(data)\nprint(\"Created employees.csv\")\n\n# Read CSV with DictReader\nprint(\"\\nReading employees:\")\nwith open('employees.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        print(f\"  {row['Name']} ({row['Age']}): {row['Role']}\")",
    "solution_code": "import csv\n\nwith open('data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['name', 'age'])\n    writer.writerow(['Alice', 30])",
    "expected_output": "Created employees.csv\n\nReading employees:\n  Alice (30): Engineer\n  Bob (25): Designer\n  Charlie (35): Manager"
  },
  "53": {
    "title": "JSON Basics",
    "chapter_title": "File Handling",
    "content": "# üìã JSON Basics: Read and Write Structured Data\n\n## What is JSON?\n\n**JSON** (JavaScript Object Notation) is the most popular format for data exchange. It looks like Python dictionaries!\n\n```json\n{\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"skills\": [\"Python\", \"SQL\"]\n}\n```\n\n## Reading JSON\n\n```python\nimport json\n\n# From a file\nwith open('data.json', 'r') as f:\n    data = json.load(f)\n\n# From a string\njson_string = '{\"name\": \"Alice\", \"age\": 30}'\ndata = json.loads(json_string)\n```\n\n## Writing JSON\n\n```python\nimport json\n\ndata = {\"name\": \"Alice\", \"age\": 30}\n\n# To a file\nwith open('data.json', 'w') as f:\n    json.dump(data, f, indent=2)\n\n# To a string\njson_string = json.dumps(data, indent=2)\n```\n\n## Pretty Printing\n\n```python\n# Compact\njson.dumps(data)  # '{\"name\":\"Alice\",\"age\":30}'\n\n# Pretty\njson.dumps(data, indent=2)\n# {\n#   \"name\": \"Alice\",\n#   \"age\": 30\n# }\n```\n\n## JSON ‚Üî Python Type Mapping\n\n| JSON | Python |\n|------|--------|\n| object | dict |\n| array | list |\n| string | str |\n| number | int/float |\n| true/false | True/False |\n| null | None |\n\n---\n\n## üéØ Your Task\n\nRead and write JSON data files.",
    "starter_code": "import json\n\n# Create data\nuser = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"email\": \"alice@example.com\",\n    \"skills\": [\"Python\", \"SQL\", \"Data Science\"]\n}\n\n# Write to JSON file\nwith open('user.json', 'w') as f:\n    json.dump(user, f, indent=2)\nprint(\"Saved user.json\")\n\n# Read from JSON file\nwith open('user.json', 'r') as f:\n    loaded_user = json.load(f)\n\nprint(f\"\\nLoaded data:\")\nprint(f\"  Name: {loaded_user['name']}\")\nprint(f\"  Skills: {', '.join(loaded_user['skills'])}\")",
    "solution_code": "import json\n\ndata = {\"name\": \"Alice\", \"age\": 30}\nprint(json.dumps(data, indent=2))",
    "expected_output": "Saved user.json\n\nLoaded data:\n  Name: Alice\n  Skills: Python, SQL, Data Science"
  },
  "54": {
    "title": "String Processing",
    "chapter_title": "Strings",
    "content": "# üî§ String Processing: Manipulate Text Like a Pro\n\n## Why String Processing?\n\nData often comes as text that needs transformation:\n- Formatting names and addresses\n- Cleaning user input\n- Preparing data for analysis\n\n## Essential String Methods\n\n### Case Transformations\n```python\nname = \"alice SMITH\"\nname.lower()   # \"alice smith\"\nname.upper()   # \"ALICE SMITH\"\nname.title()   # \"Alice Smith\"\nname.capitalize()  # \"Alice smith\"\n```\n\n### Checking Content\n```python\n\"123\".isdigit()    # True\n\"abc\".isalpha()    # True\n\"abc123\".isalnum() # True\n\"  \".isspace()     # True\n```\n\n### Padding and Alignment\n```python\n\"42\".zfill(5)      # \"00042\"\n\"hi\".ljust(5)      # \"hi   \"\n\"hi\".rjust(5)      # \"   hi\"\n\"hi\".center(5)     # \" hi  \"\n```\n\n### Joining and Splitting\n```python\n# Split text ‚Üí list\n\"a,b,c\".split(\",\")  # ['a', 'b', 'c']\n\n# Join list ‚Üí text\n\",\".join(['a', 'b', 'c'])  # \"a,b,c\"\n```\n\n## Chaining Methods\n\n```python\nraw = \"  HELLO WORLD  \"\nclean = raw.strip().lower().replace(\" \", \"_\")\n# \"hello_world\"\n```\n\n## String Formatting\n\n```python\nname = \"Alice\"\nage = 30\nf\"Name: {name}, Age: {age}\"  # f-string (recommended)\n\"Name: {}, Age: {}\".format(name, age)  # .format()\n```\n\n---\n\n## üéØ Your Task\n\nProcess and transform strings using various methods.",
    "starter_code": "# Raw input data\nraw_name = \"  JOHN DOE  \"\nraw_phone = \"555-123-4567\"\nraw_items = \"apple banana cherry\"\n\n# Process name: strip and title case\nclean_name = raw_name.strip().title()\nprint(f\"Name: {clean_name}\")\n\n# Process phone: remove dashes\nclean_phone = raw_phone.replace(\"-\", \"\")\nprint(f\"Phone: {clean_phone}\")\n\n# Process items: split into list\nitems = raw_items.split()\nprint(f\"Items: {items}\")\n\n# Join items with comma\nitems_str = \", \".join(items)\nprint(f\"Items string: {items_str}\")\n\n# Pad a number\norder_num = \"42\"\npadded = order_num.zfill(6)\nprint(f\"Order number: {padded}\")",
    "solution_code": "name = \"  ALICE SMITH  \"\nclean = name.strip().title()\nprint(clean)  # \"Alice Smith\" ",
    "expected_output": "Name: John Doe\nPhone: 5551234567\nItems: ['apple', 'banana', 'cherry']\nItems string: apple, banana, cherry\nOrder number: 000042"
  },
  "55": {
    "title": "Data Parsing",
    "chapter_title": "Strings",
    "content": "# üîç Data Parsing: Extract Information from Text\n\n## What is Parsing?\n\n**Parsing** means extracting structured data from unstructured text:\n- \"Name: Alice Smith\" ‚Üí Extract just \"Alice Smith\"\n- \"Price: $29.99\" ‚Üí Extract just 29.99 as a number\n- \"2024-01-15\" ‚Üí Extract year, month, day\n\n## The split() Method\n\nSplit text into parts based on a delimiter:\n\n```python\nline = \"John,25,Engineer\"\nparts = line.split(\",\")\n# ['John', '25', 'Engineer']\n\nname = parts[0]  # 'John'\nage = int(parts[1])  # 25 (as integer)\njob = parts[2]  # 'Engineer'\n```\n\n## Parsing Key-Value Pairs\n\n```python\nline = \"name=Alice\"\nkey, value = line.split(\"=\")\n# key = \"name\", value = \"Alice\"\n```\n\n## Multi-Level Parsing\n\n```python\ndata = \"Alice:25|Bob:30|Charlie:35\"\n\n# First split by |\npeople = data.split(\"|\")\n# ['Alice:25', 'Bob:30', 'Charlie:35']\n\n# Then split each by :\nfor person in people:\n    name, age = person.split(\":\")\n    print(f\"{name} is {age} years old\")\n```\n\n## Strip Whitespace\n\n```python\nline = \"  Hello World  \"\nclean = line.strip()  # \"Hello World\"\n\n# Each part after split\nparts = \"apple , banana , cherry\".split(\",\")\nclean_parts = [p.strip() for p in parts]\n# ['apple', 'banana', 'cherry']\n```\n\n---\n\n## üéØ Your Task\n\nParse structured data from comma-separated and key-value formats.",
    "starter_code": "# CSV-like data\ncsv_line = \"Alice Smith,25,Engineer,75000\"\n\n# Parse by splitting on comma\nparts = csv_line.split(\",\")\nname = parts[0]\nage = int(parts[1])\ntitle = parts[2]\nsalary = int(parts[3])\n\nprint(f\"Name: {name}\")\nprint(f\"Age: {age}\")\nprint(f\"Title: {title}\")\nprint(f\"Salary: ${salary:,}\")\n\n# Key-value data\nkv_data = \"color=blue|size=large|price=29.99\"\nfor pair in kv_data.split(\"|\"):\n    key, value = pair.split(\"=\")\n    print(f\"  {key}: {value}\")",
    "solution_code": "line = \"Alice,25,Engineer\"\nparts = line.split(\",\")\nprint(f\"Name: {parts[0]}\")\nprint(f\"Age: {int(parts[1])}\")",
    "expected_output": "Name: Alice Smith\nAge: 25\nTitle: Engineer\nSalary: $75,000"
  },
  "56": {
    "title": "Counting Words",
    "chapter_title": "Strings",
    "content": "# üìù Word Counting: Text Analysis Foundation\n\n## Why Count Words?\n\nWord counting is fundamental to text analysis:\n- Document length validation\n- Reading time estimation\n- Writing metrics (average word length)\n- Keyword frequency analysis\n\n## Basic Word Count\n\n```python\ntext = \"Hello world, how are you today?\"\nwords = text.split()  # Split on whitespace\nword_count = len(words)  # 6\n```\n\n## Understanding split()\n\n```python\n# Default: splits on any whitespace\n\"hello   world\".split()  # ['hello', 'world']\n\n# Custom delimiter\n\"apple,banana,cherry\".split(\",\")  # ['apple', 'banana', 'cherry']\n```\n\n## Handling Punctuation\n\n```python\n# Simple approach\nimport re\ntext = \"Hello, world! How are you?\"\nwords = re.findall(r'\\w+', text)  # ['Hello', 'world', 'How', 'are', 'you']\n```\n\n## Word Frequency\n\n```python\nfrom collections import Counter\n\ntext = \"the quick brown fox jumps over the lazy dog\"\nword_counts = Counter(text.lower().split())\n# Counter({'the': 2, 'quick': 1, 'brown': 1, ...})\n\n# Most common words\nword_counts.most_common(3)  # [('the', 2), ('quick', 1), ('brown', 1)]\n```\n\n## Text Statistics\n\n```python\ndef text_stats(text):\n    words = text.split()\n    return {\n        'words': len(words),\n        'chars': len(text),\n        'avg_word_length': sum(len(w) for w in words) / len(words)\n    }\n```\n\n---\n\n## üéØ Your Task\n\nCount words and calculate text statistics.",
    "starter_code": "from collections import Counter\n\ntext = '''Python is a great programming language.\nPython is easy to learn. Python is powerful and versatile.\nLearning Python opens many career opportunities.'''\n\n# Basic word count\nwords = text.lower().split()\nword_count = len(words)\n\n# Unique words\nunique_words = len(set(words))\n\n# Word frequency\nword_freq = Counter(words)\ntop_5 = word_freq.most_common(5)\n\nprint(f\"Total words: {word_count}\")\nprint(f\"Unique words: {unique_words}\")\nprint(f\"\\nTop 5 most common words:\")\nfor word, count in top_5:\n    print(f\"  '{word}': {count} times\")",
    "solution_code": "text = \"hello world hello python world\"\nwords = text.lower().split()\nprint(f\"Count: {len(words)}\")\nprint(f\"Unique: {len(set(words))}\")",
    "expected_output": "Total words: 23\nUnique words: 14\n\nTop 5 most common words:\n  'python': 4 times\n  'is': 3 times"
  },
  "57": {
    "title": "Search in Text",
    "chapter_title": "Strings",
    "content": "# üîç Searching Text: Find What You Need\n\n## Quick Containment Check\n\nUse `in` operator for simple checks:\n\n```python\ntext = \"Hello World\"\n\"World\" in text  # True\n\"world\" in text  # False (case-sensitive!)\n\"Mondo\" in text  # False\n```\n\n## Finding Position with find()\n\nReturns the index of first occurrence, or -1 if not found:\n\n```python\ntext = \"Hello World\"\ntext.find(\"World\")  # 6\ntext.find(\"o\")      # 4 (first 'o')\ntext.find(\"xyz\")    # -1 (not found)\n```\n\n## Finding from the End: rfind()\n\n```python\ntext = \"Hello World\"\ntext.rfind(\"o\")  # 7 (last 'o')\n```\n\n## Counting Occurrences\n\n```python\ntext = \"banana\"\ntext.count(\"a\")   # 3\ntext.count(\"an\")  # 2\n```\n\n## Starts/Ends With\n\n```python\nfilename = \"data.csv\"\nfilename.startswith(\"data\")  # True\nfilename.endswith(\".csv\")    # True\nfilename.endswith(\".txt\")    # False\n```\n\n## Case-Insensitive Search\n\n```python\ntext = \"Hello World\"\n\"world\" in text.lower()  # True\n```\n\n## Real-World Example\n\n```python\ndef find_emails(text):\n    '''Find email-like patterns'''\n    words = text.split()\n    return [w for w in words if \"@\" in w and \".\" in w]\n```\n\n---\n\n## üéØ Your Task\n\nUse various search methods to analyze text content.",
    "starter_code": "text = '''Welcome to Python Programming!\nPython is a great language for data science.\nLearn Python today and boost your career.\nPython makes coding fun and productive.'''\n\n# Check if keyword exists\nkeyword = \"Python\"\nfound = keyword in text\nprint(f\"Contains '{keyword}': {found}\")\n\n# Count occurrences\ncount = text.count(\"Python\")\nprint(f\"Count of 'Python': {count}\")\n\n# Find first and last position\nfirst_pos = text.find(\"Python\")\nlast_pos = text.rfind(\"Python\")\nprint(f\"First 'Python' at index: {first_pos}\")\nprint(f\"Last 'Python' at index: {last_pos}\")\n\n# Check starts/ends\nstarts_with_welcome = text.startswith(\"Welcome\")\nends_with_period = text.endswith(\".\")\nprint(f\"Starts with 'Welcome': {starts_with_welcome}\")\nprint(f\"Ends with '.': {ends_with_period}\")",
    "solution_code": "text = \"Hello World Python\"\nprint(\"Python\" in text)  # True\nprint(text.find(\"World\"))  # 6\nprint(text.count(\"o\"))  # 3",
    "expected_output": "Contains 'Python': True\nCount of 'Python': 4\nFirst 'Python' at index: 11\nLast 'Python' at index: 124"
  },
  "58": {
    "title": "Replace Text",
    "chapter_title": "Strings",
    "content": "# üîÑ Text Replacement: Find and Replace Made Easy\n\n## The replace() Method\n\nFind all occurrences and replace them:\n\n```python\ntext = \"Hello World\"\nnew_text = text.replace(\"World\", \"Python\")\n# \"Hello Python\"\n```\n\n## Multiple Replacements\n\nReplace returns a new string, so you can chain:\n\n```python\nmessy = \"Hello...World!!!\"\nclean = messy.replace(\".\", \"\").replace(\"!\", \"\")\n# \"HelloWorld\"\n```\n\nOr use a loop:\n\n```python\ntext = \"I like apples and apple pie\"\nreplacements = {\"apples\": \"oranges\", \"apple\": \"orange\"}\nfor old, new in replacements.items():\n    text = text.replace(old, new)\n```\n\n## Replace Only N Occurrences\n\n```python\ntext = \"one two one two one\"\ntext.replace(\"one\", \"1\", 1)  # Replace only first\n# \"1 two one two one\"\n```\n\n## Case-Sensitive!\n\n```python\ntext = \"Hello HELLO hello\"\ntext.replace(\"hello\", \"hi\")  # Only replaces lowercase\n# \"Hello HELLO hi\"\n```\n\nFor case-insensitive, use regex:\n```python\nimport re\nre.sub(\"hello\", \"hi\", text, flags=re.IGNORECASE)\n# \"hi hi hi\"\n```\n\n## Common Use Cases\n\n```python\n# Clean phone numbers\nphone = \"(555) 123-4567\".replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\" \", \"\")\n\n# Sanitize user input\nusername = username.replace(\" \", \"_\")\n\n# Template replacement\ntemplate = \"Hello {name}, welcome to {site}!\"\nmessage = template.replace(\"{name}\", \"Alice\").replace(\"{site}\", \"Python Academy\")\n```\n\n---\n\n## üéØ Your Task\n\nUse replace() to clean and transform text data.",
    "starter_code": "# Clean up messy text\nmessy_text = \"Hello...World!!! This is ***Python*** programming.\"\n\n# Remove unwanted characters\nclean = messy_text.replace(\".\", \" \").replace(\"!\", \"\").replace(\"*\", \"\")\nprint(f\"Cleaned: {clean}\")\n\n# Template replacement\ntemplate = \"Dear {name}, your order #{order_id} has shipped!\"\nmessage = template.replace(\"{name}\", \"Alice\").replace(\"{order_id}\", \"12345\")\nprint(f\"Message: {message}\")\n\n# Standardize data\ndata = \"apple, BANANA, Cherry\"\nstandardized = data.replace(\", \", \"|\").lower()\nprint(f\"Standardized: {standardized}\")",
    "solution_code": "text = \"Hello World\"\nnew = text.replace(\"World\", \"Python\")\nprint(new)  # \"Hello Python\" ",
    "expected_output": "Cleaned: Hello   World    This is  Python  programming \nMessage: Dear Alice, your order #12345 has shipped!\nStandardized: apple|banana|cherry"
  },
  "59": {
    "id": 59,
    "title": "Importing Modules",
    "content": "# üì¶ Importing Modules\n\n## What is a Module?\n\nA **module** is a file containing Python code (functions, classes, variables) that you can reuse in other programs. Think of it as a toolbox of pre-written code.\n\n## Why Use Modules?\n\n| Benefit | Description |\n| --- | --- |\n| **Don't reinvent the wheel** | Experts have already written tested code |\n| **Organization** | Split large programs into manageable files |\n| **Reusability** | Write once, use everywhere |\n| **Community** | Access thousands of open-source packages |\n\n## Built-in Modules\n\nPython comes with many useful modules:\n\n```python\nimport math\nprint(math.sqrt(16))  # 4.0\nprint(math.pi)        # 3.14159...\nprint(math.floor(3.7))  # 3\nprint(math.ceil(3.2))   # 4\n```\n\n## How Import Works\n\nWhen you write `import math`:\n1. Python finds the module file\n2. Runs the code once\n3. Creates a namespace `math` with all its contents\n4. You access items with `math.something`\n\n---\n\n## üéØ Your Task\n\nImport the `math` module and print the square root of `25`.\n",
    "starter_code": "# Import math\n\n\n# Print sqrt of 25\n",
    "solution_code": "# Import math\nimport math\n\n# Print sqrt of 25\nprint(math.sqrt(25))",
    "expected_output": "5.0",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "60": {
    "id": 60,
    "title": "From Import",
    "content": "# üéØ Specific Imports\n\n## Import Only What You Need\n\nInstead of importing the entire module, import specific items:\n\n```python\nfrom math import pi, sqrt\n\n# Now use directly - no math. prefix needed!\nprint(pi)       # 3.14159...\nprint(sqrt(16)) # 4.0\n```\n\n## Comparison\n\n| Style | Syntax | Usage |\n| --- | --- | --- |\n| Full import | `import math` | `math.sqrt(16)` |\n| Specific import | `from math import sqrt` | `sqrt(16)` |\n\n## Import All (Use Carefully!)\n\n```python\nfrom math import *  # Imports EVERYTHING\n```\n\n‚ö†Ô∏è **Warning**: This can cause naming conflicts if two modules have functions with the same name!\n\n## Best Practices\n\n```python\n# Good - explicit about what you're using\nfrom math import pi, sqrt, floor\n\n# Good - clear namespace\nimport math\n\n# Risky - unclear what's available\nfrom math import *\n```\n\n---\n\n## üéØ Your Task\n\nFrom the `math` module, import `pi` and print it rounded to 2 decimal places.\n",
    "starter_code": "# Import pi from math\n\n\n# Print rounded to 2 decimals\n",
    "solution_code": "# Import pi from math\nfrom math import pi\n\n# Print rounded to 2 decimals\nprint(round(pi, 2))",
    "expected_output": "3.14",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "61": {
    "id": 61,
    "title": "Random Module",
    "content": "# üé≤ Random Numbers\n\n## The random Module\n\nGenerate random values for games, simulations, testing, and more!\n\n```python\nimport random\n\nrandom.randint(1, 10)           # Random integer 1-10 (inclusive)\nrandom.random()                  # Random float 0.0 to 1.0\nrandom.choice(['a', 'b', 'c'])  # Random pick from list\nrandom.shuffle(my_list)          # Shuffle list in place\nrandom.sample(my_list, 3)        # Pick 3 random items\n```\n\n## Real-World Uses\n\n- Games: dice rolls, card dealing, enemy spawns\n- Testing: generating test data\n- Statistics: sampling data\n- Security: generating tokens (use `secrets` module for true security)\n\n## Reproducibility with Seeds\n\nFor testing, you often need the same \"random\" results:\n\n```python\nrandom.seed(42)  # Set the seed\nprint(random.randint(1, 100))  # Always 82 with seed 42!\nprint(random.randint(1, 100))  # Always 35 with seed 42!\n```\n\nResetting the seed gives the same sequence every time.\n\n---\n\n## üéØ Your Task\n\nSet `random.seed(42)`, then generate and print a random integer between 1 and 100 (inclusive).\n",
    "starter_code": "import random\n\n# Set seed for consistency\nrandom.seed(42)\n\n# Print random 1-100\n",
    "solution_code": "import random\n\n# Set seed for consistency\nrandom.seed(42)\n\n# Print random 1-100\nprint(random.randint(1, 100))",
    "expected_output": "82",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "62": {
    "id": 62,
    "title": "Datetime Module",
    "content": "# üìÖ Working with Dates and Times\n\n## The datetime Module\n\nHandle dates, times, and durations:\n\n```python\nfrom datetime import date, datetime, timedelta\n\n# Current date/time\ntoday = date.today()      # 2024-01-15\nnow = datetime.now()      # 2024-01-15 14:30:00\n\n# Create specific dates\nbirthday = date(1995, 6, 15)\nmeeting = datetime(2024, 12, 25, 10, 30)\n```\n\n## Date Formatting\n\nConvert dates to custom string formats:\n\n```python\nnow = datetime.now()\nnow.strftime(\"%Y-%m-%d\")     # \"2024-01-15\"\nnow.strftime(\"%B %d, %Y\")    # \"January 15, 2024\"\nnow.strftime(\"%H:%M:%S\")     # \"14:30:00\"\n```\n\n## Date Arithmetic\n\nAdd or subtract time using `timedelta`:\n\n```python\nfrom datetime import timedelta\n\ntoday = date.today()\ntomorrow = today + timedelta(days=1)\nnext_week = today + timedelta(weeks=1)\n```\n\n---\n\n## üéØ Your Task\n\nCreate a date object for January 1, 2024 using `date(2024, 1, 1)` and print it.\n",
    "starter_code": "from datetime import date\n\n# Create Jan 1, 2024\n\n\n# Print it\n",
    "solution_code": "from datetime import date\n\n# Create Jan 1, 2024\nnew_year = date(2024, 1, 1)\n\n# Print it\nprint(new_year)",
    "expected_output": "2024-01-01",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "63": {
    "id": 63,
    "title": "Collections Module",
    "content": "# üìä Counter from Collections\n\n## The Counter Class\n\n`Counter` is a specialized dictionary for counting things:\n\n```python\nfrom collections import Counter\n\n# Count items in a list\ncolors = ['red', 'blue', 'red', 'green', 'blue', 'red']\ncounts = Counter(colors)\nprint(counts)  # Counter({'red': 3, 'blue': 2, 'green': 1})\n\n# Count characters in a string\nletter_counts = Counter(\"mississippi\")\n# Counter({'i': 4, 's': 4, 'p': 2, 'm': 1})\n```\n\n## Useful Counter Methods\n\n```python\ncounts.most_common(2)      # [('red', 3), ('blue', 2)]\ncounts['red']              # 3\ncounts['purple']           # 0 (no KeyError!)\ncounts.total()             # Sum of all counts\n```\n\n## Why Use Counter?\n\n- Count word frequencies in text\n- Find most common items\n- Tally votes or scores\n- Analyze data distributions\n\n---\n\n## üéØ Your Task\n\nCount the letters in the word `\"hello\"` using Counter and print the result.\n",
    "starter_code": "from collections import Counter\n\nword = \"hello\"\n\n# Count and print\n",
    "solution_code": "from collections import Counter\n\nword = \"hello\"\n\n# Count and print\ncounts = Counter(word)\nprint(counts)",
    "expected_output": "Counter({'l': 2, 'h': 1, 'e': 1, 'o': 1})",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "64": {
    "title": "Aliases",
    "chapter_title": "Modules",
    "content": "# üìõ Module Aliases: Shorter Names for Convenience\n\n## Why Use Aliases?\n\nSome module names are long. Aliases make code cleaner:\n\n```python\n# Without alias\nimport matplotlib.pyplot\nmatplotlib.pyplot.plot(x, y)\n\n# With alias\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\n```\n\n## Common Conventions\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n```\n\nThese are **standard conventions**‚Äîeveryone uses them!\n\n## The `as` Keyword\n\n```python\nimport module_name as alias\n\n# Now use alias instead of module_name\nalias.function()\n```\n\n## Function/Class Aliases\n\n```python\nfrom collections import Counter as C\ncounts = C(['a', 'b', 'a'])\n\nfrom datetime import datetime as dt\nnow = dt.now()\n```\n\n## Why Conventions Matter\n\n```python\n# Everyone understands this:\ndf = pd.read_csv('data.csv')\nplt.plot(df['x'], df['y'])\n\n# But this is confusing:\npizza = pd.read_csv('data.csv')  # Pizza??\n```\n\n## Creating Your Own Aliases\n\n```python\n# For your own modules\nimport my_long_module_name as mlm\n\n# For long submodules\nfrom sklearn.preprocessing import StandardScaler as SS\n```\n\n---\n\n## üéØ Your Task\n\nPractice using module aliases with common data science libraries.",
    "starter_code": "# Standard aliases\nimport numpy as np\nimport random as rnd\n\n# Create data with aliases\ndata = np.array([1, 2, 3, 4, 5])\nprint(f\"NumPy array: {data}\")\nprint(f\"Mean: {np.mean(data)}\")\nprint(f\"Sum: {np.sum(data)}\")\n\n# Random with alias\nrnd.seed(42)\nsample = rnd.sample(range(100), 5)\nprint(f\"\\nRandom sample: {sample}\")\n\n# Custom alias\nfrom collections import Counter as cnt\nwords = ['apple', 'banana', 'apple', 'cherry']\nword_counts = cnt(words)\nprint(f\"\\nWord counts: {dict(word_counts)}\")",
    "solution_code": "import numpy as np\nimport pandas as pd\n\ndata = np.array([1, 2, 3])\nprint(np.mean(data))",
    "expected_output": "NumPy array: [1 2 3 4 5]\nMean: 3.0\nSum: 15"
  },
  "65": {
    "id": 65,
    "title": "OS Path",
    "content": "# üìÇ OS Path Operations\n\n## Working with File Paths\n\nThe `os` module helps work with file paths in a **cross-platform** way:\n\n```python\nimport os\n\n# Join paths (works on Windows AND Mac/Linux)\npath = os.path.join(\"folder\", \"subfolder\", \"file.txt\")\n# Windows: folder\\subfolder\\file.txt\n# Mac/Linux: folder/subfolder/file.txt\n```\n\n## Common Path Operations\n\n```python\nimport os\n\n# Check if exists\nos.path.exists(\"file.txt\")     # True/False\n\n# Get parts of a path\nos.path.dirname(\"/a/b/c.txt\")  # \"/a/b\"\nos.path.basename(\"/a/b/c.txt\") # \"c.txt\"\nos.path.splitext(\"data.csv\")   # (\"data\", \".csv\")\n\n# Get current directory\nos.getcwd()\n\n# List files in directory\nos.listdir(\".\")\n```\n\n## Why Use os.path?\n\nNever hardcode paths like `\"folder/file.txt\"` because:\n- Windows uses `\\`, Mac/Linux use `/`\n- `os.path.join()` handles this automatically!\n\n---\n\n## üéØ Your Task\n\nUse `os.path.join()` to combine `\"data\"` and `\"report.csv\"` into a path, then print it.\n",
    "starter_code": "import os\n\n# Join path parts\n\n\n# Print path\n",
    "solution_code": "import os\n\n# Join path parts\npath = os.path.join(\"data\", \"report.csv\")\n\n# Print path\nprint(path)",
    "expected_output": "data/report.csv",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "66": {
    "id": 66,
    "title": "Itertools",
    "content": "# üîÑ Itertools Module\n\n## Powerful Iteration Tools\n\nThe `itertools` module provides advanced iteration utilities:\n\n```python\nfrom itertools import combinations, permutations, product\n\n# Combinations: order doesn't matter\nlist(combinations([1, 2, 3], 2))\n# [(1, 2), (1, 3), (2, 3)]\n\n# Permutations: order matters\nlist(permutations([1, 2, 3], 2))\n# [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]\n\n# Product: all combinations of multiple lists\nlist(product(['A', 'B'], [1, 2]))\n# [('A', 1), ('A', 2), ('B', 1), ('B', 2)]\n```\n\n## Other Useful Functions\n\n```python\nfrom itertools import cycle, count, chain\n\n# Infinite repeating\ncolors = cycle(['red', 'green', 'blue'])\nnext(colors)  # 'red', 'green', 'blue', 'red'...\n\n# Chain multiple iterables\nlist(chain([1, 2], [3, 4]))  # [1, 2, 3, 4]\n```\n\n---\n\n## üéØ Your Task\n\nGet all combinations of 2 items from the list `[\"A\", \"B\", \"C\"]` and print the result.\n",
    "starter_code": "from itertools import combinations\n\nletters = [\"A\", \"B\", \"C\"]\n\n# Get pairs and print\n",
    "solution_code": "from itertools import combinations\n\nletters = [\"A\", \"B\", \"C\"]\n\n# Get pairs and print\npairs = list(combinations(letters, 2))\nprint(pairs)",
    "expected_output": "[('A', 'B'), ('A', 'C'), ('B', 'C')]",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "67": {
    "id": 67,
    "title": "Functools",
    "content": "# ‚öôÔ∏è Functools Module\n\n## Functions That Work with Functions\n\n`functools` provides higher-order functions - functions that work on other functions.\n\n## The reduce Function\n\nApplies a function cumulatively to reduce a list to a single value:\n\n```python\nfrom functools import reduce\n\n# Sum all numbers: ((1+2)+3)+4 = 10\nresult = reduce(lambda x, y: x + y, [1, 2, 3, 4])\n\n# Find maximum\nmax_val = reduce(lambda x, y: x if x > y else y, [3, 1, 4, 1, 5])\n# Result: 5\n```\n\n## How reduce Works Step by Step\n\n```python\nreduce(lambda x, y: x * y, [2, 3, 4, 5])\n# Step 1: 2 * 3 = 6\n# Step 2: 6 * 4 = 24\n# Step 3: 24 * 5 = 120\n```\n\n## Other Useful Functions\n\n```python\nfrom functools import partial\n\n# Create a specialized function\ndef power(base, exp):\n    return base ** exp\n\nsquare = partial(power, exp=2)\ncube = partial(power, exp=3)\n\nprint(square(5))  # 25\nprint(cube(5))    # 125\n```\n\n---\n\n## üéØ Your Task\n\nUse `reduce` with a lambda to multiply all numbers in `[2, 3, 4]` together. Print the result (should be 24).\n",
    "starter_code": "from functools import reduce\n\nnumbers = [2, 3, 4]\n\n# Multiply all and print\n",
    "solution_code": "from functools import reduce\n\nnumbers = [2, 3, 4]\n\n# Multiply all and print\nresult = reduce(lambda x, y: x * y, numbers)\nprint(result)",
    "expected_output": "24",
    "chapter_id": 9,
    "chapter_title": "Modules & Packages"
  },
  "68": {
    "id": 68,
    "title": "Creating DataFrames",
    "content": "# üêº Pandas DataFrames\n\n## What is Pandas?\n\n**Pandas** is Python's premier library for data analysis. It provides powerful tools for working with tabular data (rows and columns).\n\n## What is a DataFrame?\n\nA **DataFrame** is a 2D table with labeled rows and columns - like a spreadsheet or SQL table.\n\n![Pandas DataFrame](/assets/python-diagrams/dataframe_basic.png)\n\n```python\nimport pandas as pd\n\ndata = {\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"NYC\", \"LA\", \"Chicago\"]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n```\n\nOutput:\n```\n      name  age     city\n0    Alice   25      NYC\n1      Bob   30       LA\n2  Charlie   35  Chicago\n```\n\n## Why Pandas?\n\n| Feature | Benefit |\n| --- | --- |\n| Handles millions of rows | Process big data efficiently |\n| Built-in data cleaning | Handle missing values, duplicates |\n| Powerful grouping | Aggregate and summarize data |\n| File I/O | Read CSV, Excel, JSON, SQL |\n\n---\n\n## üéØ Your Task\n\nCreate a DataFrame from `{\"name\": [\"Alice\", \"Bob\"], \"score\": [85, 90]}` and print it.\n",
    "starter_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\"], \"score\": [85, 90]}\n\n# Create DataFrame\n\n\n# Print it\n",
    "solution_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\"], \"score\": [85, 90]}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Print it\nprint(df)",
    "expected_output": "    name  score\n0  Alice     85\n1    Bob     90",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "69": {
    "id": 69,
    "title": "Selecting Columns",
    "content": "# üìä Selecting Data from DataFrames\n\n## Accessing Columns\n\nThere are several ways to select data:\n\n```python\n# Single column ‚Üí returns a Series\ndf[\"name\"]\ndf.name  # Dot notation (if column name is valid identifier)\n\n# Multiple columns ‚Üí returns a DataFrame\ndf[[\"name\", \"age\"]]\n```\n\n## Accessing Rows\n\n```python\n# By position (integer index)\ndf.iloc[0]       # First row\ndf.iloc[0:3]     # First 3 rows\ndf.iloc[-1]      # Last row\n\n# By label\ndf.loc[0]        # Row with label 0\ndf.loc[0:2]      # Rows with labels 0 through 2 (inclusive!)\n\n# Handy shortcuts\ndf.head(3)       # First 3 rows\ndf.tail(2)       # Last 2 rows\n```\n\n## Selecting Both Rows and Columns\n\n```python\n# Specific rows and columns\ndf.loc[0:2, [\"name\", \"age\"]]\ndf.iloc[0:3, 0:2]\n```\n\n---\n\n## üéØ Your Task\n\nGiven a DataFrame with name, age, and city columns, print only the \"name\" column.\n",
    "starter_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30], \"city\": [\"NYC\", \"LA\"]}\ndf = pd.DataFrame(data)\n\n# Print name column\n",
    "solution_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30], \"city\": [\"NYC\", \"LA\"]}\ndf = pd.DataFrame(data)\n\n# Print name column\nprint(df[\"name\"])",
    "expected_output": "0    Alice\n1      Bob\nName: name, dtype: object",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "70": {
    "id": 70,
    "title": "Filtering Rows",
    "content": "# üîç Filtering Data\n\n## Boolean Indexing\n\nFilter rows based on conditions:\n\n```python\n# Rows where age > 25\ndf[df[\"age\"] > 25]\n\n# Rows where name is \"Alice\"\ndf[df[\"name\"] == \"Alice\"]\n\n# Rows where score is between 80 and 90\ndf[(df[\"score\"] >= 80) & (df[\"score\"] <= 90)]\n```\n\n## Multiple Conditions\n\nUse `&` for AND, `|` for OR (wrap each condition in parentheses):\n\n```python\n# AND: both conditions must be true\ndf[(df[\"age\"] > 25) & (df[\"city\"] == \"NYC\")]\n\n# OR: at least one must be true\ndf[(df[\"age\"] > 40) | (df[\"score\"] > 90)]\n```\n\n## Filtering with isin()\n\n```python\n# Check if value is in a list\ndf[df[\"city\"].isin([\"NYC\", \"LA\"])]\n```\n\n## Filtering String Columns\n\n```python\ndf[df[\"name\"].str.contains(\"li\")]     # Contains \"li\"\ndf[df[\"name\"].str.startswith(\"A\")]    # Starts with \"A\"\n```\n\n---\n\n## üéØ Your Task\n\nFilter rows where score > 80 and print the result.\n",
    "starter_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"score\": [85, 70, 95]}\ndf = pd.DataFrame(data)\n\n# Filter score > 80\n",
    "solution_code": "import pandas as pd\n\ndata = {\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"score\": [85, 70, 95]}\ndf = pd.DataFrame(data)\n\n# Filter score > 80\nprint(df[df[\"score\"] > 80])",
    "expected_output": "      name  score\n0    Alice     85\n2  Charlie     95",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "71": {
    "id": 71,
    "title": "Basic Statistics",
    "content": "# üìà Statistics in Pandas\n\n## Quick Column Statistics\n\nPandas makes statistical analysis easy:\n\n```python\ndf[\"column\"].mean()     # Average\ndf[\"column\"].median()   # Middle value\ndf[\"column\"].sum()      # Total\ndf[\"column\"].min()      # Minimum\ndf[\"column\"].max()      # Maximum\ndf[\"column\"].std()      # Standard deviation\ndf[\"column\"].count()    # Count non-null values\n```\n\n## Get All Stats at Once\n\n```python\ndf.describe()\n```\n\nReturns count, mean, std, min, 25%, 50%, 75%, max for all numeric columns!\n\n## Statistics Across the DataFrame\n\n```python\ndf.mean()         # Mean of each column\ndf.sum(axis=1)    # Sum across each row\n```\n\n## Value Distribution\n\n```python\ndf[\"column\"].value_counts()   # Count each unique value\ndf[\"column\"].unique()         # Get unique values\ndf[\"column\"].nunique()        # Count unique values\n```\n\n---\n\n## üéØ Your Task\n\nCalculate and print the mean of the \"value\" column.\n",
    "starter_code": "import pandas as pd\n\ndata = {\"value\": [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Print mean\n",
    "solution_code": "import pandas as pd\n\ndata = {\"value\": [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Print mean\nprint(df[\"value\"].mean())",
    "expected_output": "30.0",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "72": {
    "id": 72,
    "title": "Adding Columns",
    "content": "# ‚ûï Creating New Columns\n\n## Calculate from Existing Columns\n\nCreate new columns based on existing data:\n\n```python\n# Simple calculation\ndf[\"total\"] = df[\"price\"] * df[\"quantity\"]\n\n# Using multiple columns\ndf[\"profit\"] = df[\"revenue\"] - df[\"cost\"]\n\n# Apply a function\ndf[\"age_in_months\"] = df[\"age\"] * 12\n```\n\n## Transform String Columns\n\n```python\ndf[\"name_upper\"] = df[\"name\"].str.upper()\ndf[\"name_length\"] = df[\"name\"].str.len()\ndf[\"first_initial\"] = df[\"name\"].str[0]\n```\n\n## Conditional Columns\n\n```python\n# Using apply with lambda\ndf[\"status\"] = df[\"score\"].apply(lambda x: \"Pass\" if x >= 70 else \"Fail\")\n\n# Using numpy where\nimport numpy as np\ndf[\"status\"] = np.where(df[\"score\"] >= 70, \"Pass\", \"Fail\")\n```\n\n## Modify Existing Columns\n\n```python\ndf[\"price\"] = df[\"price\"] * 1.10  # 10% increase\n```\n\n---\n\n## üéØ Your Task\n\nAdd a \"total\" column that equals price √ó quantity, then print the DataFrame.\n",
    "starter_code": "import pandas as pd\n\ndata = {\"price\": [100, 200], \"quantity\": [2, 3]}\ndf = pd.DataFrame(data)\n\n# Add total column\n\n\n# Print df\n",
    "solution_code": "import pandas as pd\n\ndata = {\"price\": [100, 200], \"quantity\": [2, 3]}\ndf = pd.DataFrame(data)\n\n# Add total column\ndf[\"total\"] = df[\"price\"] * df[\"quantity\"]\n\n# Print df\nprint(df)",
    "expected_output": "   price  quantity  total\n0    100         2    200\n1    200         3    600",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "73": {
    "title": "GroupBy",
    "chapter_title": "Pandas",
    "content": "# üî¢ GroupBy: Aggregate Data by Category\n\n## What is GroupBy?\n\nSplit data into groups, apply a function, combine results:\n\n```python\ndf.groupby('category')['sales'].sum()\n```\n\n\"Give me total sales for each category\"\n\n## The Split-Apply-Combine Pattern\n\n```\nOriginal Data ‚Üí Split by Category ‚Üí Apply Sum ‚Üí Combine Results\n```\n\n## Basic Usage\n\n```python\n# Sum by category\ndf.groupby('category')['amount'].sum()\n\n# Mean by category\ndf.groupby('category')['amount'].mean()\n\n# Count by category\ndf.groupby('category').size()\n```\n\n## Multiple Aggregations\n\n```python\ndf.groupby('category')['amount'].agg(['sum', 'mean', 'count'])\n```\n\n## Multiple Columns\n\n```python\ndf.groupby('category').agg({\n    'amount': 'sum',\n    'quantity': 'mean',\n    'order_id': 'count'\n})\n```\n\n## Group by Multiple Columns\n\n```python\ndf.groupby(['category', 'region'])['sales'].sum()\n```\n\n## Common Patterns\n\n```python\n# Revenue per customer\ndf.groupby('customer_id')['amount'].sum()\n\n# Average rating per product\ndf.groupby('product')['rating'].mean()\n\n# Orders per day\ndf.groupby('date')['order_id'].count()\n```\n\n---\n\n## üéØ Your Task\n\nUse groupby to aggregate data by categories.",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'department': ['Sales', 'Sales', 'IT', 'IT', 'HR', 'HR'],\n    'employee': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n    'salary': [60000, 55000, 80000, 75000, 50000, 52000]\n})\n\nprint(\"Original data:\")\nprint(df)\n\n# Sum salary by department\ntotal_by_dept = df.groupby('department')['salary'].sum()\nprint(\"\\nTotal salary by department:\")\nprint(total_by_dept)\n\n# Average salary by department\navg_by_dept = df.groupby('department')['salary'].mean()\nprint(\"\\nAverage salary by department:\")\nprint(avg_by_dept)\n\n# Multiple aggregations\nsummary = df.groupby('department')['salary'].agg(['sum', 'mean', 'count'])\nprint(\"\\nFull summary:\")\nprint(summary)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({'dept': ['A', 'A', 'B'], 'sales': [100, 150, 200]})\nprint(df.groupby('dept')['sales'].sum())",
    "expected_output": "Total salary by department:\ndepartment\nHR        102000\nIT        155000\nSales     115000"
  },
  "74": {
    "title": "Sorting",
    "chapter_title": "Pandas",
    "content": "# üìà Sorting DataFrames: Order Your Data\n\n## Why Sort?\n\nSorting helps you:\n- Find top/bottom values\n- Organize data logically\n- Prepare for deduplication\n\n## Sort by Column\n\n```python\n# Ascending (default)\ndf.sort_values('price')\n\n# Descending\ndf.sort_values('price', ascending=False)\n```\n\n## Sort by Multiple Columns\n\n```python\n# Sort by category, then by price within each category\ndf.sort_values(['category', 'price'], ascending=[True, False])\n```\n\n## Sort by Index\n\n```python\ndf.sort_index()  # Sort by row index\ndf.sort_index(ascending=False)  # Descending\n```\n\n## Sorting Options\n\n```python\n# Modify original DataFrame\ndf.sort_values('price', inplace=True)\n\n# Put NaN values first/last\ndf.sort_values('price', na_position='first')\n\n# Ignore index (reset after sort)\ndf.sort_values('price', ignore_index=True)\n```\n\n## Common Patterns\n\n```python\n# Top 10 by sales\ndf.sort_values('sales', ascending=False).head(10)\n\n# Bottom 5 ratings\ndf.sort_values('rating').head(5)\n\n# Latest orders\ndf.sort_values('date', ascending=False)\n```\n\n---\n\n## üéØ Your Task\n\nSort a DataFrame by various columns and combinations.",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['A', 'B', 'C', 'D', 'E'],\n    'price': [50, 30, 45, 60, 25],\n    'sales': [100, 250, 150, 75, 300]\n})\n\nprint(\"Original:\")\nprint(df)\n\n# Sort by price (ascending)\nby_price = df.sort_values('price')\nprint(\"\\nBy price (low to high):\")\nprint(by_price)\n\n# Sort by sales (descending) - top sellers\nby_sales = df.sort_values('sales', ascending=False)\nprint(\"\\nTop sellers:\")\nprint(by_sales)\n\n# Top 3 by sales\ntop_3 = df.sort_values('sales', ascending=False).head(3)\nprint(\"\\nTop 3:\")\nprint(top_3)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({'name': ['A', 'B', 'C'], 'price': [30, 10, 20]})\nprint(df.sort_values('price'))",
    "expected_output": "By price (low to high):\n  product  price  sales\n4       E     25    300\n1       B     30    250\n2       C     45    150"
  },
  "75": {
    "id": 75,
    "title": "Handling Missing Data",
    "content": "# ‚ùì Handling Missing Values\n\n## Detecting Missing Data\n\nIn Pandas, missing values are represented as `NaN` (Not a Number):\n\n```python\nimport numpy as np\n\ndf.isna()           # True where NaN\ndf.notna()          # True where NOT NaN\ndf.isna().sum()     # Count NaN per column\ndf.isna().any()     # Any NaN in each column?\n```\n\n## Dealing with Missing Data\n\n```python\n# Drop rows with any NaN\ndf.dropna()\n\n# Drop rows where specific column is NaN\ndf.dropna(subset=[\"name\"])\n\n# Fill NaN with a value\ndf.fillna(0)\ndf.fillna(df.mean())  # Fill with column mean\ndf.fillna(method=\"ffill\")  # Forward fill\n```\n\n## Real-World Approach\n\nDecide based on context:\n- **Drop**: If few missing and rows not critical\n- **Fill with value**: If you have a sensible default\n- **Fill with statistic**: Mean, median, mode for numeric data\n- **Keep as-is**: Some algorithms handle NaN\n\n---\n\n## üéØ Your Task\n\nFill NaN values with 0 and print the result.\n",
    "starter_code": "import pandas as pd\nimport numpy as np\n\ndata = {\"value\": [1, np.nan, 3]}\ndf = pd.DataFrame(data)\n\n# Fill NaN with 0\n",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndata = {\"value\": [1, np.nan, 3]}\ndf = pd.DataFrame(data)\n\n# Fill NaN with 0\nprint(df.fillna(0))",
    "expected_output": "   value\n0    1.0\n1    0.0\n2    3.0",
    "chapter_id": 10,
    "chapter_title": "Pandas & Data Wrangling"
  },
  "76": {
    "title": "Value Counts",
    "chapter_title": "Pandas",
    "content": "# üìä Value Counts: Frequency Analysis in Pandas\n\n## What Does value_counts() Do?\n\nCount how often each unique value appears in a column:\n\n```python\ndf['color'].value_counts()\n# red      45\n# blue     30\n# green    25\n```\n\n## Basic Usage\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'color': ['red', 'blue', 'red', 'red', 'blue']})\ndf['color'].value_counts()\n# red     3\n# blue    2\n```\n\n## Options\n\n```python\n# Normalize to percentages\ndf['color'].value_counts(normalize=True)\n# red     0.6 (60%)\n# blue    0.4 (40%)\n\n# Include NaN values\ndf['color'].value_counts(dropna=False)\n\n# Sort by index instead of counts\ndf['color'].value_counts().sort_index()\n```\n\n## With Binning\n\n```python\n# Group numbers into bins\ndf['age'].value_counts(bins=5)  # 5 equal-width bins\n```\n\n## Common Patterns\n\n```python\n# Top 5 most common\ndf['product'].value_counts().head(5)\n\n# Least common\ndf['product'].value_counts().tail(5)\n\n# Specific value count\ndf['status'].value_counts()['completed']\n```\n\n## Real-World Uses\n\n```python\n# Customer segmentation\ndf['tier'].value_counts()\n\n# Error analysis\ndf['error_code'].value_counts().head(10)\n```\n\n---\n\n## üéØ Your Task\n\nUse value_counts() to analyze category distributions.",
    "starter_code": "import pandas as pd\n\n# Sample data\ndf = pd.DataFrame({\n    'product': ['Widget', 'Gadget', 'Widget', 'Tool', 'Widget', 'Gadget', 'Tool', 'Widget'],\n    'status': ['sold', 'sold', 'returned', 'sold', 'sold', 'returned', 'sold', 'sold']\n})\n\n# Basic value counts\nprint(\"Product frequency:\")\nprint(df['product'].value_counts())\n\n# As percentages\nprint(\"\\nStatus distribution (%):\")\nprint(df['status'].value_counts(normalize=True).round(2))\n\n# Top product\ntop = df['product'].value_counts().index[0]\ncount = df['product'].value_counts().iloc[0]\nprint(f\"\\nTop product: {top} ({count} sales)\")",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({'color': ['red', 'blue', 'red', 'red']})\nprint(df['color'].value_counts())",
    "expected_output": "Product frequency:\nproduct\nWidget    4\nGadget    2\nTool      2\nName: count, dtype: int64"
  },
  "77": {
    "id": 77,
    "title": "Line Plot",
    "content": "# üìà Creating Line Charts\n\n## Why Visualize Data?\n\nVisualizations help us:\n- **Understand patterns** at a glance\n- **Communicate insights** to others\n- **Spot trends, outliers, and anomalies**\n- **Tell stories with data**\n\n## Matplotlib: Python's Plotting Library\n\n```python\nimport matplotlib.pyplot as plt\n\n# Create a simple line plot\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 9, 16, 25]  # Squares\n\nplt.plot(x, y)\nplt.show()\n```\n\n## How plt.plot() Works\n\n1. Create a figure and axes\n2. Plot the data points\n3. Connect them with lines\n4. `plt.show()` displays the result\n\n## When to Use Line Charts\n\n- **Time series data** (stock prices over time)\n- **Trends** (website traffic by month)\n- **Continuous data** where order matters\n\n---\n\n## üéØ Your Task\n\nCreate a line plot with x values `[0, 1, 2, 3, 4]` and y values `[0, 1, 4, 9, 16]` (squares).\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nx = [0, 1, 2, 3, 4]\ny = [0, 1, 4, 9, 16]\n\n# Create line plot and show\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nx = [0, 1, 2, 3, 4]\ny = [0, 1, 4, 9, 16]\n\n# Create line plot and show\nplt.plot(x, y)\nplt.show()",
    "expected_output": "[Graph: Line plot showing squares 0-16]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "78": {
    "id": 78,
    "title": "Bar Chart",
    "content": "# üìä Bar Charts\n\n## When to Use Bar Charts\n\nBar charts are perfect for:\n- **Comparing categories** (sales by region)\n- **Showing quantities** across groups\n- **Discrete, categorical data**\n\n## Creating a Bar Chart\n\n```python\nimport matplotlib.pyplot as plt\n\ncategories = [\"Apples\", \"Oranges\", \"Bananas\"]\nvalues = [25, 40, 30]\n\nplt.bar(categories, values)\nplt.show()\n```\n\n## Customization Options\n\n```python\n# Horizontal bars\nplt.barh(categories, values)\n\n# Custom colors\nplt.bar(categories, values, color=['red', 'orange', 'yellow'])\n\n# Add border\nplt.bar(categories, values, edgecolor='black', linewidth=1)\n```\n\n## Bar Chart vs Line Chart\n\n| Bar Chart | Line Chart |\n| --- | --- |\n| Categorical data | Sequential data |\n| Comparing groups | Showing trends |\n| No inherent order | Order matters |\n\n---\n\n## üéØ Your Task\n\nCreate a bar chart with categories `[\"A\", \"B\", \"C\"]` and values `[25, 40, 30]`.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\ncategories = [\"A\", \"B\", \"C\"]\nvalues = [25, 40, 30]\n\n# Create bar chart\n",
    "solution_code": "import matplotlib.pyplot as plt\n\ncategories = [\"A\", \"B\", \"C\"]\nvalues = [25, 40, 30]\n\n# Create bar chart\nplt.bar(categories, values)\nplt.show()",
    "expected_output": "[Graph: Bar chart with A=25, B=40, C=30]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "79": {
    "id": 79,
    "title": "Scatter Plot",
    "content": "# ‚≠ê Scatter Plots\n\n## When to Use Scatter Plots\n\nScatter plots show the **relationship between two variables**:\n- Do taller people weigh more?\n- Does more advertising lead to more sales?\n- Is there a correlation?\n\n## Creating a Scatter Plot\n\n```python\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\nplt.scatter(x, y)\nplt.show()\n```\n\n## Customization\n\n```python\n# Size and color\nplt.scatter(x, y, s=100, c='red')\n\n# Different sizes per point\nsizes = [20, 50, 100, 200, 500]\nplt.scatter(x, y, s=sizes)\n\n# Color by a third variable\ncolors = [1, 2, 3, 4, 5]\nplt.scatter(x, y, c=colors, cmap='viridis')\nplt.colorbar()  # Add color legend\n```\n\n## Reading Scatter Plots\n\n- **Positive correlation**: Points trend up-right\n- **Negative correlation**: Points trend down-right\n- **No correlation**: Points randomly scattered\n\n---\n\n## üéØ Your Task\n\nCreate a scatter plot with x `[1, 2, 3, 4, 5]` and y `[2, 4, 5, 4, 5]`.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\n# Create scatter plot\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\n# Create scatter plot\nplt.scatter(x, y)\nplt.show()",
    "expected_output": "[Graph: Scatter plot with 5 points]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "80": {
    "id": 80,
    "title": "Histogram",
    "content": "# üìä Histograms\n\n## What is a Histogram?\n\nA histogram shows the **distribution** of data - how values are spread across ranges (bins).\n\n## Bar Chart vs Histogram\n\n| Bar Chart | Histogram |\n| --- | --- |\n| Categorical data | Continuous data |\n| Each bar is a category | Each bar is a range |\n| Bars have gaps | Bars touch |\n\n## Creating a Histogram\n\n```python\nimport matplotlib.pyplot as plt\n\nscores = [65, 70, 75, 80, 85, 90, 95, 70, 75, 80]\n\nplt.hist(scores, bins=5)\nplt.show()\n```\n\n## Bins: Dividing Your Data\n\nThe `bins` parameter controls how data is grouped:\n- More bins = more detail\n- Fewer bins = smoother view\n\n```python\nplt.hist(data, bins=10)   # Default\nplt.hist(data, bins=20)   # More detail\nplt.hist(data, bins=[0, 50, 70, 90, 100])  # Custom edges\n```\n\n## Reading Histograms\n\n- **Normal distribution**: Bell curve shape\n- **Skewed right**: Tail extends right\n- **Skewed left**: Tail extends left\n- **Uniform**: All bars similar height\n\n---\n\n## üéØ Your Task\n\nCreate a histogram of exam scores `[65, 70, 75, 80, 85, 90, 95, 70, 75, 80, 85, 80]` with 5 bins.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nscores = [65, 70, 75, 80, 85, 90, 95, 70, 75, 80, 85, 80]\n\n# Create histogram with 5 bins\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nscores = [65, 70, 75, 80, 85, 90, 95, 70, 75, 80, 85, 80]\n\n# Create histogram with 5 bins\nplt.hist(scores, bins=5)\nplt.show()",
    "expected_output": "[Graph: Histogram of score distribution]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "81": {
    "id": 81,
    "title": "Pie Chart",
    "content": "# ü•ß Pie Charts\n\n## When to Use Pie Charts\n\nPie charts show **parts of a whole**:\n- Market share percentages\n- Budget allocation\n- Survey response distribution\n\n## Creating a Pie Chart\n\n```python\nimport matplotlib.pyplot as plt\n\nlabels = [\"Rent\", \"Food\", \"Transport\", \"Entertainment\"]\nvalues = [30, 25, 20, 25]\n\nplt.pie(values, labels=labels)\nplt.show()\n```\n\n## Customization\n\n```python\n# Add percentages\nplt.pie(values, labels=labels, autopct='%1.1f%%')\n\n# Explode a slice\nexplode = [0.1, 0, 0, 0]  # First slice stands out\nplt.pie(values, labels=labels, explode=explode)\n\n# Custom colors\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\nplt.pie(values, labels=labels, colors=colors)\n\n# Make it a circle (equal aspect ratio)\nplt.axis('equal')\n```\n\n## Pie Chart Best Practices\n\n- Use for **5 or fewer categories** (too many is confusing)\n- Label clearly\n- Start largest slice at 12 o'clock\n- Consider bar charts for precise comparisons\n\n---\n\n## üéØ Your Task\n\nCreate a pie chart showing market share with labels `[\"Apple\", \"Google\", \"Microsoft\"]` and values `[30, 45, 25]`.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nlabels = [\"Apple\", \"Google\", \"Microsoft\"]\nvalues = [30, 45, 25]\n\n# Create pie chart\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nlabels = [\"Apple\", \"Google\", \"Microsoft\"]\nvalues = [30, 45, 25]\n\n# Create pie chart\nplt.pie(values, labels=labels)\nplt.show()",
    "expected_output": "[Graph: Pie chart with 3 segments]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "82": {
    "id": 82,
    "title": "Adding Labels",
    "content": "# üè∑Ô∏è Chart Labels and Titles\n\n## Professional Charts Need Labels\n\nA chart without labels is like a map without names - useless!\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3], [1, 4, 9])\n\n# Add labels and title\nplt.title(\"My Chart Title\")\nplt.xlabel(\"X Axis Label\")\nplt.ylabel(\"Y Axis Label\")\n\nplt.show()\n```\n\n## More Customization\n\n```python\n# Font sizes\nplt.title(\"Title\", fontsize=16, fontweight='bold')\nplt.xlabel(\"X Label\", fontsize=12)\nplt.ylabel(\"Y Label\", fontsize=12)\n\n# Add a grid\nplt.grid(True)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set axis limits\nplt.xlim(0, 10)\nplt.ylim(0, 100)\n\n# Add text annotation\nplt.text(x, y, \"Label here\")\n```\n\n## Complete Example\n\n```python\nplt.plot(days, temps)\nplt.title(\"Daily Temperature\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Temperature (¬∞C)\")\nplt.grid(True)\nplt.show()\n```\n\n---\n\n## üéØ Your Task\n\nPlot temperatures `[20, 22, 25, 23, 21]` over days 1-5. Add title \"Daily Temperature\" and labels \"Day\" and \"Temp (¬∞C)\".\n",
    "starter_code": "import matplotlib.pyplot as plt\n\ndays = [1, 2, 3, 4, 5]\ntemps = [20, 22, 25, 23, 21]\n\n# Create plot with labels\n",
    "solution_code": "import matplotlib.pyplot as plt\n\ndays = [1, 2, 3, 4, 5]\ntemps = [20, 22, 25, 23, 21]\n\n# Create plot with labels\nplt.plot(days, temps)\nplt.title(\"Daily Temperature\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Temp (¬∞C)\")\nplt.show()",
    "expected_output": "[Graph: Line plot with labeled axes]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "83": {
    "id": 83,
    "title": "Multiple Lines",
    "content": "# üìà Plotting Multiple Series\n\n## Comparing Multiple Datasets\n\nPlot multiple lines to compare trends:\n\n```python\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny1 = [1, 2, 3, 4]\ny2 = [1, 4, 9, 16]\n\nplt.plot(x, y1, label=\"Linear\")\nplt.plot(x, y2, label=\"Quadratic\")\nplt.legend()  # Show the legend!\nplt.show()\n```\n\n## The Legend\n\nThe `label` parameter names each line, and `plt.legend()` displays them:\n\n```python\nplt.plot(x, y1, label=\"Sales 2023\")\nplt.plot(x, y2, label=\"Sales 2024\")\nplt.legend()\n\n# Position the legend\nplt.legend(loc='upper left')\nplt.legend(loc='best')  # Auto-position\n```\n\n## Different Line Styles\n\n```python\nplt.plot(x, y1, 'b-', label=\"Solid blue\")   # blue, solid\nplt.plot(x, y2, 'r--', label=\"Dashed red\")  # red, dashed\nplt.plot(x, y3, 'g:', label=\"Dotted green\") # green, dotted\n```\n\n---\n\n## üéØ Your Task\n\nPlot two sales lines for months 1-4. Sales A: `[10, 15, 13, 18]`, Sales B: `[8, 12, 16, 14]`. Add a legend.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nmonths = [1, 2, 3, 4]\nsales_a = [10, 15, 13, 18]\nsales_b = [8, 12, 16, 14]\n\n# Plot both lines with legend\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nmonths = [1, 2, 3, 4]\nsales_a = [10, 15, 13, 18]\nsales_b = [8, 12, 16, 14]\n\n# Plot both lines with legend\nplt.plot(months, sales_a, label=\"Sales A\")\nplt.plot(months, sales_b, label=\"Sales B\")\nplt.legend()\nplt.show()",
    "expected_output": "[Graph: Two line series with legend]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "84": {
    "id": 84,
    "title": "Colors and Styles",
    "content": "# üé® Styling Your Charts\n\n## Color Options\n\nMatplotlib supports many color formats:\n\n```python\n# Named colors\nplt.plot(x, y, color='red')\nplt.plot(x, y, color='skyblue')\n\n# Hex codes\nplt.plot(x, y, color='#FF5733')\n\n# RGB tuples (0-1 range)\nplt.plot(x, y, color=(0.2, 0.4, 0.6))\n```\n\n## Line Styles\n\n| Style | Code |\n| --- | --- |\n| Solid | `'-'` |\n| Dashed | `'--'` |\n| Dotted | `':'` |\n| Dash-dot | `'-.'` |\n\n## Markers\n\n| Marker | Code |\n| --- | --- |\n| Circle | `'o'` |\n| Square | `'s'` |\n| Triangle | `'^'` |\n| X | `'x'` |\n| Plus | `'+'` |\n\n## Combining Styles\n\n```python\nplt.plot(x, y, color='red', linestyle='--', marker='o', \n         linewidth=2, markersize=8)\n```\n\n---\n\n## üéØ Your Task\n\nCreate a line plot with red color, dashed line style, and circle markers for x `[1, 2, 3, 4]`, y `[1, 4, 2, 3]`.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [1, 4, 2, 3]\n\n# Plot with red, dashed, circles\n",
    "solution_code": "import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [1, 4, 2, 3]\n\n# Plot with red, dashed, circles\nplt.plot(x, y, color=\"red\", linestyle=\"--\", marker=\"o\")\nplt.show()",
    "expected_output": "[Graph: Red dashed line with circle markers]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "85": {
    "id": 85,
    "title": "Subplots",
    "content": "# üñºÔ∏è Multiple Plots in One Figure\n\n## Why Subplots?\n\nCombine related visualizations for comparison:\n- Before/after comparisons\n- Different metrics side by side\n- Dashboard-style layouts\n\n## Creating Subplots\n\n```python\nimport matplotlib.pyplot as plt\n\n# Create 1 row, 2 columns of plots\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nax1.plot([1, 2, 3], [1, 4, 9])\nax1.set_title(\"Line Plot\")\n\nax2.bar([\"A\", \"B\", \"C\"], [10, 20, 15])\nax2.set_title(\"Bar Chart\")\n\nplt.tight_layout()  # Prevent overlap\nplt.show()\n```\n\n## Grid Layouts\n\n```python\n# 2 rows, 2 columns\nfig, axes = plt.subplots(2, 2)\n\naxes[0, 0].plot(x, y1)    # Top left\naxes[0, 1].bar(x, y2)     # Top right\naxes[1, 0].scatter(x, y3) # Bottom left\naxes[1, 1].hist(data)     # Bottom right\n```\n\n## Figure Size\n\n```python\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n```\n\n---\n\n## üéØ Your Task\n\nCreate 2 side-by-side subplots: left with a line plot of `[1, 2, 3]`, right with a bar chart of categories `[\"A\", \"B\"]` with values `[5, 8]`.\n",
    "starter_code": "import matplotlib.pyplot as plt\n\n# Create 1x2 subplots\n",
    "solution_code": "import matplotlib.pyplot as plt\n\n# Create 1x2 subplots\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot([1, 2, 3])\nax2.bar([\"A\", \"B\"], [5, 8])\nplt.show()",
    "expected_output": "[Graph: Two subplots side by side]",
    "chapter_id": 11,
    "chapter_title": "Data Visualization"
  },
  "86": {
    "id": 86,
    "title": "Linear Search",
    "content": "# üîç Linear Search\n\n## What is an Algorithm?\n\nAn **algorithm** is a step-by-step procedure for solving a problem. It's like a recipe - specific instructions to achieve a result.\n\n## Linear Search: The Simplest Search\n\nCheck each element one by one until you find what you're looking for:\n\n```python\ndef linear_search(arr, target):\n    for i, val in enumerate(arr):\n        if val == target:\n            return i  # Found! Return index\n    return -1  # Not found\n```\n\n## How It Works (Step by Step)\n\nSearching for `8` in `[5, 2, 8, 1, 9]`:\n\n| Step | Check | Match? | Action |\n| --- | --- | --- | --- |\n| 1 | 5 | No | Continue |\n| 2 | 2 | No | Continue |\n| 3 | 8 | **Yes!** | Return index 2 |\n\n## Time Complexity\n\n- **Best case**: O(1) - target is first element\n- **Worst case**: O(n) - target is last or not found\n- **Average**: O(n/2) ‚Üí O(n)\n\nLinear search is simple but slow for large datasets.\n\n---\n\n## üéØ Your Task\n\nSearch for `8` in `[5, 2, 8, 1, 9]` and print its index.\n",
    "starter_code": "numbers = [5, 2, 8, 1, 9]\ntarget = 8\n\n# Find index of target\n",
    "solution_code": "numbers = [5, 2, 8, 1, 9]\ntarget = 8\n\n# Find index of target\nfor i, val in enumerate(numbers):\n    if val == target:\n        print(i)\n        break",
    "expected_output": "2",
    "chapter_id": 12,
    "chapter_title": "Algorithms"
  },
  "87": {
    "id": 87,
    "title": "Binary Search",
    "content": "# üîç Binary Search\n\n## The Divide and Conquer Approach\n\nFor **sorted arrays**, binary search is much faster:\n\n1. Look at the middle element\n2. If it's the target, done!\n3. If target is smaller, search left half\n4. If target is larger, search right half\n5. Repeat until found or range is empty\n\n## The Algorithm\n\n```python\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1  # Search right half\n        else:\n            right = mid - 1  # Search left half\n    \n    return -1  # Not found\n```\n\n## Example: Finding 7\n\nArray: `[1, 3, 5, 7, 9, 11, 13]`\n\n| Step | Left | Right | Mid | arr[mid] | Action |\n| --- | --- | --- | --- | --- | --- |\n| 1 | 0 | 6 | 3 | 7 | Found! |\n\n## Time Complexity: O(log n)\n\nWith each step, we eliminate half the remaining elements!\n- 1000 items ‚Üí ~10 steps\n- 1,000,000 items ‚Üí ~20 steps\n\n---\n\n## üéØ Your Task\n\nUse binary search to find index of `7` in `[1, 3, 5, 7, 9, 11, 13]`.\n",
    "starter_code": "sorted_nums = [1, 3, 5, 7, 9, 11, 13]\ntarget = 7\n\nleft, right = 0, len(sorted_nums) - 1\nresult = -1\n\n# Binary search\n",
    "solution_code": "sorted_nums = [1, 3, 5, 7, 9, 11, 13]\ntarget = 7\n\nleft, right = 0, len(sorted_nums) - 1\nresult = -1\n\n# Binary search\nwhile left <= right:\n    mid = (left + right) // 2\n    if sorted_nums[mid] == target:\n        result = mid\n        break\n    elif sorted_nums[mid] < target:\n        left = mid + 1\n    else:\n        right = mid - 1\n\nprint(result)",
    "expected_output": "3",
    "chapter_id": 12,
    "chapter_title": "Algorithms"
  },
  "88": {
    "id": 88,
    "title": "Bubble Sort",
    "content": "# ü´ß Bubble Sort\n\n## How Bubble Sort Works\n\nCompare adjacent elements and swap if out of order. Larger values \"bubble up\" to the end.\n\n## The Algorithm\n\n```python\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(n - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n```\n\n## Visualization\n\nSorting `[64, 34, 25]`:\n\n**Pass 1:**\n- Compare 64, 34 ‚Üí swap ‚Üí `[34, 64, 25]`\n- Compare 64, 25 ‚Üí swap ‚Üí `[34, 25, 64]`\n\n**Pass 2:**\n- Compare 34, 25 ‚Üí swap ‚Üí `[25, 34, 64]`\n- Compare 34, 64 ‚Üí no swap\n\n**Result:** `[25, 34, 64]` ‚úì\n\n## Time Complexity\n\n- **Always**: O(n¬≤) - compares every pair\n- **Not efficient** for large datasets\n- But easy to understand and implement!\n\n## Why \"Bubble\"?\n\nEach pass, the largest unsorted element \"bubbles up\" to its correct position.\n\n---\n\n## üéØ Your Task\n\nSort `[64, 34, 25, 12, 22]` using bubble sort and print the result.\n",
    "starter_code": "arr = [64, 34, 25, 12, 22]\n\n# Bubble sort\n\n\n# Print sorted array\n",
    "solution_code": "arr = [64, 34, 25, 12, 22]\n\n# Bubble sort\nfor i in range(len(arr)):\n    for j in range(len(arr) - 1):\n        if arr[j] > arr[j + 1]:\n            arr[j], arr[j + 1] = arr[j + 1], arr[j]\n\n# Print sorted array\nprint(arr)",
    "expected_output": "[12, 22, 25, 34, 64]",
    "chapter_id": 12,
    "chapter_title": "Algorithms"
  },
  "89": {
    "id": 89,
    "title": "Find Maximum",
    "content": "# üîù Finding the Maximum Value\n\n## The Algorithm\n\nTrack the largest value seen so far:\n\n```python\ndef find_max(arr):\n    max_val = arr[0]  # Assume first is largest\n    \n    for val in arr:\n        if val > max_val:\n            max_val = val  # Found a larger one!\n    \n    return max_val\n```\n\n## Step by Step\n\nFinding max in `[3, 7, 2, 9, 4]`:\n\n| Step | Current | max_val | Action |\n| --- | --- | --- | --- |\n| 1 | 3 | 3 | Initialize |\n| 2 | 7 | 7 | Update (7 > 3) |\n| 3 | 2 | 7 | No change |\n| 4 | 9 | 9 | Update (9 > 7) |\n| 5 | 4 | 9 | No change |\n\n**Result:** 9\n\n## Python's Built-in\n\n```python\nmax([3, 7, 2, 9, 4])  # 9\nmin([3, 7, 2, 9, 4])  # 2\n```\n\n## Time Complexity: O(n)\n\nMust check every element at least once.\n\n---\n\n## üéØ Your Task\n\nFind the maximum value in `[3, 7, 2, 9, 4, 1]` and print it.\n",
    "starter_code": "numbers = [3, 7, 2, 9, 4, 1]\n\n# Find maximum\n",
    "solution_code": "numbers = [3, 7, 2, 9, 4, 1]\n\n# Find maximum\nmax_val = numbers[0]\nfor num in numbers:\n    if num > max_val:\n        max_val = num\n\nprint(max_val)",
    "expected_output": "9",
    "chapter_id": 12,
    "chapter_title": "Algorithms"
  },
  "90": {
    "title": "Count Occurrences",
    "chapter_title": "Data Structures",
    "content": "# üî¢ Counting Occurrences: Frequency Analysis\n\n## Why Count Occurrences?\n\nKnowing how often things appear is fundamental:\n- \"Which product sells most?\"\n- \"What's the most common error?\"\n- \"How often does each letter appear?\"\n\n## List count() Method\n\n```python\nfruits = ['apple', 'banana', 'apple', 'cherry', 'apple']\napple_count = fruits.count('apple')  # 3\n```\n\n## String count() Method\n\n```python\ntext = \"mississippi\"\ntext.count('s')   # 4\ntext.count('ss')  # 2\ntext.count('i')   # 4\n```\n\n## Count All with Counter\n\n```python\nfrom collections import Counter\n\nfruits = ['apple', 'banana', 'apple', 'cherry', 'apple']\ncounts = Counter(fruits)\n# Counter({'apple': 3, 'banana': 1, 'cherry': 1})\n\ncounts['apple']          # 3\ncounts.most_common(2)    # [('apple', 3), ('banana', 1)]\n```\n\n## Manual Counting (Dictionary)\n\n```python\nitems = ['a', 'b', 'a', 'c', 'a', 'b']\ncounts = {}\nfor item in items:\n    counts[item] = counts.get(item, 0) + 1\n# {'a': 3, 'b': 2, 'c': 1}\n```\n\n## Real-World Example\n\n```python\n# Word frequency in text\nfrom collections import Counter\n\ntext = \"the quick brown fox jumps over the lazy dog\"\nword_counts = Counter(text.lower().split())\nprint(word_counts.most_common(3))\n```\n\n---\n\n## üéØ Your Task\n\nCount occurrences of items in various data structures.",
    "starter_code": "from collections import Counter\n\n# Count in a list\ncolors = ['red', 'blue', 'red', 'green', 'blue', 'red', 'yellow']\ncolor_counts = Counter(colors)\nprint(f\"Color counts: {dict(color_counts)}\")\nprint(f\"Most common: {color_counts.most_common(2)}\")\n\n# Count in a string\ntext = \"programming\"\nletter_counts = Counter(text)\nprint(f\"\\nLetter counts: {dict(letter_counts)}\")\nprint(f\"Most common letter: {letter_counts.most_common(1)}\")\n\n# Simple count() method\nprint(f\"\\n'r' in 'programming': {text.count('r')} occurrences\")",
    "solution_code": "from collections import Counter\n\nitems = ['a', 'b', 'a', 'c', 'a']\ncounts = Counter(items)\nprint(counts.most_common(2))",
    "expected_output": "Color counts: {'red': 3, 'blue': 2, 'green': 1, 'yellow': 1}\nMost common: [('red', 3), ('blue', 2)]"
  },
  "91": {
    "title": "Reverse Array",
    "chapter_title": "Data Structures",
    "content": "# üîÑ Reversing Arrays: Flip Data Around\n\n## Why Reverse?\n\nReversing arrays is useful for:\n- Displaying data in reverse chronological order\n- Undo operations (reverse the action sequence)\n- Palindrome checking\n- Algorithm building blocks\n\n## Three Ways to Reverse\n\n### 1. Slicing (Creates New List)\n```python\noriginal = [1, 2, 3, 4, 5]\nreversed_list = original[::-1]  # [5, 4, 3, 2, 1]\n# Original unchanged!\n```\n\n### 2. reverse() Method (Modifies In-Place)\n```python\nmy_list = [1, 2, 3, 4, 5]\nmy_list.reverse()  # Now [5, 4, 3, 2, 1]\n# Original is modified!\n```\n\n### 3. reversed() Function (Creates Iterator)\n```python\noriginal = [1, 2, 3, 4, 5]\nreversed_iter = reversed(original)\nreversed_list = list(reversed_iter)  # [5, 4, 3, 2, 1]\n# Original unchanged!\n```\n\n## When to Use Which\n\n| Method | Returns | Original |\n|--------|---------|----------|\n| `[::-1]` | New list | Unchanged |\n| `.reverse()` | None | Modified |\n| `reversed()` | Iterator | Unchanged |\n\n## Reversing Strings\n\nStrings are immutable, so use slicing:\n```python\ntext = \"Hello\"\nreversed_text = text[::-1]  # \"olleH\"\n```\n\n---\n\n## üéØ Your Task\n\nReverse arrays using different methods and compare results.",
    "starter_code": "# Original list\nnumbers = [1, 2, 3, 4, 5]\nprint(f\"Original: {numbers}\")\n\n# Method 1: Slicing (new list)\nreversed_slice = numbers[::-1]\nprint(f\"Slicing: {reversed_slice}\")\n\n# Method 2: reversed() function\nreversed_func = list(reversed(numbers))\nprint(f\"reversed(): {reversed_func}\")\n\n# Method 3: reverse() method (in-place)\nnumbers_copy = numbers.copy()\nnumbers_copy.reverse()\nprint(f\"reverse(): {numbers_copy}\")\n\n# Bonus: Reverse a string\ntext = \"Python\"\nreversed_text = text[::-1]\nprint(f\"\\nReversed string: '{text}' ‚Üí '{reversed_text}'\")",
    "solution_code": "nums = [1, 2, 3, 4, 5]\nprint(nums[::-1])  # [5, 4, 3, 2, 1]",
    "expected_output": "Original: [1, 2, 3, 4, 5]\nSlicing: [5, 4, 3, 2, 1]\nreversed(): [5, 4, 3, 2, 1]\nreverse(): [5, 4, 3, 2, 1]"
  },
  "92": {
    "title": "Remove Duplicates",
    "chapter_title": "Data Structures",
    "content": "# üßπ Remove Duplicates: Clean Up Repeated Values\n\n## Why Remove Duplicates?\n\nDuplicate data causes problems:\n- Inflated statistics\n- Wasted storage\n- Incorrect analysis\n\n## Quick Way: Convert to Set\n\n```python\nitems = [1, 2, 2, 3, 3, 3]\nunique = list(set(items))  # [1, 2, 3]\n```\n\n**Caveat**: Sets don't preserve order!\n\n## Preserve Order: dict.fromkeys()\n\n```python\nitems = ['b', 'a', 'b', 'c', 'a']\nunique = list(dict.fromkeys(items))  # ['b', 'a', 'c']\n```\n\n## Manual Method (Full Control)\n\n```python\nitems = [1, 2, 2, 3]\nunique = []\nfor item in items:\n    if item not in unique:\n        unique.append(item)\n```\n\n## For String Characters\n\n```python\ntext = \"mississippi\"\nunique_chars = ''.join(dict.fromkeys(text))  # \"misp\"\n```\n\n## With Pandas\n\n```python\ndf = df.drop_duplicates()  # Remove duplicate rows\ndf = df.drop_duplicates(subset=['column'])  # Based on one column\n```\n\n## Performance Comparison\n\n| Method | Speed | Preserves Order |\n|--------|-------|-----------------|\n| set() | Fast | ‚ùå |\n| dict.fromkeys() | Fast | ‚úÖ |\n| Manual loop | Slow | ‚úÖ |\n\n---\n\n## üéØ Your Task\n\nRemove duplicates from lists while controlling order.",
    "starter_code": "# Remove duplicates - different methods\n\n# Original with duplicates\nitems = ['apple', 'banana', 'apple', 'cherry', 'banana', 'date', 'apple']\nprint(f\"Original: {items}\")\nprint(f\"Count: {len(items)}\")\n\n# Method 1: Using set (fastest, no order guarantee)\nunique_set = list(set(items))\nprint(f\"\\nUsing set: {unique_set}\")\n\n# Method 2: Using dict.fromkeys (preserves order)\nunique_ordered = list(dict.fromkeys(items))\nprint(f\"Using dict.fromkeys: {unique_ordered}\")\n\n# Method 3: Manual loop (most control)\nunique_manual = []\nfor item in items:\n    if item not in unique_manual:\n        unique_manual.append(item)\nprint(f\"Manual: {unique_manual}\")\n\nprint(f\"\\nReduced from {len(items)} to {len(unique_ordered)} items\")",
    "solution_code": "items = [1, 2, 2, 3, 3, 3]\nunique = list(dict.fromkeys(items))\nprint(unique)",
    "expected_output": "Original: ['apple', 'banana', 'apple', 'cherry', 'banana', 'date', 'apple']\nUsing set: ['apple', 'banana', 'cherry', 'date']"
  },
  "93": {
    "id": 93,
    "title": "Two Sum",
    "content": "# üéØ The Two Sum Problem\n\n## The Problem\n\nFind two numbers in an array that add up to a target sum.\n\n## Brute Force Approach\n\nCheck every pair:\n\n```python\ndef two_sum_brute(nums, target):\n    for i in range(len(nums)):\n        for j in range(i + 1, len(nums)):\n            if nums[i] + nums[j] == target:\n                return [nums[i], nums[j]]\n    return None\n```\n\n**Time Complexity**: O(n¬≤)\n\n## Optimized Approach (Hash Map)\n\n```python\ndef two_sum_fast(nums, target):\n    seen = {}\n    for num in nums:\n        complement = target - num\n        if complement in seen:\n            return [complement, num]\n        seen[num] = True\n    return None\n```\n\n**Time Complexity**: O(n)\n\n## Why This is Famous\n\nThis is **THE** most common coding interview question! It teaches:\n- Hash tables\n- Trade-offs between time and space\n- Problem-solving strategies\n\n---\n\n## üéØ Your Task\n\nFind two numbers in `[2, 7, 11, 15]` that add up to 9 and print them.\n",
    "starter_code": "nums = [2, 7, 11, 15]\ntarget = 9\n\n# Find two numbers that sum to target\n",
    "solution_code": "nums = [2, 7, 11, 15]\ntarget = 9\n\n# Find two numbers that sum to target\nfor i in range(len(nums)):\n    for j in range(i + 1, len(nums)):\n        if nums[i] + nums[j] == target:\n            print(nums[i], nums[j])",
    "expected_output": "2 7",
    "chapter_id": 12,
    "chapter_title": "Algorithms"
  },
  "94": {
    "title": "Fibonacci",
    "chapter_title": "Data Structures",
    "content": "# üêö Fibonacci Sequence: Nature's Number Pattern\n\n## What is Fibonacci?\n\nEach number is the sum of the two before it:\n```\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89...\n```\n\n## Why It Matters\n\nFibonacci appears everywhere:\n- Flower petals (usually 3, 5, 8, 13...)\n- Spiral shells\n- Stock market analysis\n- Algorithm design\n\n## Generating Fibonacci\n\n### Iterative (Fast)\n```python\ndef fibonacci(n):\n    fib = [0, 1]\n    for i in range(2, n):\n        fib.append(fib[i-1] + fib[i-2])\n    return fib[:n]\n\nfibonacci(10)  # [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n```\n\n### Recursive (Elegant but Slow)\n```python\ndef fib(n):\n    if n <= 1:\n        return n\n    return fib(n-1) + fib(n-2)\n```\n\n### Generator (Memory Efficient)\n```python\ndef fib_gen(n):\n    a, b = 0, 1\n    for _ in range(n):\n        yield a\n        a, b = b, a + b\n```\n\n## The Golden Ratio\n\nAs Fibonacci numbers grow, their ratio approaches the **golden ratio** (‚âà1.618):\n```python\n55 / 34 ‚âà 1.617\n89 / 55 ‚âà 1.618\n```\n\n---\n\n## üéØ Your Task\n\nGenerate the Fibonacci sequence and explore its properties.",
    "starter_code": "def fibonacci(n):\n    '''Generate first n Fibonacci numbers'''\n    if n <= 0:\n        return []\n    if n == 1:\n        return [0]\n    \n    fib = [0, 1]\n    for i in range(2, n):\n        fib.append(fib[i-1] + fib[i-2])\n    return fib\n\n# Generate sequence\nfib_10 = fibonacci(10)\nprint(f\"First 10: {fib_10}\")\n\n# Show the ratio approaching golden ratio\nprint(\"\\nRatios (approaching 1.618...):\")\nfor i in range(2, 10):\n    ratio = fib_10[i] / fib_10[i-1] if fib_10[i-1] != 0 else 0\n    print(f\"  {fib_10[i]} / {fib_10[i-1]} = {ratio:.4f}\")",
    "solution_code": "def fib(n):\n    seq = [0, 1]\n    for i in range(2, n):\n        seq.append(seq[-1] + seq[-2])\n    return seq\n\nprint(fib(10))",
    "expected_output": "First 10: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"
  },
  "95": {
    "id": 95,
    "title": "Mean (Average)",
    "content": "# üìä Mean (Average): The Center of Your Data\n\n## What is the Mean?\n\nThe **mean** (or average) is the most common way to find the \"center\" of a dataset. It tells you what value you'd expect if all the data were spread evenly.\n\n## Real-World Analogy\n\nImagine you and 4 friends pool your money: $10, $20, $30, $15, $25. The mean ($20) is what each person would have if you divided the total equally.\n\n## How to Calculate\n\n```python\n# Mean = Sum of all values / Number of values\ndata = [10, 20, 30, 15, 25]\nmean = sum(data) / len(data)  # 100 / 5 = 20\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([10, 20, 30, 15, 25])\nmean = np.mean(data)  # 20.0\n```\n\n## When to Use the Mean\n\n‚úÖ Data is roughly symmetric (no extreme outliers)\n‚úÖ You want a single representative value\n‚ùå Avoid when you have extreme outliers (use median instead)\n\n## Data Science Application\n\nCalculating average revenue, average test scores, or average customer ratings.\n\n---\n\n## üéØ Your Task\n\nCalculate the mean of the following sales data and print it with 2 decimal places.",
    "starter_code": "import numpy as np\n\n# Weekly sales data\nsales = np.array([1250.50, 1340.75, 980.25, 1560.00, 1420.30, 1180.90, 1650.25])\n\n# Calculate the mean\nmean_sales = None  # Your code here\n\nprint(f\"Average weekly sales: ${mean_sales:.2f}\")",
    "solution_code": "import numpy as np\n\n# Weekly sales data\nsales = np.array([1250.50, 1340.75, 980.25, 1560.00, 1420.30, 1180.90, 1650.25])\n\n# Calculate the mean\nmean_sales = np.mean(sales)\n\nprint(f\"Average weekly sales: ${mean_sales:.2f}\")",
    "expected_output": "Average weekly sales: $1340.42",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "96": {
    "id": 96,
    "title": "Median",
    "content": "# üìè Median: The Middle Value\n\n## What is the Median?\n\nThe **median** is the middle value when data is sorted. Unlike the mean, it's not affected by extreme values (outliers).\n\n## Real-World Analogy\n\nImagine lining up 5 people by height. The person in the middle represents the median height‚Äîit doesn't matter if the tallest person is a basketball player or an average adult.\n\n## How to Calculate\n\n1. Sort the data\n2. If odd number of values: pick the middle one\n3. If even number: average the two middle values\n\n```python\n# Odd number of values\ndata = [3, 1, 9, 5, 7]\n# Sorted: [1, 3, 5, 7, 9]\n# Median = 5 (middle value)\n\n# Even number of values\ndata = [3, 1, 9, 5]\n# Sorted: [1, 3, 5, 9]\n# Median = (3 + 5) / 2 = 4\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([3, 1, 9, 5, 7])\nmedian = np.median(data)  # 5.0\n```\n\n## Mean vs Median\n\n| Scenario | Use Mean | Use Median |\n|----------|----------|------------|\n| Symmetric data | ‚úÖ | ‚úÖ |\n| Outliers present | ‚ùå | ‚úÖ |\n| Income data | ‚ùå | ‚úÖ |\n\n---\n\n## üéØ Your Task\n\nCalculate the median home price from this dataset.",
    "starter_code": "import numpy as np\n\n# Home prices (in thousands)\nprices = np.array([250, 275, 300, 285, 950, 290, 265, 280])\n\n# Calculate median (note the outlier at 950!)\nmedian_price = None  # Your code here\n\nprint(f\"Median home price: ${median_price}K\")",
    "solution_code": "import numpy as np\n\n# Home prices (in thousands)\nprices = np.array([250, 275, 300, 285, 950, 290, 265, 280])\n\n# Calculate median (note the outlier at 950!)\nmedian_price = np.median(prices)\n\nprint(f\"Median home price: ${median_price}K\")",
    "expected_output": "Median home price: $282.5K",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "97": {
    "id": 97,
    "title": "Mode",
    "content": "# üéØ Mode: The Most Frequent Value\n\n## What is the Mode?\n\nThe **mode** is the value that appears most often in a dataset. A dataset can have:\n- **No mode**: all values appear once\n- **One mode**: one value appears most often\n- **Multiple modes**: several values tie for most frequent\n\n## Real-World Analogy\n\nIn a shoe store, the mode shoe size tells you which size to stock the most of!\n\n## How to Calculate\n\n```python\nfrom collections import Counter\ndata = [1, 2, 2, 3, 3, 3, 4]\n# 3 appears 3 times - it's the mode!\n\ncounter = Counter(data)\nmode = counter.most_common(1)[0][0]  # Returns 3\n```\n\n## Using SciPy\n\n```python\nfrom scipy import stats\ndata = [1, 2, 2, 3, 3, 3, 4]\nmode_result = stats.mode(data, keepdims=True)\nprint(mode_result.mode[0])  # 3\n```\n\n## When to Use Mode\n\n‚úÖ Categorical data (colors, sizes, categories)\n‚úÖ Finding most popular item\n‚úÖ Discrete data with repeated values\n‚ùå Continuous data (use mean/median instead)\n\n---\n\n## üéØ Your Task\n\nFind the most popular product size from sales data.",
    "starter_code": "from collections import Counter\n\n# Product sizes sold today\nsizes = ['M', 'L', 'S', 'M', 'XL', 'M', 'L', 'S', 'M', 'L', 'M', 'S']\n\n# Find the mode (most common size)\ncounter = Counter(sizes)\nmost_common = None  # Get the most common size\n\nprint(f\"Most popular size: {most_common}\")",
    "solution_code": "from collections import Counter\n\n# Product sizes sold today\nsizes = ['M', 'L', 'S', 'M', 'XL', 'M', 'L', 'S', 'M', 'L', 'M', 'S']\n\n# Find the mode (most common size)\ncounter = Counter(sizes)\nmost_common = counter.most_common(1)[0][0]\n\nprint(f\"Most popular size: {most_common}\")",
    "expected_output": "Most popular size: M",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "98": {
    "id": 98,
    "title": "Range",
    "content": "# üìè Range: Spread of Your Data\n\n## What is the Range?\n\nThe **range** is the simplest measure of spread‚Äîit's just the difference between the largest and smallest values.\n\n```\nRange = Maximum - Minimum\n```\n\n## Real-World Analogy\n\nIf temperatures this week ranged from 60¬∞F to 85¬∞F, the range is 25¬∞F. This tells you how much the temperature varied.\n\n## How to Calculate\n\n```python\ndata = [10, 25, 15, 30, 20]\nrange_value = max(data) - min(data)  # 30 - 10 = 20\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([10, 25, 15, 30, 20])\nrange_value = np.ptp(data)  # Peak-to-peak = 20\n# Or: np.max(data) - np.min(data)\n```\n\n## Limitations\n\n‚ö†Ô∏è Range is sensitive to outliers!\n- Data: [10, 15, 20, 25, 30] ‚Üí Range = 20\n- Data: [10, 15, 20, 25, 100] ‚Üí Range = 90 (misleading!)\n\n## Better Alternatives\n\nFor more robust spread measures, consider:\n- **Interquartile Range (IQR)**\n- **Standard Deviation**\n\n---\n\n## üéØ Your Task\n\nCalculate the range of daily temperatures.",
    "starter_code": "import numpy as np\n\n# Daily high temperatures (Fahrenheit)\ntemps = np.array([72, 75, 68, 82, 79, 85, 71, 88, 69, 74])\n\n# Calculate range\ntemp_range = None  # Your code here\n\nprint(f\"Temperature range: {temp_range}¬∞F\")",
    "solution_code": "import numpy as np\n\n# Daily high temperatures (Fahrenheit)\ntemps = np.array([72, 75, 68, 82, 79, 85, 71, 88, 69, 74])\n\n# Calculate range\ntemp_range = np.ptp(temps)  # or np.max(temps) - np.min(temps)\n\nprint(f\"Temperature range: {temp_range}¬∞F\")",
    "expected_output": "Temperature range: 20¬∞F",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "99": {
    "id": 99,
    "title": "Variance",
    "content": "# üìä Variance: Measuring Data Spread\n\n## What is Variance?\n\n**Variance** measures how spread out data points are from the mean. Higher variance = more spread.\n\n## The Formula\n\n1. Find the mean\n2. Subtract mean from each value (get deviations)\n3. Square each deviation\n4. Take the average of squared deviations\n\n```\nVariance = Œ£(x - mean)¬≤ / n\n```\n\n## Real-World Analogy\n\nTwo basketball players both average 20 points per game. Player A scores 19, 20, 21, 20, 20 (low variance‚Äîconsistent). Player B scores 5, 35, 10, 30, 20 (high variance‚Äîunpredictable).\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([10, 12, 23, 23, 16, 23, 21, 16])\n\n# Population variance (default)\nvar_pop = np.var(data)  \n\n# Sample variance (use ddof=1)\nvar_sample = np.var(data, ddof=1)\n```\n\n## Population vs Sample Variance\n\n- **Population**: Use when you have ALL the data\n- **Sample**: Use when you have a subset (use `ddof=1`)\n\n---\n\n## üéØ Your Task\n\nCalculate the variance of test scores to understand consistency.",
    "starter_code": "import numpy as np\n\n# Test scores for two classes\nclass_a = np.array([85, 87, 84, 86, 85, 88, 84, 86])\nclass_b = np.array([70, 95, 60, 100, 75, 90, 65, 85])\n\n# Calculate variance for each class\nvar_a = None  # Your code\nvar_b = None  # Your code\n\nprint(f\"Class A variance: {var_a:.2f}\")\nprint(f\"Class B variance: {var_b:.2f}\")\nprint(f\"More consistent class: {'A' if var_a < var_b else 'B'}\")",
    "solution_code": "import numpy as np\n\n# Test scores for two classes\nclass_a = np.array([85, 87, 84, 86, 85, 88, 84, 86])\nclass_b = np.array([70, 95, 60, 100, 75, 90, 65, 85])\n\n# Calculate variance for each class\nvar_a = np.var(class_a)\nvar_b = np.var(class_b)\n\nprint(f\"Class A variance: {var_a:.2f}\")\nprint(f\"Class B variance: {var_b:.2f}\")\nprint(f\"More consistent class: {'A' if var_a < var_b else 'B'}\")",
    "expected_output": "Class A variance: 1.75\nClass B variance: 200.00\nMore consistent class: A",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "100": {
    "id": 100,
    "title": "Standard Deviation",
    "content": "# ÔøΩÔøΩ Standard Deviation: Making Variance Useful\n\n## What is Standard Deviation?\n\n**Standard deviation (œÉ or std)** is the square root of variance. It measures spread in the same units as your data, making it more interpretable.\n\n## Why Not Just Use Variance?\n\nVariance is in \"squared units\" which is hard to interpret:\n- Data: test scores (0-100)\n- Variance: 225 \"squared points\" ü§î\n- Std Dev: 15 points ‚úÖ (much clearer!)\n\n## The Formula\n\n```\nStandard Deviation = ‚àöVariance\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([10, 12, 23, 23, 16, 23, 21, 16])\n\nstd_dev = np.std(data)  # Population std dev\nstd_sample = np.std(data, ddof=1)  # Sample std dev\n```\n\n## The 68-95-99.7 Rule\n\nFor normal distributions:\n- **68%** of data falls within 1 std dev of mean\n- **95%** of data falls within 2 std devs\n- **99.7%** of data falls within 3 std devs\n\n## Data Science Use Cases\n\n- **Quality Control**: Products outside 2 std devs may be defective\n- **Finance**: Stock volatility measured by std dev\n- **Grading**: Standardizing test scores\n\n---\n\n## üéØ Your Task\n\nCalculate the standard deviation of stock returns to measure volatility.",
    "starter_code": "import numpy as np\n\n# Daily stock returns (%)\nreturns = np.array([1.2, -0.8, 2.1, -1.5, 0.5, 1.8, -0.3, 0.9, -1.2, 1.5])\n\n# Calculate standard deviation (volatility)\nvolatility = None  # Your code here\n\nprint(f\"Stock volatility: {volatility:.2f}%\")",
    "solution_code": "import numpy as np\n\n# Daily stock returns (%)\nreturns = np.array([1.2, -0.8, 2.1, -1.5, 0.5, 1.8, -0.3, 0.9, -1.2, 1.5])\n\n# Calculate standard deviation (volatility)\nvolatility = np.std(returns)\n\nprint(f\"Stock volatility: {volatility:.2f}%\")",
    "expected_output": "Stock volatility: 1.16%",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "101": {
    "id": 101,
    "title": "Percentiles",
    "content": "# üìä Percentiles: Where Does Your Data Fall?\n\n## What are Percentiles?\n\nA **percentile** tells you what percentage of data falls below a certain value.\n\n- **25th percentile (Q1)**: 25% of data is below this\n- **50th percentile (Q2)**: The median\n- **75th percentile (Q3)**: 75% of data is below this\n\n## Real-World Analogy\n\nIf your test score is in the 90th percentile, you scored higher than 90% of test-takers!\n\n## Using NumPy\n\n```python\nimport numpy as np\ndata = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nq1 = np.percentile(data, 25)   # 3.25\nq2 = np.percentile(data, 50)   # 5.5 (median)\nq3 = np.percentile(data, 75)   # 7.75\n```\n\n## Interquartile Range (IQR)\n\n```\nIQR = Q3 - Q1\n```\n\nIQR is useful for detecting outliers:\n- Values below Q1 - 1.5*IQR are outliers\n- Values above Q3 + 1.5*IQR are outliers\n\n## Data Science Applications\n\n- **Salary analysis**: \"You're in the 75th percentile for your role\"\n- **Performance metrics**: \"Response time at 95th percentile\"\n- **Grading curves**: Assigning grades based on percentiles\n\n---\n\n## üéØ Your Task\n\nCalculate Q1, Q2 (median), Q3, and IQR for salary data.",
    "starter_code": "import numpy as np\n\n# Employee salaries (in thousands)\nsalaries = np.array([45, 52, 58, 62, 65, 68, 72, 78, 85, 95, 120])\n\n# Calculate percentiles\nq1 = None  # 25th percentile\nq2 = None  # 50th percentile (median)  \nq3 = None  # 75th percentile\niqr = None  # Interquartile range\n\nprint(f\"Q1: ${q1}K\")\nprint(f\"Median: ${q2}K\")\nprint(f\"Q3: ${q3}K\")\nprint(f\"IQR: ${iqr}K\")",
    "solution_code": "import numpy as np\n\n# Employee salaries (in thousands)\nsalaries = np.array([45, 52, 58, 62, 65, 68, 72, 78, 85, 95, 120])\n\n# Calculate percentiles\nq1 = np.percentile(salaries, 25)\nq2 = np.percentile(salaries, 50)\nq3 = np.percentile(salaries, 75)\niqr = q3 - q1\n\nprint(f\"Q1: ${q1}K\")\nprint(f\"Median: ${q2}K\")\nprint(f\"Q3: ${q3}K\")\nprint(f\"IQR: ${iqr}K\")",
    "expected_output": "Q1: $58.0K\nMedian: $68.0K\nQ3: $85.0K\nIQR: $27.0K",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "102": {
    "id": 102,
    "title": "Correlation",
    "content": "# üîó Correlation: Relationships Between Variables\n\n## What is Correlation?\n\n**Correlation** measures the strength and direction of the relationship between two variables. It ranges from -1 to +1.\n\n| Value | Meaning |\n|-------|---------|\n| +1 | Perfect positive correlation |\n| 0 | No correlation |\n| -1 | Perfect negative correlation |\n\n## Real-World Examples\n\n- **Positive**: Height and weight (taller people tend to weigh more)\n- **Negative**: Speed and fuel efficiency (faster = less efficient)\n- **None**: Shoe size and IQ (unrelated)\n\n## Using NumPy\n\n```python\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 5, 4, 5])\n\n# Correlation matrix\ncorr_matrix = np.corrcoef(x, y)\ncorrelation = corr_matrix[0, 1]  # 0.83\n```\n\n## Interpreting Correlation Strength\n\n| Absolute Value | Strength |\n|----------------|----------|\n| 0.0 - 0.3 | Weak |\n| 0.3 - 0.7 | Moderate |\n| 0.7 - 1.0 | Strong |\n\n## ‚ö†Ô∏è Important Warning\n\n**Correlation ‚â† Causation!**\n\nIce cream sales and drowning deaths are correlated (both increase in summer), but ice cream doesn't cause drowning!\n\n---\n\n## üéØ Your Task\n\nCalculate the correlation between study hours and exam scores.",
    "starter_code": "import numpy as np\n\n# Study data\nhours_studied = np.array([1, 2, 3, 4, 5, 6, 7, 8])\nexam_scores = np.array([52, 58, 65, 70, 78, 82, 88, 95])\n\n# Calculate correlation\ncorrelation = None  # Your code here\n\nprint(f\"Correlation: {correlation:.3f}\")\nprint(f\"Relationship: {'Strong' if abs(correlation) > 0.7 else 'Moderate' if abs(correlation) > 0.3 else 'Weak'}\")",
    "solution_code": "import numpy as np\n\n# Study data\nhours_studied = np.array([1, 2, 3, 4, 5, 6, 7, 8])\nexam_scores = np.array([52, 58, 65, 70, 78, 82, 88, 95])\n\n# Calculate correlation\ncorrelation = np.corrcoef(hours_studied, exam_scores)[0, 1]\n\nprint(f\"Correlation: {correlation:.3f}\")\nprint(f\"Relationship: {'Strong' if abs(correlation) > 0.7 else 'Moderate' if abs(correlation) > 0.3 else 'Weak'}\")",
    "expected_output": "Correlation: 0.994\nRelationship: Strong",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "103": {
    "id": 103,
    "title": "Z-Score",
    "content": "# üìè Z-Score: Standardizing Your Data\n\n## What is a Z-Score?\n\nA **Z-score** tells you how many standard deviations a value is from the mean. It standardizes data to a common scale.\n\n## The Formula\n\n```\nZ = (X - mean) / std_dev\n```\n\n## Interpreting Z-Scores\n\n| Z-Score | Meaning |\n|---------|---------|\n| 0 | At the mean |\n| +1 | 1 std dev above mean |\n| -1 | 1 std dev below mean |\n| +2 | 2 std devs above (unusual) |\n| ¬±3+ | Very unusual (outlier) |\n\n## Real-World Example\n\nTwo students take different tests:\n- Alice: 80/100 (class mean: 70, std: 10) ‚Üí Z = 1.0\n- Bob: 85/100 (class mean: 75, std: 5) ‚Üí Z = 2.0\n\nBob's score is more impressive relative to his class!\n\n## Using Python\n\n```python\nimport numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\nmean = np.mean(data)\nstd = np.std(data)\n\nz_scores = (data - mean) / std\n```\n\n## Data Science Applications\n\n- **Comparing**: Scores from different scales\n- **Outlier detection**: |Z| > 3 is often considered an outlier\n- **Feature scaling**: Standardization for ML models\n\n---\n\n## üéØ Your Task\n\nCalculate Z-scores for student test scores and identify any outliers (|Z| > 2).",
    "starter_code": "import numpy as np\n\n# Test scores\nscores = np.array([72, 85, 90, 78, 95, 45, 88, 82, 79, 84])\n\n# Calculate Z-scores\nmean = np.mean(scores)\nstd = np.std(scores)\nz_scores = None  # Calculate (scores - mean) / std\n\n# Find outliers\nprint(f\"Mean: {mean:.1f}, Std: {std:.1f}\")\nprint(\"\\nZ-scores and outlier status:\")\nfor score, z in zip(scores, z_scores):\n    status = \"‚ö†Ô∏è OUTLIER\" if abs(z) > 2 else \"\"\n    print(f\"  Score {score}: Z = {z:.2f} {status}\")",
    "solution_code": "import numpy as np\n\n# Test scores\nscores = np.array([72, 85, 90, 78, 95, 45, 88, 82, 79, 84])\n\n# Calculate Z-scores\nmean = np.mean(scores)\nstd = np.std(scores)\nz_scores = (scores - mean) / std\n\n# Find outliers\nprint(f\"Mean: {mean:.1f}, Std: {std:.1f}\")\nprint(\"\\nZ-scores and outlier status:\")\nfor score, z in zip(scores, z_scores):\n    status = \"‚ö†Ô∏è OUTLIER\" if abs(z) > 2 else \"\"\n    print(f\"  Score {score}: Z = {z:.2f} {status}\")",
    "expected_output": "Mean: 79.8, Std: 13.1\n\nZ-scores and outlier status:",
    "chapter_id": 13,
    "chapter_title": "Statistics"
  },
  "104": {
    "id": 104,
    "title": "Train/Test Split",
    "content": "# üîÄ Train/Test Split: Evaluating ML Models Properly\n\n## Why Split Data?\n\nIf you train AND test on the same data, your model might just \"memorize\" the answers! This is called **overfitting**.\n\n## The Solution: Train/Test Split\n\nSplit your data into two sets:\n- **Training set (80%)**: Model learns patterns\n- **Test set (20%)**: Model is evaluated on unseen data\n\n## Real-World Analogy\n\nImagine studying for an exam:\n- **Training**: You study the practice problems\n- **Testing**: The actual exam has NEW questions\n\nIf the exam had the exact same questions as practice, everyone would get 100%!\n\n## Using scikit-learn\n\n```python\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2,    # 20% for testing\n    random_state=42   # For reproducibility\n)\n```\n\n## Key Parameters\n\n- **test_size**: Fraction for testing (0.2 = 20%)\n- **random_state**: Seed for reproducibility\n- **stratify**: Maintain class proportions (for classification)\n\n---\n\n## üéØ Your Task\n\nSplit the dataset into training (80%) and testing (20%) sets.",
    "starter_code": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Sample data: features and target\nnp.random.seed(42)\nX = np.random.rand(100, 3)  # 100 samples, 3 features\ny = np.random.randint(0, 2, 100)  # Binary target\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=None,  # Set to 20%\n    random_state=42\n)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")",
    "solution_code": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Sample data: features and target\nnp.random.seed(42)\nX = np.random.rand(100, 3)  # 100 samples, 3 features\ny = np.random.randint(0, 2, 100)  # Binary target\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,  # 20% for testing\n    random_state=42\n)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")",
    "expected_output": "Training samples: 80\nTesting samples: 20",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "105": {
    "id": 105,
    "title": "Linear Regression",
    "content": "# üìà Linear Regression: Predicting Continuous Values\n\n## What is Linear Regression?\n\n**Linear regression** finds the best straight line through your data to predict a continuous target variable.\n\n## The Equation\n\n```\ny = mx + b\n```\n- **y**: Predicted value\n- **m**: Slope (how much y changes when x increases by 1)\n- **x**: Input feature\n- **b**: Intercept (y value when x = 0)\n\n## Real-World Example\n\nPredicting house prices based on square footage:\n- Input (x): House size in sq ft\n- Output (y): Predicted price\n\n## Using scikit-learn\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\n```\n\n## Evaluating the Model\n\n**R¬≤ Score** (coefficient of determination):\n- 1.0 = Perfect predictions\n- 0.0 = Model is no better than predicting the mean\n- Negative = Worse than the mean!\n\n---\n\n## üéØ Your Task\n\nTrain a linear regression model to predict sales based on advertising spend.",
    "starter_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Advertising spend (X) and Sales (y)\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 100  # Ad spend $0-100\ny = 2.5 * X.flatten() + 10 + np.random.randn(50) * 5  # Sales with noise\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\nscore = model.score(X_test, y_test)\nprint(f\"R¬≤ Score: {score:.3f}\")\nprint(f\"For every $1 spent on ads, sales increase by ${model.coef_[0]:.2f}\")",
    "solution_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Advertising spend (X) and Sales (y)\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 100  # Ad spend $0-100\ny = 2.5 * X.flatten() + 10 + np.random.randn(50) * 5  # Sales with noise\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\nscore = model.score(X_test, y_test)\nprint(f\"R¬≤ Score: {score:.3f}\")\nprint(f\"For every $1 spent on ads, sales increase by ${model.coef_[0]:.2f}\")",
    "expected_output": "R¬≤ Score: 0.997\nFor every $1 spent on ads, sales increase by $2.50",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "106": {
    "id": 106,
    "title": "Model Accuracy",
    "content": "# üìä Measuring Model Performance\n\n## Classification Accuracy\n\nFor classification problems, accuracy is the percentage of correct predictions:\n\n$$\\text{accuracy} = \\frac{\\text{correct predictions}}{\\text{total predictions}}$$\n\n```python\nfrom sklearn.metrics import accuracy_score\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(accuracy)  # 0.8 (80% correct)\n```\n\n## Interpreting Accuracy\n\n- **100%**: Perfect (suspicious - might be overfitting!)\n- **90%+**: Generally very good\n- **50%**: No better than random guessing (for binary classification)\n\n## Other Metrics\n\n| Metric | Best For |\n| --- | --- |\n| Accuracy | Balanced classes |\n| Precision | When false positives are costly |\n| Recall | When false negatives are costly |\n| F1-Score | Balance of precision and recall |\n\n## Why Accuracy Isn't Everything\n\nIf 99% of emails are not spam, a model that predicts \"not spam\" for everything gets 99% accuracy but is useless!\n\nAlways consider the **context** of your problem.\n\n---\n\n## üéØ Your Task\n\nCalculate accuracy for true `[1, 0, 1, 1, 0]` vs predicted `[1, 0, 0, 1, 0]` and print it.\n",
    "starter_code": "from sklearn.metrics import accuracy_score\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\n# Calculate accuracy\n",
    "solution_code": "from sklearn.metrics import accuracy_score\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\n# Calculate accuracy\nacc = accuracy_score(y_true, y_pred)\nprint(acc)",
    "expected_output": "0.8",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "107": {
    "id": 107,
    "title": "K-Nearest Neighbors",
    "content": "# üéØ K-Nearest Neighbors (KNN): Classification by Similarity\n\n## What is KNN?\n\n**K-Nearest Neighbors** classifies data points based on their closest neighbors. It's like asking: \"What are most of my neighbors?\"\n\n## Real-World Analogy\n\nMoving to a new neighborhood? KNN is like predicting your political views based on your 5 closest neighbors' views‚Äîyou'll probably vote like your neighbors!\n\n## How It Works\n\n1. Pick K (number of neighbors to consider)\n2. Find the K closest data points\n3. Take a vote‚Äîmajority class wins!\n\n## The Algorithm\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n```\n\n## Choosing K\n\n- **Small K (1-3)**: Sensitive to noise, may overfit\n- **Large K (10+)**: Smoother boundaries, may underfit\n- **Rule of thumb**: Start with ‚àön (square root of samples)\n\n## Distance Metrics\n\n- **Euclidean**: Straight-line distance (most common)\n- **Manhattan**: City-block distance\n- **Minkowski**: Generalized distance\n\n---\n\n## ÔøΩÔøΩ Your Task\n\nBuild a KNN classifier to predict flower species from the Iris dataset.",
    "starter_code": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train KNN model with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Evaluate\naccuracy = knn.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.1%}\")",
    "solution_code": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train KNN model with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Evaluate\naccuracy = knn.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.1%}\")",
    "expected_output": "Accuracy: 100.0%",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "108": {
    "id": 108,
    "title": "Decision Tree",
    "content": "# üå≥ Decision Trees: If-Then-Else Machine Learning\n\n## What is a Decision Tree?\n\nA **Decision Tree** makes predictions by learning a series of if-then-else questions about your data‚Äîjust like a flowchart!\n\n## Real-World Analogy\n\n\"Should I play tennis today?\"\n- Is it sunny? (Yes ‚Üí Check humidity)\n  - Is humidity high? (Yes ‚Üí No tennis)\n  - Is humidity normal? (Yes ‚Üí Play tennis!)\n- Is it rainy? (Yes ‚Üí No tennis)\n\n## How It Works\n\nThe tree asks questions that best split the data:\n1. Find the feature that best separates classes\n2. Split data based on that feature\n3. Repeat until all data is pure (or max depth reached)\n\n## Using scikit-learn\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth=3)\ntree.fit(X_train, y_train)\npredictions = tree.predict(X_test)\n```\n\n## Key Parameters\n\n- **max_depth**: Limit tree depth (prevents overfitting)\n- **min_samples_split**: Minimum samples to split a node\n- **criterion**: 'gini' or 'entropy' for measuring splits\n\n## Pros & Cons\n\n‚úÖ Easy to understand and visualize\n‚úÖ Handles non-linear relationships\n‚ùå Can overfit easily\n‚ùå Unstable‚Äîsmall data changes = big tree changes\n\n---\n\n## üéØ Your Task\n\nTrain a decision tree classifier on the wine dataset.",
    "starter_code": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Load wine dataset\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train decision tree (limit depth to prevent overfitting)\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Evaluate\naccuracy = tree.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.1%}\")",
    "solution_code": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Load wine dataset\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train decision tree (limit depth to prevent overfitting)\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Evaluate\naccuracy = tree.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.1%}\")",
    "expected_output": "Accuracy: 91.7%",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "109": {
    "id": 109,
    "title": "Feature Scaling",
    "content": "# ÔøΩÔøΩ Feature Scaling: Putting Features on Equal Footing\n\n## Why Scale Features?\n\nMany ML algorithms are sensitive to feature scales. If one feature ranges 0-1000 and another 0-1, the algorithm may favor the larger one!\n\n## Two Main Methods\n\n### 1. Standardization (Z-Score Scaling)\nMakes mean=0 and std=1\n```python\nX_scaled = (X - mean) / std\n```\n\n### 2. Min-Max Scaling (Normalization)\nScales to 0-1 range\n```python\nX_scaled = (X - min) / (max - min)\n```\n\n## Using scikit-learn\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standardization\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Use same scaler!\n\n# Min-Max\nminmax = MinMaxScaler()\nX_normalized = minmax.fit_transform(X_train)\n```\n\n## When to Scale\n\n| Algorithm | Needs Scaling? |\n|-----------|---------------|\n| KNN | ‚úÖ Yes |\n| SVM | ‚úÖ Yes |\n| Neural Networks | ‚úÖ Yes |\n| Decision Trees | ‚ùå No |\n| Random Forest | ‚ùå No |\n\n## ‚ö†Ô∏è Common Mistake\n\n**Always fit the scaler on training data only!** Then transform both train and test with the same scaler.\n\n---\n\n## üéØ Your Task\n\nScale features for a KNN classifier and compare accuracy.",
    "starter_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwine = load_wine()\nX, y = wine.data, wine.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Without scaling\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nacc_unscaled = knn.score(X_test, y_test)\n\n# With scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn_scaled = KNeighborsClassifier(n_neighbors=5)\nknn_scaled.fit(X_train_scaled, y_train)\nacc_scaled = knn_scaled.score(X_test_scaled, y_test)\n\nprint(f\"Without scaling: {acc_unscaled:.1%}\")\nprint(f\"With scaling: {acc_scaled:.1%}\")",
    "solution_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwine = load_wine()\nX, y = wine.data, wine.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Without scaling\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nacc_unscaled = knn.score(X_test, y_test)\n\n# With scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn_scaled = KNeighborsClassifier(n_neighbors=5)\nknn_scaled.fit(X_train_scaled, y_train)\nacc_scaled = knn_scaled.score(X_test_scaled, y_test)\n\nprint(f\"Without scaling: {acc_unscaled:.1%}\")\nprint(f\"With scaling: {acc_scaled:.1%}\")",
    "expected_output": "Without scaling: 69.4%\nWith scaling: 97.2%",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "110": {
    "id": 110,
    "title": "Cross Validation",
    "content": "# üîÑ Cross-Validation: Robust Model Evaluation\n\n## The Problem with Single Train/Test Split\n\nOne random split might be lucky or unlucky! Your accuracy could vary based on which data ended up in test set.\n\n## The Solution: K-Fold Cross-Validation\n\n1. Split data into K equal parts (folds)\n2. Train on K-1 folds, test on remaining fold\n3. Repeat K times (each fold is test set once)\n4. Average the K scores\n\n## Visual Example (5-Fold)\n\n```\nRound 1: [Test][Train][Train][Train][Train] ‚Üí 85%\nRound 2: [Train][Test][Train][Train][Train] ‚Üí 88%\nRound 3: [Train][Train][Test][Train][Train] ‚Üí 82%\nRound 4: [Train][Train][Train][Test][Train] ‚Üí 86%\nRound 5: [Train][Train][Train][Train][Test] ‚Üí 84%\n\nAverage: 85% ¬± 2.2%\n```\n\n## Using scikit-learn\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5)  # 5-fold CV\nprint(f\"Mean: {scores.mean():.2f} ¬± {scores.std():.2f}\")\n```\n\n## When to Use\n\n‚úÖ Limited data (can't afford to set aside 20% for test)\n‚úÖ Want more reliable accuracy estimates\n‚úÖ Hyperparameter tuning\n\n---\n\n## üéØ Your Task\n\nEvaluate a model using 5-fold cross-validation.",
    "starter_code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Create model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Perform 5-fold cross-validation\nscores = cross_val_score(knn, X, y, cv=5)\n\nprint(\"Fold scores:\", [f\"{s:.1%}\" for s in scores])\nprint(f\"Mean accuracy: {scores.mean():.1%} ¬± {scores.std():.1%}\")",
    "solution_code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Create model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Perform 5-fold cross-validation\nscores = cross_val_score(knn, X, y, cv=5)\n\nprint(\"Fold scores:\", [f\"{s:.1%}\" for s in scores])\nprint(f\"Mean accuracy: {scores.mean():.1%} ¬± {scores.std():.1%}\")",
    "expected_output": "Fold scores: ['96.7%', '100.0%', '93.3%', '96.7%', '100.0%']\nMean accuracy: 97.3% ¬± 2.5%",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "111": {
    "id": 111,
    "title": "Confusion Matrix",
    "content": "# üìä Confusion Matrix: Understanding Model Errors\n\n## What is a Confusion Matrix?\n\nA **confusion matrix** shows exactly HOW your model made mistakes‚Äînot just overall accuracy.\n\n## The 2x2 Matrix (Binary Classification)\n\n|  | Predicted: No | Predicted: Yes |\n|--|--------------|----------------|\n| **Actual: No** | TN (True Negative) | FP (False Positive) |\n| **Actual: Yes** | FN (False Negative) | TP (True Positive) |\n\n## Key Metrics\n\n- **Accuracy**: (TP + TN) / Total\n- **Precision**: TP / (TP + FP) ‚Äî \"Of predicted positives, how many were right?\"\n- **Recall**: TP / (TP + FN) ‚Äî \"Of actual positives, how many did we find?\"\n- **F1 Score**: Harmonic mean of precision and recall\n\n## Real-World Example\n\nMedical test for a disease:\n- **False Positive**: Healthy person told they're sick (stress, unnecessary treatment)\n- **False Negative**: Sick person told they're healthy (dangerous!)\n\n## Using scikit-learn\n\n```python\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ncm = confusion_matrix(y_true, y_pred)\nprint(classification_report(y_true, y_pred))\n```\n\n---\n\n## üéØ Your Task\n\nGenerate a confusion matrix and classification report for a classifier.",
    "starter_code": "from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# Load breast cancer dataset (binary classification)\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))",
    "solution_code": "from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# Load breast cancer dataset (binary classification)\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))",
    "expected_output": "Confusion Matrix:\n[[41  2]\n [ 3 68]]",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "112": {
    "id": 112,
    "title": "Complete ML Pipeline",
    "content": "# üîß Complete ML Pipeline: Putting It All Together\n\n## The ML Pipeline Steps\n\nA complete machine learning pipeline includes all steps from raw data to predictions:\n\n```\nRaw Data ‚Üí Preprocessing ‚Üí Feature Engineering ‚Üí Train/Test Split\n    ‚Üí Model Training ‚Üí Evaluation ‚Üí Deployment\n```\n\n## Step 1: Load and Explore Data\n```python\nimport pandas as pd\ndf = pd.read_csv('data.csv')\nprint(df.head())\nprint(df.describe())\n```\n\n## Step 2: Preprocess and Clean\n```python\n# Handle missing values\ndf = df.dropna()\n\n# Encode categorical variables\ndf['category'] = df['category'].map({'A': 0, 'B': 1, 'C': 2})\n```\n\n## Step 3: Feature Engineering\n```python\n# Create new features\ndf['age_squared'] = df['age'] ** 2\ndf['income_per_age'] = df['income'] / df['age']\n```\n\n## Step 4: Split Data\n```python\nfrom sklearn.model_selection import train_test_split\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n```\n\n## Step 5: Scale Features\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\n## Step 6: Train and Evaluate\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_scaled, y_train)\naccuracy = model.score(X_test_scaled, y_test)\n```\n\n---\n\n## üéØ Your Task\n\nBuild a complete ML pipeline for customer churn prediction.",
    "starter_code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Simulated customer data\nnp.random.seed(42)\nn_samples = 500\nX = np.column_stack([\n    np.random.rand(n_samples) * 100,  # tenure\n    np.random.rand(n_samples) * 1000,  # monthly_spend\n    np.random.randint(0, 5, n_samples)  # support_tickets\n])\ny = (X[:, 2] > 2).astype(int)  # Churn if > 2 tickets\n\n# Step 1: Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 2: Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 3: Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Step 4: Evaluate\ny_pred = model.predict(X_test_scaled)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.1%}\")",
    "solution_code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nnp.random.seed(42)\nn_samples = 500\nX = np.column_stack([\n    np.random.rand(n_samples) * 100,\n    np.random.rand(n_samples) * 1000,\n    np.random.randint(0, 5, n_samples)\n])\ny = (X[:, 2] > 2).astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\ny_pred = model.predict(X_test_scaled)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.1%}\")",
    "expected_output": "Accuracy: 100.0%",
    "chapter_id": 14,
    "chapter_title": "Machine Learning Intro"
  },
  "113": {
    "id": 113,
    "title": "üíÄ FINAL BOSS: Full Stack Data Scientist",
    "content": "# üíÄ FINAL BOSS: The Full Stack Data Scientist\n\nCongratulations on making it this far! You've mastered Python fundamentals, data manipulation, visualization, statistics, and machine learning. Now it's time to prove your skills by building a **complete end-to-end data science pipeline**.\n\n---\n\n## üèÜ The Ultimate Challenge\n\nYou are a data scientist at a real estate company. Your task is to build a **house price prediction model** using the skills you've learned throughout this course.\n\n## üìã Your Mission\n\nComplete ALL of the following steps:\n\n### Step 1: Create the Dataset\nGenerate synthetic house data with:\n- **Features**: House size (square feet)\n- **Target**: House price (with some realistic noise)\n- Use `np.random.seed(42)` for reproducibility\n\n### Step 2: Explore the Data\nCalculate and print basic statistics:\n- Mean and standard deviation of house sizes\n- Mean and standard deviation of house prices\n\n### Step 3: Split the Data\nUse `train_test_split` to create:\n- 80% training data\n- 20% test data\n- Use `random_state=42`\n\n### Step 4: Train a Model\n- Create a `LinearRegression` model\n- Fit it on the training data\n\n### Step 5: Evaluate Performance\n- Make predictions on test data\n- Calculate and print the R¬≤ score\n- Make a price prediction for a 2000 sqft house\n\n### Step 6: Celebrate! üéâ\nPrint a congratulations message!\n\n---\n\n## üìä Expected Output Format\n\nYour output should look like this:\n```\nSize: mean=XXXX, std=XXX\nPrice: mean=XXXXXX, std=XXXXX\n\nModel R¬≤ Score: 0.XXXX\nPrice prediction for 2000 sqft: $XXX,XXX\n\nüéâ CONGRATULATIONS! You've completed the course!\n```\n\n---\n\n## üí° Hints\n\n- Import: `numpy`, `train_test_split`, `LinearRegression`, `r2_score`\n- Use `.reshape(-1, 1)` to make arrays 2D for sklearn\n- Use f-strings with formatting: `f\"{value:.2f}\"` for 2 decimals\n- Use `{value:,.0f}` for comma-separated numbers\n\n---\n\n## üéØ Complete the Pipeline Below\n\n\n> [!TIP]\n> **Verification Tip**: We've set `np.random.seed(42)` so your random dataset matches the answer key perfectly.",
    "starter_code": "np.random.seed(42)  # üîí Fixed seed for verification\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Step 1: Create the Dataset\n# Generate 100 houses with sizes between 500-3000 sqft\n# Price = size * 100 + random noise\n\n\n# Step 2: Explore the Data\n# Print mean and std for size and price\n\n\n# Step 3: Split the Data (80/20 split)\n\n\n# Step 4: Train a Linear Regression Model\n\n\n# Step 5: Evaluate - Calculate R¬≤ and predict for 2000 sqft\n\n\n# Step 6: Print congratulations message\n",
    "solution_code": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Step 1: Create the Dataset\nnp.random.seed(42)\nsize = np.random.randint(500, 3000, 100).reshape(-1, 1)\nprice = size * 100 + np.random.normal(0, 10000, (100, 1))\n\n# Step 2: Explore the Data\nprint(f\"Size: mean={size.mean():.0f}, std={size.std():.0f}\")\nprint(f\"Price: mean={price.mean():.0f}, std={price.std():.0f}\")\n\n# Step 3: Split the Data (80/20 split)\nX_train, X_test, y_train, y_test = train_test_split(\n    size, price, test_size=0.2, random_state=42\n)\n\n# Step 4: Train a Linear Regression Model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Step 5: Evaluate - Calculate R¬≤ and predict for 2000 sqft\ny_pred = model.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f\"\\nModel R¬≤ Score: {r2:.4f}\")\nprint(f\"Price prediction for 2000 sqft: ${model.predict([[2000]])[0][0]:,.0f}\")\n\n# Step 6: Print congratulations message\nprint(\"\\nüéâ CONGRATULATIONS! You've completed the course!\")",
    "expected_output": "Size: mean=1716, std=712\nPrice: mean=171620, std=71218\n\nModel R¬≤ Score: 0.9808\nPrice prediction for 2000 sqft: $199,917\n\nüéâ CONGRATULATIONS! You've completed the course!",
    "chapter_id": 101,
    "chapter_title": "Final Boss: Data Scientist"
  },
  "114": {
    "id": 114,
    "title": "NumPy Introduction",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üî¢ Introduction to NumPy\n\n## Definition\n**NumPy** (Numerical Python) is a powerful library that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on them.\n\n## Why Do We Need NumPy?\n\nImagine you have a list of 1 million numbers and need to multiply each by 2. With regular Python lists, you'd need a loop that runs 1 million times. With NumPy, it's a single operation that runs 50x faster!\n\n## Real-World Analogy\n\nThink of NumPy arrays like a **spreadsheet column**:\n- All values are the same type (all numbers)\n- You can apply formulas to the entire column at once\n- Operations are optimized for speed\n\n## NumPy vs Python Lists\n\n| Feature | Python List | NumPy Array |\n| --- | --- | --- |\n| Speed | Slow (loops required) | **50x faster** |\n| Memory | Uses more RAM | **Efficient storage** |\n| Math Operations | Manual loops | **Vectorized (instant)** |\n| Data Types | Mixed allowed | Same type only |\n\n## How to Create Arrays\n\n```python\nimport numpy as np\n\n# From a Python list\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)  # [1 2 3 4 5]\n\n# Special arrays\nzeros = np.zeros(5)       # [0. 0. 0. 0. 0.]\nones = np.ones(3)         # [1. 1. 1.]\nrange_arr = np.arange(0, 10, 2)  # [0 2 4 6 8]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Array** | NumPy's main data structure - a grid of values |\n| **Vectorization** | Applying operations to entire arrays at once |\n| **Dimension** | The number of axes (1D = list, 2D = table) |\n\n---\n\n## üéØ Your Task\n\nCreate a NumPy array containing the numbers `10, 20, 30, 40, 50` and print it.\n\n**Hint**: Use `np.array([...])` with your numbers inside the brackets.\n",
    "starter_code": "import numpy as np\n\n# Create an array with 10, 20, 30, 40, 50\n",
    "solution_code": "import numpy as np\n\n# Create an array with 10, 20, 30, 40, 50\narr = np.array([10, 20, 30, 40, 50])\nprint(arr)",
    "expected_output": "[10 20 30 40 50]",
    "order": 1
  },
  "115": {
    "id": 115,
    "title": "Vectorized Operations",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# ‚ö° Vectorized Operations\n\n## Definition\n**Vectorization** means applying an operation to an entire array at once, without writing loops. This is NumPy's superpower!\n\n## Why Vectorization Matters\n\nIn regular Python:\n```python\nnumbers = [1, 2, 3, 4, 5]\ndoubled = []\nfor n in numbers:\n    doubled.append(n * 2)  # Slow loop!\n# doubled = [2, 4, 6, 8, 10]\n```\n\nWith NumPy (vectorized):\n```python\nimport numpy as np\narr = np.array([1, 2, 3, 4, 5])\ndoubled = arr * 2  # Instant! No loop needed!\n# [2 4 6 8 10]\n```\n\n## Real-World Analogy\n\nImagine you're a teacher grading tests:\n- **Loop way**: Grade one test, write score, pick up next test, repeat 100 times\n- **Vectorized way**: Use a grading machine that scores all 100 tests at once!\n\n## All Math Operations Work!\n\n```python\narr = np.array([10, 20, 30])\n\narr + 5    # [15 25 35]  - Add 5 to each\narr - 5    # [ 5 15 25]  - Subtract 5 from each\narr * 2    # [20 40 60]  - Multiply each by 2\narr / 10   # [1. 2. 3.]  - Divide each by 10\narr ** 2   # [100 400 900] - Square each\n```\n\n## Array-to-Array Operations\n\nYou can also operate on two arrays element-by-element:\n\n```python\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\na + b  # [5, 7, 9]\na * b  # [4, 10, 18]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Element-wise** | Operation applied to each element individually |\n| **Broadcasting** | NumPy's ability to work with different-sized arrays |\n| **Scalar** | A single number (like 5 or 2.5) |\n\n---\n\n## üéØ Your Task\n\nCreate an array `[2, 4, 6, 8]` and multiply every element by 3. Print the result.\n\n**Expected output**: `[ 6 12 18 24]`\n",
    "starter_code": "import numpy as np\n\n# Create array and multiply by 3\n",
    "solution_code": "import numpy as np\n\narr = np.array([2, 4, 6, 8])\nresult = arr * 3\nprint(result)",
    "expected_output": "[ 6 12 18 24]",
    "order": 2
  },
  "116": {
    "id": 116,
    "title": "Boolean Indexing",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üéØ Array Indexing & Boolean Filtering\n\n## Definition\n**Indexing** lets you access specific elements. **Boolean filtering** lets you select elements that match a condition‚Äîthis is one of NumPy's most powerful features!\n\n## Basic Indexing (Like Lists)\n\n```python\nimport numpy as np\narr = np.array([10, 20, 30, 40, 50])\n\narr[0]     # 10 (first element)\narr[-1]    # 50 (last element)\narr[1:4]   # [20 30 40] (slice from index 1 to 3)\narr[::2]   # [10 30 50] (every other element)\n```\n\n## Boolean Filtering (The Magic!)\n\nThe real power: select elements that match a condition!\n\n```python\narr = np.array([15, 25, 35, 45, 55])\n\n# Get all elements greater than 30\nbig = arr[arr > 30]  # [35 45 55]\n\n# Get all even elements\narr = np.array([1, 2, 3, 4, 5, 6])\nevens = arr[arr % 2 == 0]  # [2 4 6]\n```\n\n## Real-World Analogy\n\nThink of boolean filtering like a **coffee filter**:\n- You pour in a mix of coffee and water (your array)\n- The filter only lets through what matches the criteria (the coffee!)\n- Everything else stays behind\n\n## How It Works Under the Hood\n\n```python\narr = np.array([10, 20, 30, 40])\n\n# Step 1: Create a boolean mask\nmask = arr > 25  # [False, False, True, True]\n\n# Step 2: Use mask to filter\nresult = arr[mask]  # [30, 40]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Index** | Position of an element (starts at 0) |\n| **Slice** | A range of elements (like `arr[1:4]`) |\n| **Boolean Mask** | An array of True/False values used for filtering |\n\n---\n\n## üéØ Your Task\n\nGiven the array `[5, 10, 15, 20, 25, 30]`, use boolean indexing to get only values greater than 15. Print the result.\n\n**Expected output**: `[20 25 30]`\n",
    "starter_code": "import numpy as np\n\narr = np.array([5, 10, 15, 20, 25, 30])\n\n# Get values > 15\n",
    "solution_code": "import numpy as np\n\narr = np.array([5, 10, 15, 20, 25, 30])\nresult = arr[arr > 15]\nprint(result)",
    "expected_output": "[20 25 30]",
    "order": 3
  },
  "117": {
    "id": 117,
    "title": "Broadcasting",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üì° Broadcasting\n\n## Definition\n**Broadcasting** is NumPy's ability to perform operations on arrays of different shapes. It \"broadcasts\" the smaller array across the larger one.\n\n## Why Broadcasting is Powerful\n\nWithout broadcasting, you'd need matching array sizes. With broadcasting, NumPy automatically expands the smaller array:\n\n```python\nimport numpy as np\n\n# Add a scalar to every element (scalar broadcasts)\narr = np.array([1, 2, 3, 4, 5])\nresult = arr + 10  # [11, 12, 13, 14, 15]\n```\n\n## Real-World Analogy\n\nImagine you're giving everyone in a class a $5 bonus:\n- **Without broadcasting**: Write \"$5\" on 30 separate sticky notes\n- **With broadcasting**: Announce \"$5 for everyone!\" once\n\n## Practical Example: Centering Data\n\nA common data science task is \"centering\" data‚Äîsubtracting the mean so the new mean is 0:\n\n```python\ndata = np.array([100, 200, 300, 400, 500])\n\n# Calculate the mean\nmean = np.mean(data)  # 300.0\n\n# Subtract mean from ALL elements (broadcasting!)\ncentered = data - mean\n# [-200. -100.    0.  100.  200.]\n\n# Now the mean is 0!\nprint(np.mean(centered))  # 0.0\n```\n\n## 2D Broadcasting\n\nBroadcasting works with matrices too:\n\n```python\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Add 10 to every element\nresult = matrix + 10\n# [[11 12 13]\n#  [14 15 16]\n#  [17 18 19]]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Broadcasting** | Automatic expansion of arrays for operations |\n| **Centering** | Subtracting the mean from all values |\n| **Normalization** | Scaling data to a standard range |\n\n---\n\n## üéØ Your Task\n\nGiven `[10, 20, 30, 40, 50]`:\n1. Calculate its mean using `np.mean()`\n2. Subtract the mean from the array (centering it)\n3. Print the centered array\n\n**Expected output**: `[-20. -10.   0.  10.  20.]`\n",
    "starter_code": "import numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\n\n# Calculate mean and center\n",
    "solution_code": "import numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\nmean = np.mean(data)\ncentered = data - mean\nprint(centered)",
    "expected_output": "[-20. -10.   0.  10.  20.]",
    "order": 4
  },
  "118": {
    "id": 118,
    "title": "NumPy Statistics",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìä NumPy Statistical Functions\n\n## Definition\nNumPy provides fast, built-in functions for common statistical calculations‚Äîno loops required!\n\n## Why Use NumPy for Statistics?\n\n| Pure Python | NumPy |\n| --- | --- |\n| `sum(my_list)` | `np.sum(arr)` ‚Üê **10x faster** |\n| Manual loop for mean | `np.mean(arr)` ‚Üê **One line** |\n| No built-in std | `np.std(arr)` ‚Üê **Instant** |\n\n## Common Statistical Functions\n\n```python\nimport numpy as np\narr = np.array([10, 20, 30, 40, 50])\n\nnp.sum(arr)    # 150 - Total of all elements\nnp.mean(arr)   # 30.0 - Average\nnp.std(arr)    # 14.14 - Standard deviation\nnp.min(arr)    # 10 - Minimum value\nnp.max(arr)    # 50 - Maximum value\nnp.median(arr) # 30.0 - Middle value\n```\n\n## Real-World Analogy\n\nThink of NumPy stats like a **calculator with special buttons**:\n- Instead of typing `10 + 20 + 30 + 40 + 50 =` then `√∑ 5 =`\n- Just press the **MEAN** button and it does everything!\n\n## Aggregation on 2D Arrays\n\nFor matrices, you can aggregate along axes:\n\n```python\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6]])\n\nnp.sum(matrix)           # 21 (all elements)\nnp.sum(matrix, axis=0)   # [5, 7, 9] (sum each column)\nnp.sum(matrix, axis=1)   # [6, 15] (sum each row)\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Aggregation** | Combining values into a single result |\n| **Axis** | The direction of operation (0=columns, 1=rows) |\n| **Reduction** | Operations that reduce array size (like sum) |\n\n---\n\n## üéØ Your Task\n\nCreate an array `[100, 200, 300, 400, 500]` and print:\n1. The sum\n2. The mean  \n3. The maximum\n\nPrint each value on a new line.\n",
    "starter_code": "import numpy as np\n\narr = np.array([100, 200, 300, 400, 500])\n\n# Print sum, mean, max\n",
    "solution_code": "import numpy as np\n\narr = np.array([100, 200, 300, 400, 500])\nprint(np.sum(arr))\nprint(np.mean(arr))\nprint(np.max(arr))",
    "expected_output": "1500\n300.0\n500",
    "order": 5
  },
  "119": {
    "id": 119,
    "title": "Handling Missing Data",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üßπ Handling Missing Data with dropna()\n\n## Definition\n**Missing data** appears in real datasets as `NaN` (Not a Number) or `None`. The `dropna()` method removes rows containing missing values.\n\n## Why Missing Data Matters\n\nReal-world data is messy! Surveys have unanswered questions, sensors malfunction, data entry has errors. You must handle missing values before analysis, or your calculations will be wrong.\n\n## Real-World Analogy\n\nThink of a class attendance sheet:\n- Some students forgot to sign in (missing data!)\n- If you count only signed rows, you get accurate attendance\n- `dropna()` is like only counting the complete rows\n\n## Detecting Missing Values\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', None],\n    'age': [25, None, 30]\n})\n\n# Check for missing values\nprint(df.isnull())\n#     name    age\n# 0  False  False\n# 1  False   True\n# 2   True  False\n\n# Count missing per column\nprint(df.isnull().sum())\n# name    1\n# age     1\n```\n\n## Using dropna()\n\n```python\n# Drop rows with ANY missing value\ndf_clean = df.dropna()\n\n# Drop rows only if ALL values are missing\ndf_clean = df.dropna(how='all')\n\n# Drop based on specific column\ndf_clean = df.dropna(subset=['age'])\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **NaN** | \"Not a Number\" - represents missing data |\n| **dropna()** | Method to remove rows with missing values |\n| **isnull()** | Method to detect missing values (returns True/False) |\n\n---\n\n## üéØ Your Task\n\nGiven a DataFrame with missing values in 'product' and 'price' columns, use `dropna()` to remove rows with any missing data. Print the cleaned DataFrame.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', None, 'Orange'],\n    'price': [1.0, 0.5, 0.75, None]\n})\n\n# Drop rows with missing values\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', None, 'Orange'],\n    'price': [1.0, 0.5, 0.75, None]\n})\nclean_df = df.dropna()\nprint(clean_df)",
    "expected_output": "  product  price\n0   Apple    1.0\n1  Banana    0.5",
    "order": 1
  },
  "120": {
    "id": 120,
    "title": "Filling Missing Values",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üîß Filling Missing Values with fillna()\n\n## Definition\n**fillna()** replaces missing values with a specified value instead of dropping the entire row. This preserves more data!\n\n## Why Fill Instead of Drop?\n\n| Situation | Best Approach |\n| --- | --- |\n| Few missing values | Fill with mean/median |\n| Categorical data | Fill with mode or \"Unknown\" |\n| Time series | Forward/backward fill |\n| Critical analysis | Drop (if you need complete data) |\n\n## Real-World Analogy\n\nImagine a survey where some people skipped the age question:\n- **Drop approach**: Throw away their entire survey (loses data!)\n- **Fill approach**: Estimate their age from the average (preserves other answers)\n\n## Filling Strategies\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'score': [85, None, 90, None, 75]\n})\n\n# Fill with a specific value\ndf['score'].fillna(0)  # Replace None with 0\n\n# Fill with the mean\nmean_score = df['score'].mean()  # 83.33\ndf['score'].fillna(mean_score)\n\n# Fill with the median (good for outliers)\ndf['score'].fillna(df['score'].median())\n\n# Forward fill (use previous value)\ndf['score'].fillna(method='ffill')\n\n# Backward fill (use next value)\ndf['score'].fillna(method='bfill')\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **fillna()** | Method to replace NaN with a value |\n| **Imputation** | The process of replacing missing values |\n| **Forward fill** | Copy the previous valid value |\n\n---\n\n## üéØ Your Task\n\nGiven a DataFrame with missing scores, calculate the mean of existing scores and use `fillna()` to replace missing values with that mean. Print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'student': ['Alice', 'Bob', 'Carol', 'David'],\n    'score': [80.0, None, 90.0, None]\n})\n\n# Fill missing with mean\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'student': ['Alice', 'Bob', 'Carol', 'David'],\n    'score': [80.0, None, 90.0, None]\n})\nmean_score = df['score'].mean()\ndf['score'] = df['score'].fillna(mean_score)\nprint(df)",
    "expected_output": "  student  score\n0   Alice   80.0\n1     Bob   85.0\n2   Carol   90.0\n3   David   85.0",
    "order": 2
  },
  "121": {
    "id": 121,
    "title": "Removing Duplicates",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üóëÔ∏è Removing Duplicates with drop_duplicates()\n\n## Definition\n**Duplicate rows** are exact copies of other rows. `drop_duplicates()` removes them, keeping only unique rows.\n\n## Why Duplicates Are Dangerous\n\nDuplicates can:\n- **Skew averages**: Count the same person twice = wrong average\n- **Inflate counts**: \"100 customers\" might really be 80 unique people\n- **Waste memory**: Storing the same data multiple times\n\n## Real-World Analogy\n\nImagine a guest list where some people RSVP'd multiple times:\n- Without removing duplicates, you'd order too much food!\n- `drop_duplicates()` ensures you count each guest only once\n\n## Finding Duplicates\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Alice', 'Carol'],\n    'age': [25, 30, 25, 28]\n})\n\n# Check for duplicates\nprint(df.duplicated())\n# 0    False\n# 1    False\n# 2     True  <- This row is a duplicate of row 0!\n# 3    False\n\n# Count duplicates\nprint(df.duplicated().sum())  # 1\n```\n\n## Removing Duplicates\n\n```python\n# Remove duplicate rows (keeps first occurrence)\ndf_unique = df.drop_duplicates()\n\n# Keep last occurrence instead\ndf_unique = df.drop_duplicates(keep='last')\n\n# Check for duplicates in specific columns only\ndf_unique = df.drop_duplicates(subset=['name'])\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Duplicate** | A row that exactly matches another row |\n| **drop_duplicates()** | Method to remove duplicate rows |\n| **keep** | Parameter to choose which duplicate to keep ('first' or 'last') |\n\n---\n\n## üéØ Your Task\n\nGiven a DataFrame with duplicate city/temperature entries, use `drop_duplicates()` to remove the duplicate rows. Print the unique DataFrame.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'city': ['NYC', 'LA', 'NYC', 'Chicago', 'LA'],\n    'temp': [75, 85, 75, 65, 85]\n})\n\n# Remove duplicates\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'city': ['NYC', 'LA', 'NYC', 'Chicago', 'LA'],\n    'temp': [75, 85, 75, 65, 85]\n})\ndf_unique = df.drop_duplicates()\nprint(df_unique)",
    "expected_output": "      city  temp\n0      NYC    75\n1       LA    85\n3  Chicago    65",
    "order": 3
  },
  "122": {
    "id": 122,
    "title": "String Cleanup",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# ‚úÇÔ∏è String Cleanup Methods\n\n## Definition\nReal-world text data is often messy‚Äîextra spaces, inconsistent capitalization, special characters. Pandas provides `.str` methods to clean strings efficiently.\n\n## Why String Cleanup Matters\n\n| Raw Data | Problem | Clean Data |\n| --- | --- | --- |\n| \"  Alice  \" | Extra spaces | \"Alice\" |\n| \"BOB\" + \"bob\" | Won't match! | \"Bob\" + \"Bob\" |\n| \"john DOE\" | Inconsistent | \"John Doe\" |\n\n## Real-World Analogy\n\nThink of string cleanup like a **spell checker**:\n- It finds formatting issues (extra spaces, wrong case)\n- It suggests corrections (strip, lowercase, etc.)\n- You apply the fix to the entire column at once\n\n## Common String Methods\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['  Alice  ', 'BOB', 'carol  ']\n})\n\n# Strip whitespace from both ends\ndf['name'] = df['name'].str.strip()\n# ['Alice', 'BOB', 'carol']\n\n# Convert to lowercase\ndf['name'] = df['name'].str.lower()\n# ['alice', 'bob', 'carol']\n\n# Convert to uppercase\ndf['name'] = df['name'].str.upper()\n# ['ALICE', 'BOB', 'CAROL']\n\n# Convert to title case (first letter capitalized)\ndf['name'] = df['name'].str.title()\n# ['Alice', 'Bob', 'Carol']\n```\n\n## Chaining Methods\n\nYou can chain multiple string operations:\n\n```python\ndf['name'] = df['name'].str.strip().str.title()\n# ' john DOE ' ‚Üí 'John Doe'\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **.str** | Accessor for string methods on a Series |\n| **strip()** | Remove leading/trailing whitespace |\n| **title()** | Capitalize first letter of each word |\n\n---\n\n## üéØ Your Task\n\nGiven names with extra whitespace and inconsistent capitalization, chain `.str.strip()` and `.str.title()` to clean them up. Print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['  john DOE  ', 'JANE smith', '  bob WILSON']\n})\n\n# Clean names\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['  john DOE  ', 'JANE smith', '  bob WILSON']\n})\ndf['name'] = df['name'].str.strip().str.title()\nprint(df)",
    "expected_output": "          name\n0     John Doe\n1   Jane Smith\n2   Bob Wilson",
    "order": 4
  },
  "123": {
    "id": 123,
    "title": "Type Conversion",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üîÑ Data Type Conversion with astype()\n\n## Definition\n**Data types** determine what operations you can perform. Numbers stored as strings can't be used for math! `astype()` converts columns to the correct type.\n\n## Why Types Matter\n\n| Data | Type | Problem |\n| --- | --- | --- |\n| \"25\" | String | Can't do: \"25\" + \"30\" = \"2530\" (concatenation!) |\n| 25 | Integer | Can do: 25 + 30 = 55 ‚úì |\n| \"2024-01-15\" | String | Can't calculate date differences |\n\n## Real-World Analogy\n\nThink of data types like **plug adapters**:\n- A European plug won't fit a US outlet\n- You need an adapter (type conversion) to make it work\n- `astype()` is your universal adapter!\n\n## Checking Current Types\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'quantity': ['10', '20', '30'],  # Strings!\n    'price': [5.0, 10.0, 15.0]\n})\n\nprint(df.dtypes)\n# quantity    object   <- 'object' usually means string!\n# price       float64  <- Correct numeric type\n```\n\n## Converting Types\n\n```python\n# Convert to integer\ndf['quantity'] = df['quantity'].astype(int)\n\n# Convert to float\ndf['quantity'] = df['quantity'].astype(float)\n\n# Convert to string\ndf['price'] = df['price'].astype(str)\n\n# Now you can do math!\ndf['total'] = df['quantity'] * df['price']\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **dtype** | The data type of a column |\n| **astype()** | Method to convert to a different type |\n| **object** | Pandas type for strings (and mixed types) |\n\n---\n\n## üéØ Your Task\n\nGiven quantities stored as strings, convert them to integers using `astype(int)`, then calculate the total (quantity √ó price) and print the sum.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Orange'],\n    'quantity': ['5', '10', '8'],\n    'price': [1.0, 0.5, 0.75]\n})\n\n# Convert and calculate\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Orange'],\n    'quantity': ['5', '10', '8'],\n    'price': [1.0, 0.5, 0.75]\n})\ndf['quantity'] = df['quantity'].astype(int)\ndf['total'] = df['quantity'] * df['price']\nprint(df['total'].sum())",
    "expected_output": "16.0",
    "order": 5
  },
  "124": {
    "id": 124,
    "title": "üîç EDA Capstone",
    "chapter_id": 97,
    "chapter_title": "EDA Mini-Boss",
    "content": "# üö¢ EDA Capstone: Titanic Analysis\n\n## What is EDA?\n**Exploratory Data Analysis (EDA)** is the process of examining a dataset to understand its main characteristics‚Äîbefore building models or drawing conclusions.\n\n## Why This Dataset?\n\nThe Titanic dataset is the \"Hello World\" of data science. Every data scientist has analyzed it. You'll learn to:\n- Summarize data quickly\n- Calculate statistics by groups\n- Draw insights from numbers\n\n## Real-World Analogy\n\nThink of EDA like being a **detective**:\n- You examine the evidence (data)\n- You look for patterns (statistics)\n- You form hypotheses (insights)\n- You present your findings (reports)\n\n## The Dataset\n\nWe'll use a simplified Titanic-style dataset:\n- `survived`: 0 = Did not survive, 1 = Survived\n- `pclass`: Passenger class (1 = First, 2 = Second, 3 = Third)\n\n## Key EDA Techniques\n\n```python\nimport pandas as pd\n\n# 1. Check the size\nlen(df)  # Number of rows\n\n# 2. Calculate overall statistics\ndf['survived'].mean()  # Survival rate (0.0 to 1.0)\n\n# 3. Group by and aggregate\ndf.groupby('pclass')['survived'].mean()\n# Shows survival rate for each class\n```\n\n## What You'll Discover\n\nHistorical fact: **First-class passengers had much higher survival rates** than third-class passengers. Your analysis will reveal this pattern!\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **EDA** | Exploratory Data Analysis |\n| **groupby()** | Split data into groups for analysis |\n| **Aggregation** | Summarizing groups (mean, sum, count) |\n\n---\n\n## üéØ Your Mission\n\n1. Print the total number of passengers\n2. Calculate and print the overall survival rate (as a percentage)\n3. Print the survival rate by passenger class\n\n**Hint**: Use `df.groupby('pclass')['survived'].mean()` for step 3!\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'survived': [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0],\n    'pclass': [1, 3, 3, 1, 3, 2, 1, 3, 3, 2, 3, 2]\n})\n\n# Analyze!\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'survived': [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0],\n    'pclass': [1, 3, 3, 1, 3, 2, 1, 3, 3, 2, 3, 2]\n})\nprint(f\"Total passengers: {len(df)}\")\nprint(f\"Survival rate: {df['survived'].mean()*100:.1f}%\")\nprint(\"Survival by class:\")\nfor c, r in df.groupby('pclass')['survived'].mean().items():\n    print(f\"  Class {c}: {r*100:.1f}%\")",
    "expected_output": "Total passengers: 12\nSurvival rate: 41.7%\nSurvival by class:\n  Class 1: 100.0%\n  Class 2: 33.3%\n  Class 3: 0.0%",
    "order": 1
  },
  "125": {
    "id": 125,
    "title": "Creating Special Arrays",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üî¢ Creating Special Arrays\n\n## Definition\nNumPy provides convenient functions to create arrays filled with specific values or patterns‚Äîwithout manually typing each element.\n\n## Why Special Arrays?\n\nWhen working with data science, you often need:\n- Arrays of zeros (for initialization)\n- Arrays of ones (for masks or scaling)\n- Ranges of numbers (for plotting x-axes)\n\n## Real-World Analogy\n\nThink of special arrays like **pre-printed forms**:\n- Instead of drawing 100 blank boxes, use a pre-printed template\n- NumPy's special functions are your templates!\n\n## Special Array Functions\n\n```python\nimport numpy as np\n\n# Array of zeros\nzeros = np.zeros(5)     # [0. 0. 0. 0. 0.]\nzeros_2d = np.zeros((2, 3))  # 2 rows, 3 columns of zeros\n\n# Array of ones\nones = np.ones(4)       # [1. 1. 1. 1.]\n\n# Array of a specific value\nfives = np.full(5, 5)   # [5 5 5 5 5]\n\n# Range of numbers\nrange_arr = np.arange(0, 10, 2)  # [0 2 4 6 8]\n\n# Evenly spaced numbers\nlinspace = np.linspace(0, 1, 5)  # [0. 0.25 0.5 0.75 1.]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **zeros()** | Create array filled with 0s |\n| **ones()** | Create array filled with 1s |\n| **arange()** | Create array with range (like Python's range) |\n| **linspace()** | Create array with evenly spaced values |\n\n---\n\n## üéØ Your Task\n\nCreate an array of 5 ones using `np.ones()` and print it.\n\n**Expected output**: `[1. 1. 1. 1. 1.]`\n",
    "starter_code": "import numpy as np\n\n# Create an array of 5 ones\n",
    "solution_code": "import numpy as np\n\n# Create an array of 5 ones\narr = np.ones(5)\nprint(arr)",
    "expected_output": "[1. 1. 1. 1. 1.]",
    "order": 2
  },
  "126": {
    "id": 126,
    "title": "Array Shapes and Dimensions",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìê Array Shapes and Dimensions\n\n## Definition\n**Shape** describes the size of each dimension of an array. **Dimensions** (or axes) are the number of directions in which the array extends.\n\n## Why Shape Matters\n\nUnderstanding shape is crucial for:\n- Debugging errors (\"shapes don't match!\")\n- Reshaping data for machine learning models\n- Understanding what operations are possible\n\n## Real-World Analogy\n\nThink of shape like describing a **box**:\n- 1D array: A line of boxes (just length)\n- 2D array: A grid of boxes (rows √ó columns)\n- 3D array: A cube of boxes (depth √ó rows √ó columns)\n\n## Checking Shape\n\n```python\nimport numpy as np\n\narr_1d = np.array([1, 2, 3, 4, 5])\nprint(arr_1d.shape)  # (5,) - 1D with 5 elements\n\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr_2d.shape)  # (2, 3) - 2 rows, 3 columns\n\nprint(arr_2d.ndim)   # 2 - number of dimensions\nprint(arr_2d.size)   # 6 - total number of elements\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **shape** | Tuple describing array dimensions |\n| **ndim** | Number of dimensions (axes) |\n| **size** | Total number of elements |\n\n---\n\n## üéØ Your Task\n\nCreate the array `[10, 20, 30, 40]` and print its shape.\n\n**Expected output**: `(4,)`\n",
    "starter_code": "import numpy as np\n\n# Create array and print its shape\n",
    "solution_code": "import numpy as np\n\n# Create array and print its shape\narr = np.array([10, 20, 30, 40])\nprint(arr.shape)",
    "expected_output": "(4,)",
    "order": 3
  },
  "127": {
    "id": 127,
    "title": "Array Arithmetic",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# ‚ûï Array Arithmetic\n\n## Definition\nNumPy arrays support all standard arithmetic operations, applied element-by-element automatically.\n\n## Why Array Arithmetic?\n\nInstead of loops:\n```python\n# Slow Python way\nresult = []\nfor i in range(len(a)):\n    result.append(a[i] + b[i])\n```\n\nUse vectorized operations:\n```python\nresult = a + b  # Instant!\n```\n\n## Real-World Analogy\n\nImagine you're adjusting all prices in a store by 10%:\n- **Loop way**: Visit each item, calculate new price, update tag\n- **Vectorized way**: Apply formula to entire inventory at once!\n\n## All Operations\n\n```python\nimport numpy as np\n\na = np.array([10, 20, 30])\nb = np.array([1, 2, 3])\n\na + b   # [11 22 33] - Addition\na - b   # [ 9 18 27] - Subtraction\na * b   # [10 40 90] - Multiplication\na / b   # [10. 10. 10.] - Division\na // b  # [10 10 10] - Floor division\na % b   # [0 0 0] - Modulo\na ** b  # [10 400 27000] - Power\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Element-wise** | Operation on each corresponding pair |\n| **Operator overloading** | Standard operators work on arrays |\n\n---\n\n## üéØ Your Task\n\nCreate two arrays `[5, 10, 15]` and `[1, 2, 3]`, add them together, and print the result.\n\n**Expected output**: `[ 6 12 18]`\n",
    "starter_code": "import numpy as np\n\n# Create two arrays and add them\n",
    "solution_code": "import numpy as np\n\n# Create two arrays and add them\na = np.array([5, 10, 15])\nb = np.array([1, 2, 3])\nprint(a + b)",
    "expected_output": "[ 6 12 18]",
    "order": 5
  },
  "128": {
    "id": 128,
    "title": "Universal Functions (ufuncs)",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# ‚ö° Universal Functions (ufuncs)\n\n## Definition\n**Universal functions (ufuncs)** are NumPy functions that operate element-by-element on arrays, providing fast vectorized operations.\n\n## Why ufuncs?\n\nThey're optimized in C, making them much faster than Python loops for mathematical operations.\n\n## Real-World Analogy\n\nThink of ufuncs like a **parallel processor**:\n- Instead of calculating sin(x) for each value one at a time\n- Calculate sin() for ALL values simultaneously!\n\n## Common ufuncs\n\n```python\nimport numpy as np\n\narr = np.array([0, 30, 45, 60, 90])\nradians = np.radians(arr)  # Convert to radians\n\nnp.sin(radians)   # Sine of each element\nnp.cos(radians)   # Cosine of each element\nnp.sqrt(arr)      # Square root of each\nnp.exp(arr)       # e^x for each\nnp.log(arr + 1)   # Natural log (added 1 to avoid log(0))\nnp.abs(arr)       # Absolute value\n```\n\n## Practical Example\n\n```python\narr = np.array([1, 4, 9, 16, 25])\nsqrt_arr = np.sqrt(arr)  # [1. 2. 3. 4. 5.]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **ufunc** | Universal function - vectorized operation |\n| **np.sqrt()** | Square root of each element |\n| **np.abs()** | Absolute value of each element |\n\n---\n\n## üéØ Your Task\n\nCreate the array `[1, 4, 9, 16]` and print the square root of each element using `np.sqrt()`.\n\n**Expected output**: `[1. 2. 3. 4.]`\n",
    "starter_code": "import numpy as np\n\n# Create array and calculate square roots\n",
    "solution_code": "import numpy as np\n\n# Create array and calculate square roots\narr = np.array([1, 4, 9, 16])\nprint(np.sqrt(arr))",
    "expected_output": "[1. 2. 3. 4.]",
    "order": 6
  },
  "129": {
    "id": 129,
    "title": "Multiple Conditions",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üîó Filtering with Multiple Conditions\n\n## Definition\nYou can combine multiple boolean conditions using `&` (and), `|` (or), and `~` (not) to create complex filters.\n\n## Why Multiple Conditions?\n\nReal data analysis often requires complex queries:\n- \"Sales greater than $100 AND in California\"\n- \"Age under 18 OR over 65\"\n\n## Important: Use & | ~ NOT and/or/not\n\n```python\nimport numpy as np\n\narr = np.array([5, 15, 25, 35, 45])\n\n# WRONG: and/or don't work with arrays\n# arr[(arr > 10) and (arr < 40)]  # Error!\n\n# CORRECT: Use & | ~\narr[(arr > 10) & (arr < 40)]  # [15 25 35]\narr[(arr < 10) | (arr > 40)]  # [5 45]\narr[~(arr > 20)]              # [5 15] (NOT greater than 20)\n```\n\n## Real-World Analogy\n\nThink of conditions like **layered filters**:\n- First filter: Keep items > 10\n- Second filter: Keep items < 40\n- Combined: Only items passing BOTH filters remain\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **&** | AND - both conditions must be true |\n| **\\|** | OR - at least one condition true |\n| **~** | NOT - inverts the condition |\n\n---\n\n## üéØ Your Task\n\nFrom `[2, 8, 15, 22, 30]`, select values that are greater than 5 AND less than 25. Print the result.\n\n**Expected output**: `[ 8 15 22]`\n",
    "starter_code": "import numpy as np\n\narr = np.array([2, 8, 15, 22, 30])\n\n# Filter: > 5 AND < 25\n",
    "solution_code": "import numpy as np\n\narr = np.array([2, 8, 15, 22, 30])\n\n# Filter: > 5 AND < 25\nresult = arr[(arr > 5) & (arr < 25)]\nprint(result)",
    "expected_output": "[ 8 15 22]",
    "order": 8
  },
  "130": {
    "id": 130,
    "title": "np.where() Conditional Selection",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üîÄ Conditional Selection with np.where()\n\n## Definition\n`np.where()` returns indices where a condition is true, or can replace values based on a condition.\n\n## Why np.where()?\n\nIt's like Excel's IF function for arrays:\n- If condition is true ‚Üí use value A\n- If condition is false ‚Üí use value B\n\n## Real-World Analogy\n\nThink of `np.where()` like a **sorting hat**:\n- Check each student (element)\n- Send to Gryffindor (value A) or Slytherin (value B) based on criteria\n\n## Two Uses of np.where()\n\n```python\nimport numpy as np\n\narr = np.array([10, 25, 30, 15, 40])\n\n# Use 1: Get INDICES where condition is true\nindices = np.where(arr > 20)  # (array([1, 2, 4]),)\n\n# Use 2: Replace values conditionally\nresult = np.where(arr > 20, 'big', 'small')\n# ['small' 'big' 'big' 'small' 'big']\n\n# Use 3: Replace with calculated values\ndoubled = np.where(arr > 20, arr * 2, arr)\n# [10 50 60 15 80] - only values > 20 are doubled\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **np.where(condition)** | Returns indices where True |\n| **np.where(cond, a, b)** | Returns a where True, b where False |\n\n---\n\n## üéØ Your Task\n\nGiven `[10, 20, 30, 40, 50]`, use `np.where()` to replace values > 25 with 100, and keep others unchanged. Print the result.\n\n**Expected output**: `[ 10  20 100 100 100]`\n",
    "starter_code": "import numpy as np\n\narr = np.array([10, 20, 30, 40, 50])\n\n# Replace values > 25 with 100\n",
    "solution_code": "import numpy as np\n\narr = np.array([10, 20, 30, 40, 50])\n\n# Replace values > 25 with 100\nresult = np.where(arr > 25, 100, arr)\nprint(result)",
    "expected_output": "[ 10  20 100 100 100]",
    "order": 9
  },
  "131": {
    "id": 131,
    "title": "Normalizing Data",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìè Normalizing Data (Min-Max Scaling)\n\n## Definition\n**Normalization** scales data to a specific range (usually 0-1). This is essential for machine learning where features need to be on similar scales.\n\n## Why Normalize?\n\n| Raw Data | Problem |\n| --- | --- |\n| Age: 0-100 | |\n| Income: 0-1,000,000 | Income dominates because of larger values! |\n\nAfter normalization, both range from 0 to 1.\n\n## Real-World Analogy\n\nThink of grading on a curve:\n- Raw scores: 45, 67, 89, 23\n- Normalized: Everyone scaled relative to highest score\n\n## Min-Max Normalization Formula\n\n```\nnormalized = (x - min) / (max - min)\n```\n\n```python\nimport numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\n\n# Normalize to 0-1 range\ndata_min = np.min(data)  # 10\ndata_max = np.max(data)  # 50\n\nnormalized = (data - data_min) / (data_max - data_min)\n# [0.   0.25 0.5  0.75 1.  ]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Normalization** | Scaling to a standard range |\n| **Min-Max Scaling** | Scale to 0-1 based on min/max |\n| **Feature Scaling** | Making features comparable |\n\n---\n\n## üéØ Your Task\n\nNormalize `[0, 25, 50, 75, 100]` to the range 0-1 using min-max scaling. Print the result.\n\n**Expected output**: `[0.   0.25 0.5  0.75 1.  ]`\n",
    "starter_code": "import numpy as np\n\ndata = np.array([0, 25, 50, 75, 100])\n\n# Normalize to 0-1\n",
    "solution_code": "import numpy as np\n\ndata = np.array([0, 25, 50, 75, 100])\n\n# Normalize to 0-1\nnormalized = (data - np.min(data)) / (np.max(data) - np.min(data))\nprint(normalized)",
    "expected_output": "[0.   0.25 0.5  0.75 1.  ]",
    "order": 11
  },
  "132": {
    "id": 132,
    "title": "Standardizing Data (Z-Score)",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìä Standardizing Data (Z-Score)\n\n## Definition\n**Standardization** transforms data to have mean=0 and standard deviation=1. This is another common preprocessing step.\n\n## Why Standardize?\n\nZ-scores tell you how many standard deviations a value is from the mean:\n- Z = 0 ‚Üí exactly at the mean\n- Z = 1 ‚Üí one std deviation above mean\n- Z = -2 ‚Üí two std deviations below mean\n\n## Real-World Analogy\n\nComparing test scores from different classes:\n- Class A: Mean 70, your score 85 (you're +15 above mean)\n- Class B: Mean 50, your score 65 (you're +15 above mean too)\n- But which is more impressive? Z-scores tell you!\n\n## Z-Score Formula\n\n```\nz = (x - mean) / std\n```\n\n```python\nimport numpy as np\n\ndata = np.array([50, 60, 70, 80, 90])\n\nmean = np.mean(data)  # 70\nstd = np.std(data)    # 14.14\n\nz_scores = (data - mean) / std\n# [-1.41 -0.71  0.    0.71  1.41]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Z-score** | How many std deviations from mean |\n| **Standardization** | Transform to mean=0, std=1 |\n| **Standard deviation** | Measure of spread |\n\n---\n\n## üéØ Your Task\n\nCalculate z-scores for `[60, 70, 80, 90, 100]`. Print the result rounded to 2 decimals.\n\n**Hint**: Use `np.round(result, 2)`\n",
    "starter_code": "import numpy as np\n\ndata = np.array([60, 70, 80, 90, 100])\n\n# Calculate z-scores\n",
    "solution_code": "import numpy as np\n\ndata = np.array([60, 70, 80, 90, 100])\n\n# Calculate z-scores\nmean = np.mean(data)\nstd = np.std(data)\nz_scores = (data - mean) / std\nprint(np.round(z_scores, 2))",
    "expected_output": "[-1.41 -0.71  0.    0.71  1.41]",
    "order": 12
  },
  "133": {
    "id": 133,
    "title": "Percentiles and Quartiles",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìà Percentiles and Quartiles\n\n## Definition\n**Percentiles** divide data into 100 equal parts. The p-th percentile is the value below which p% of data falls.\n\n## Why Percentiles?\n\n- \"You scored in the 90th percentile\" = You beat 90% of test takers\n- Quartiles (25th, 50th, 75th) are used in box plots\n- Detect outliers (values beyond 1.5√ó IQR)\n\n## Real-World Analogy\n\nImagine ranking all students by height:\n- 50th percentile = Median height (half are shorter)\n- 75th percentile = Taller than 75% of students\n- 25th percentile = Taller than only 25%\n\n## Using np.percentile()\n\n```python\nimport numpy as np\n\ndata = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n\nnp.percentile(data, 50)  # 55.0 (median)\nnp.percentile(data, 25)  # 32.5 (Q1)\nnp.percentile(data, 75)  # 77.5 (Q3)\nnp.percentile(data, 90)  # 91.0 (90th percentile)\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Percentile** | Value below which X% of data falls |\n| **Quartile** | 25th (Q1), 50th (Q2/median), 75th (Q3) |\n| **IQR** | Interquartile Range = Q3 - Q1 |\n\n---\n\n## üéØ Your Task\n\nFind the 50th percentile (median) of `[10, 20, 30, 40, 50]` using `np.percentile()`. Print the result.\n\n**Expected output**: `30.0`\n",
    "starter_code": "import numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\n\n# Find 50th percentile\n",
    "solution_code": "import numpy as np\n\ndata = np.array([10, 20, 30, 40, 50])\n\n# Find 50th percentile\nresult = np.percentile(data, 50)\nprint(result)",
    "expected_output": "30.0",
    "order": 14
  },
  "134": {
    "id": 134,
    "title": "Variance and Standard Deviation",
    "chapter_id": 95,
    "chapter_title": "NumPy Fundamentals",
    "content": "# üìâ Variance and Standard Deviation\n\n## Definition\n**Variance** measures how spread out data is from the mean. **Standard deviation** is the square root of variance.\n\n## Why Measure Spread?\n\nTwo datasets can have the same mean but very different spreads:\n- Dataset A: [50, 50, 50, 50, 50] ‚Üí Mean=50, Std=0 (no spread)\n- Dataset B: [0, 25, 50, 75, 100] ‚Üí Mean=50, Std=35.4 (high spread)\n\n## Real-World Analogy\n\nImagine two pizza delivery services:\n- Both average 30 minutes delivery time\n- Service A: Always exactly 30 min (low std)\n- Service B: Sometimes 10 min, sometimes 50 min (high std)\n\nWhich is more reliable?\n\n## Calculating in NumPy\n\n```python\nimport numpy as np\n\ndata = np.array([2, 4, 6, 8, 10])\n\nvariance = np.var(data)  # 8.0\nstd_dev = np.std(data)   # 2.83\n\n# Verify: std = sqrt(variance)\nprint(np.sqrt(variance))  # 2.83\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Variance** | Average of squared deviations from mean |\n| **Std Deviation** | Square root of variance (same units as data) |\n| **Spread** | How dispersed the data is |\n\n---\n\n## üéØ Your Task\n\nCalculate the standard deviation of `[20, 40, 60, 80, 100]` using `np.std()`. Print the result.\n\n**Expected output**: `28.284271247461902`\n",
    "starter_code": "import numpy as np\n\ndata = np.array([20, 40, 60, 80, 100])\n\n# Calculate standard deviation\n",
    "solution_code": "import numpy as np\n\ndata = np.array([20, 40, 60, 80, 100])\n\n# Calculate standard deviation\nresult = np.std(data)\nprint(result)",
    "expected_output": "28.284271247461902",
    "order": 15
  },
  "135": {
    "id": 135,
    "title": "Counting Missing Values",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üî¢ Counting Missing Values\n\n## Definition\nBefore cleaning, you need to know HOW MUCH data is missing. `isnull().sum()` counts missing values per column.\n\n## Why Count First?\n\n- If 1% missing ‚Üí probably safe to drop\n- If 50% missing ‚Üí dropping loses too much data, consider filling\n- If a column is 90% missing ‚Üí maybe drop the column entirely!\n\n## Real-World Analogy\n\nBefore deciding how to fix a survey:\n- Check: How many questions were skipped?\n- Few skips ‚Üí remove incomplete surveys\n- Many skips on one question ‚Üí maybe that question was confusing\n\n## Counting Techniques\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, None, 3, None],\n    'B': [None, 2, 3, 4],\n    'C': [1, 2, 3, 4]\n})\n\n# Count missing per column\nprint(df.isnull().sum())\n# A    2\n# B    1\n# C    0\n\n# Total missing in entire DataFrame\nprint(df.isnull().sum().sum())  # 3\n\n# Percentage missing per column\nprint(df.isnull().mean() * 100)\n# A    50.0\n# B    25.0\n# C     0.0\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **isnull()** | Returns True for each missing value |\n| **sum()** | Counts True values (missing count) |\n| **mean()** | Proportion of missing values |\n\n---\n\n## üéØ Your Task\n\nCount the total number of missing values in the DataFrame. Print the count.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', None, 'Carol'],\n    'age': [25, 30, None],\n    'city': [None, 'NYC', 'LA']\n})\n\n# Count total missing values\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', None, 'Carol'],\n    'age': [25, 30, None],\n    'city': [None, 'NYC', 'LA']\n})\n\n# Count total missing values\ntotal_missing = df.isnull().sum().sum()\nprint(total_missing)",
    "expected_output": "3",
    "order": 2
  },
  "136": {
    "id": 136,
    "title": "Dropping Columns with Missing Data",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üóëÔ∏è Dropping Columns with Missing Data\n\n## Definition\nSometimes it's better to drop an entire COLUMN rather than rows‚Äîespecially if a column has too many missing values.\n\n## When to Drop Columns?\n\n| Scenario | Action |\n| --- | --- |\n| Column 80%+ missing | Drop the column |\n| Column has few unique values | Consider dropping |\n| Column not needed for analysis | Drop it |\n\n## Real-World Analogy\n\nImagine a form with an optional \"Comments\" field:\n- If 90% of people skip it, that column isn't useful\n- Better to remove it than lose 90% of your data!\n\n## How to Drop Columns\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [None, None, 3],  # Mostly missing!\n    'C': [1, 2, 3]\n})\n\n# Drop columns with ANY missing values\ndf_clean = df.dropna(axis=1)\n# Only columns A and C remain\n\n# Drop columns where > 50% is missing\nthreshold = len(df) * 0.5\ndf_clean = df.dropna(axis=1, thresh=threshold)\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **axis=0** | Operate on rows (default) |\n| **axis=1** | Operate on columns |\n| **thresh** | Minimum non-NA values required |\n\n---\n\n## üéØ Your Task\n\nDrop any columns that have ANY missing values and print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', 'Cherry'],\n    'price': [1.0, None, 1.5],\n    'quantity': [10, 20, 30]\n})\n\n# Drop columns with missing values\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', 'Cherry'],\n    'price': [1.0, None, 1.5],\n    'quantity': [10, 20, 30]\n})\n\n# Drop columns with missing values\ndf_clean = df.dropna(axis=1)\nprint(df_clean)",
    "expected_output": "  product  quantity\n0   Apple        10\n1  Banana        20\n2  Cherry        30",
    "order": 3
  },
  "137": {
    "id": 137,
    "title": "Filling with Mode (Most Common)",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üî§ Filling with Mode\n\n## Definition\nFor **categorical data** (like colors, cities, categories), use the **mode** (most frequent value) to fill missing values.\n\n## Why Mode for Categories?\n\n- Mean doesn't work: What's the average of \"Red\", \"Blue\", \"Green\"?\n- Mode = most common value = reasonable guess\n\n## Real-World Analogy\n\nFilling in a missing survey answer about favorite color:\n- If 60% said \"Blue\", 30% said \"Red\", 10% said \"Green\"\n- Best guess for missing answer: \"Blue\" (the mode)\n\n## Using Mode\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'color': ['Red', 'Blue', 'Blue', None, 'Blue', None]\n})\n\n# Find the mode (most common value)\nmode_color = df['color'].mode()[0]  # 'Blue'\n\n# Fill missing with mode\ndf['color'] = df['color'].fillna(mode_color)\n# Now all None become 'Blue'\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Mode** | Most frequently occurring value |\n| **Categorical** | Data with discrete categories |\n| **mode()[0]** | First mode (in case of ties) |\n\n---\n\n## üéØ Your Task\n\nFill missing cities with the mode (most common city). Print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Carol', 'David'],\n    'city': ['NYC', 'NYC', None, 'LA']\n})\n\n# Fill missing cities with mode\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Carol', 'David'],\n    'city': ['NYC', 'NYC', None, 'LA']\n})\n\n# Fill missing cities with mode\nmode_city = df['city'].mode()[0]\ndf['city'] = df['city'].fillna(mode_city)\nprint(df)",
    "expected_output": "    name city\n0  Alice  NYC\n1    Bob  NYC\n2  Carol  NYC\n3  David   LA",
    "order": 5
  },
  "138": {
    "id": 138,
    "title": "Filling with Median",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üìä Filling with Median\n\n## Definition\nFor **numeric data with outliers**, use the **median** (middle value) instead of mean‚Äîit's more robust!\n\n## Why Median Over Mean?\n\n| Data | Mean | Median |\n| --- | --- | --- |\n| [10, 20, 30, 40, 50] | 30 | 30 |\n| [10, 20, 30, 40, 500] | 120 | 30 |\n\nThe outlier (500) skews the mean but not the median!\n\n## Real-World Analogy\n\nHouse prices in a neighborhood:\n- Most houses: $300K-$400K\n- One mansion: $10 million\n- Mean price: ~$1.2 million (misleading!)\n- Median price: ~$350K (realistic)\n\n## Using Median\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'salary': [50000, 55000, None, 60000, 500000]  # Has outlier!\n})\n\n# Compare mean vs median\nprint(df['salary'].mean())    # 166250 (skewed by outlier)\nprint(df['salary'].median())  # 57500 (robust)\n\n# Fill with median\ndf['salary'] = df['salary'].fillna(df['salary'].median())\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **Median** | Middle value when sorted |\n| **Robust** | Not affected by outliers |\n| **Outlier** | Extreme value far from others |\n\n---\n\n## üéØ Your Task\n\nFill the missing salary with the median (not mean). Print the DataFrame.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'employee': ['Alice', 'Bob', 'Carol'],\n    'salary': [50000.0, None, 70000.0]\n})\n\n# Fill with median\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'employee': ['Alice', 'Bob', 'Carol'],\n    'salary': [50000.0, None, 70000.0]\n})\n\n# Fill with median\nmedian_salary = df['salary'].median()\ndf['salary'] = df['salary'].fillna(median_salary)\nprint(df)",
    "expected_output": "  employee   salary\n0    Alice  50000.0\n1      Bob  60000.0\n2    Carol  70000.0",
    "order": 6
  },
  "139": {
    "id": 139,
    "title": "Finding Duplicate Rows",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üîç Finding Duplicate Rows\n\n## Definition\n`duplicated()` returns True for each row that is a duplicate of a previous row.\n\n## Why Find Before Removing?\n\n- Understand how many duplicates exist\n- Investigate WHY there are duplicates (data entry error? intentional?)\n- Decide which copy to keep\n\n## Real-World Analogy\n\nBefore deleting duplicate contacts:\n- First, find them: \"You have 5 duplicate contacts\"\n- Review them: Are they really duplicates or different people?\n- Then decide: Keep the most recent version\n\n## Finding Duplicates\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Alice', 'Carol'],\n    'age': [25, 30, 25, 28]\n})\n\n# Mark duplicates\nprint(df.duplicated())\n# 0    False\n# 1    False\n# 2     True  <- duplicate!\n# 3    False\n\n# View the duplicate rows\nprint(df[df.duplicated()])\n#     name  age\n# 2  Alice   25\n\n# Count duplicates\nprint(df.duplicated().sum())  # 1\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **duplicated()** | Returns True for duplicate rows |\n| **keep='first'** | First occurrence is NOT a duplicate |\n| **keep='last'** | Last occurrence is NOT a duplicate |\n\n---\n\n## üéØ Your Task\n\nCount the number of duplicate rows in the DataFrame. Print the count.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana'],\n    'price': [1.0, 0.5, 1.0, 0.75, 0.5]\n})\n\n# Count duplicates\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana'],\n    'price': [1.0, 0.5, 1.0, 0.75, 0.5]\n})\n\n# Count duplicates\ncount = df.duplicated().sum()\nprint(count)",
    "expected_output": "2",
    "order": 8
  },
  "140": {
    "id": 140,
    "title": "Duplicates in Specific Columns",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üéØ Duplicates in Specific Columns\n\n## Definition\nSometimes you want to find duplicates based on only CERTAIN columns, not all columns.\n\n## When to Use Subset?\n\n| Scenario | Approach |\n| --- | --- |\n| Same person, different info | Check 'email' only |\n| Same product, different dates | Check 'product_id' only |\n| Exact duplicate rows | Check all columns |\n\n## Real-World Analogy\n\nCustomer database with email duplicates:\n- Same email might have different addresses (they moved!)\n- We want unique CUSTOMERS (by email), not unique rows\n\n## Using subset Parameter\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'email': ['a@b.com', 'c@d.com', 'a@b.com'],\n    'name': ['Alice', 'Bob', 'Alice Smith'],  # Names differ!\n    'city': ['NYC', 'LA', 'Boston']\n})\n\n# Duplicates considering ALL columns ‚Üí None (rows differ)\nprint(df.duplicated().sum())  # 0\n\n# Duplicates considering only email\nprint(df.duplicated(subset=['email']).sum())  # 1\n\n# Keep only unique emails\ndf_unique = df.drop_duplicates(subset=['email'])\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **subset** | List of columns to check for duplicates |\n| **keep='first'** | Keep first occurrence of duplicate |\n\n---\n\n## üéØ Your Task\n\nRemove duplicates based only on the 'product' column (keep first). Print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', 'Apple'],\n    'price': [1.00, 0.50, 1.25],\n    'store': ['A', 'B', 'C']\n})\n\n# Remove duplicates by product only\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['Apple', 'Banana', 'Apple'],\n    'price': [1.00, 0.50, 1.25],\n    'store': ['A', 'B', 'C']\n})\n\n# Remove duplicates by product only\ndf_unique = df.drop_duplicates(subset=['product'])\nprint(df_unique)",
    "expected_output": "  product  price store\n0   Apple   1.00     A\n1  Banana   0.50     B",
    "order": 9
  },
  "141": {
    "id": 141,
    "title": "String Replace and Contains",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üîÑ String Replace and Contains\n\n## Definition\n`str.replace()` replaces text patterns. `str.contains()` checks if a pattern exists in strings.\n\n## Why These Methods?\n\n- Clean up inconsistent formatting (\"$100\" ‚Üí \"100\")\n- Find rows matching a pattern (\"Find all Gmail users\")\n- Standardize data (\"USA\", \"U.S.A.\", \"United States\" ‚Üí \"USA\")\n\n## Real-World Analogy\n\nFind-and-replace in a word processor:\n- Find all instances of \"colour\" and replace with \"color\"\n- Or find all lines containing \"ERROR\" in a log file\n\n## Using str.replace()\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'price': ['$10', '$25', '$50']\n})\n\n# Remove the $ symbol\ndf['price'] = df['price'].str.replace('$', '', regex=False)\n# ['10', '25', '50']\n\n# For patterns, use regex=True\ndf['text'] = df['text'].str.replace(r'\\d+', 'NUM', regex=True)\n```\n\n## Using str.contains()\n\n```python\ndf = pd.DataFrame({\n    'email': ['alice@gmail.com', 'bob@yahoo.com', 'carol@gmail.com']\n})\n\n# Find Gmail users\ngmail_mask = df['email'].str.contains('gmail')\ngmail_users = df[gmail_mask]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **str.replace()** | Replace substring with another |\n| **str.contains()** | Check if substring exists |\n| **regex** | Regular expression pattern matching |\n\n---\n\n## üéØ Your Task\n\nRemove the \"$\" from all prices and print the DataFrame.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Orange'],\n    'price': ['$1.00', '$0.50', '$0.75']\n})\n\n# Remove $ from prices\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'item': ['Apple', 'Banana', 'Orange'],\n    'price': ['$1.00', '$0.50', '$0.75']\n})\n\n# Remove $ from prices\ndf['price'] = df['price'].str.replace('$', '', regex=False)\nprint(df)",
    "expected_output": "     item price\n0   Apple  1.00\n1  Banana  0.50\n2  Orange  0.75",
    "order": 11
  },
  "142": {
    "id": 142,
    "title": "Splitting Strings",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# ‚úÇÔ∏è Splitting Strings\n\n## Definition\n`str.split()` breaks strings into parts. Combined with `expand=True`, it creates new columns.\n\n## Why Split Strings?\n\n- Names: \"John Doe\" ‚Üí \"John\" + \"Doe\"\n- Addresses: \"123 Main St, NYC\" ‚Üí Street + City\n- Dates: \"2024-01-15\" ‚Üí Year + Month + Day\n\n## Real-World Analogy\n\nImagine a phone number \"555-123-4567\":\n- Split by \"-\" to get: area code, exchange, number\n- Now you can analyze by area code!\n\n## Splitting into New Columns\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'full_name': ['Alice Smith', 'Bob Jones', 'Carol White']\n})\n\n# Split and create new columns\ndf[['first', 'last']] = df['full_name'].str.split(' ', expand=True)\n#   full_name   first   last\n# 0  Alice Smith  Alice  Smith\n# 1   Bob Jones    Bob  Jones\n# 2  Carol White  Carol  White\n```\n\n## Extracting Specific Parts\n\n```python\n# Get just the first part (index 0)\ndf['first'] = df['full_name'].str.split(' ').str[0]\n\n# Get just the last part (index -1)\ndf['last'] = df['full_name'].str.split(' ').str[-1]\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **str.split()** | Split string by delimiter |\n| **expand=True** | Return DataFrame instead of Series |\n| **.str[n]** | Get n-th element after splitting |\n\n---\n\n## üéØ Your Task\n\nSplit full names into first and last names. Print only the first names.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['John Doe', 'Jane Smith', 'Bob Wilson']\n})\n\n# Extract first names\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['John Doe', 'Jane Smith', 'Bob Wilson']\n})\n\n# Extract first names\ndf['first_name'] = df['name'].str.split(' ').str[0]\nprint(df['first_name'])",
    "expected_output": "0    John\n1    Jane\n2     Bob\nName: first_name, dtype: object",
    "order": 12
  },
  "143": {
    "id": 143,
    "title": "Converting Dates",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# üìÖ Converting Date Strings\n\n## Definition\n`pd.to_datetime()` converts strings to datetime objects, enabling date calculations.\n\n## Why Convert Dates?\n\n| As String | As Datetime |\n| --- | --- |\n| Can't calculate difference | \"2024-01-15\" - \"2024-01-01\" = 14 days |\n| Can't extract year/month | .dt.year extracts 2024 |\n| Sorting is alphabetical | Sorting is chronological |\n\n## Real-World Analogy\n\nA calendar app can't schedule events from text \"January 15\":\n- It needs to understand it's a DATE\n- Then it can calculate \"3 days until event\"\n\n## Using to_datetime()\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10']\n})\n\n# Convert to datetime\ndf['date'] = pd.to_datetime(df['date_str'])\n\n# Now you can use .dt accessor\ndf['year'] = df['date'].dt.year      # 2024, 2024, 2024\ndf['month'] = df['date'].dt.month    # 1, 2, 3\ndf['day'] = df['date'].dt.day        # 15, 20, 10\ndf['weekday'] = df['date'].dt.day_name()  # Monday, Tuesday, etc.\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **to_datetime()** | Convert strings to datetime |\n| **.dt** | Accessor for datetime properties |\n| **datetime64** | NumPy/Pandas date type |\n\n---\n\n## üéØ Your Task\n\nConvert the date strings to datetime and extract just the month. Print the months.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'date': ['2024-03-15', '2024-06-20', '2024-09-01']\n})\n\n# Convert to datetime and extract month\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'date': ['2024-03-15', '2024-06-20', '2024-09-01']\n})\n\n# Convert to datetime and extract month\ndf['date'] = pd.to_datetime(df['date'])\nprint(df['date'].dt.month)",
    "expected_output": "0    3\n1    6\n2    9\nName: date, dtype: int32",
    "order": 14
  },
  "144": {
    "id": 144,
    "title": "Handling Conversion Errors",
    "chapter_id": 96,
    "chapter_title": "Data Cleaning",
    "content": "# ‚ö†Ô∏è Handling Conversion Errors\n\n## Definition\nWhen converting types, some values might fail (e.g., \"N/A\" can't become a number). Use `errors` parameter to handle gracefully.\n\n## Why Handle Errors?\n\nReal data has messy values:\n- \"12.5\" ‚Üí fine\n- \"N/A\" ‚Üí can't convert to float!\n- \"unknown\" ‚Üí can't convert!\n\nWithout error handling, your code crashes.\n\n## Real-World Analogy\n\nA calculator that crashes on invalid input is useless:\n- Better: Show \"Error\" for that entry\n- Or: Skip that row and continue\n\n## Error Handling Options\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'value': ['10', '20', 'N/A', '40']\n})\n\n# errors='raise' (default) - crashes on bad data\n# df['value'].astype(float)  # Error!\n\n# errors='coerce' - bad values become NaN\ndf['value'] = pd.to_numeric(df['value'], errors='coerce')\n# [10.0, 20.0, NaN, 40.0]\n\n# errors='ignore' - leave bad values as-is\ndf['value'] = pd.to_numeric(df['value'], errors='ignore')\n```\n\n## Key Vocabulary\n\n| Term | Meaning |\n| --- | --- |\n| **errors='raise'** | Raise exception on bad data |\n| **errors='coerce'** | Convert bad data to NaN |\n| **errors='ignore'** | Leave bad data unchanged |\n\n---\n\n## üéØ Your Task\n\nConvert the values to numeric, using `errors='coerce'` to handle non-numeric values. Print the result.\n",
    "starter_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'quantity': ['10', '20', 'unknown', '30']\n})\n\n# Convert to numeric with error handling\n",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'quantity': ['10', '20', 'unknown', '30']\n})\n\n# Convert to numeric with error handling\ndf['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\nprint(df['quantity'])",
    "expected_output": "0    10.0\n1    20.0\n2     NaN\n3    30.0\nName: quantity, dtype: float64",
    "order": 15
  },
  "1001": {
    "id": 1001,
    "title": "What is a Database?",
    "content": "# üóÉÔ∏è What is a Database?\n\n## Definition\n\nA **database** is an organized collection of data stored electronically. Think of it as a digital filing cabinet where information is stored in a structured way so you can easily find, update, and manage it.\n\n## Why Do We Need Databases?\n\n| Without Database | With Database |\n| --- | --- |\n| Data in scattered files | Data in one organized place |\n| Hard to find information | Fast searching |\n| Duplicate data everywhere | Single source of truth |\n| Multiple people can't edit | Concurrent access |\n\n## Real-World Examples\n\n- **E-commerce**: Products, orders, customers\n- **Social Media**: Users, posts, likes, comments\n- **Banking**: Accounts, transactions, balances\n- **Healthcare**: Patients, appointments, records\n\n## Types of Databases\n\n| Type | Description | Example |\n| --- | --- | --- |\n| **Relational (SQL)** | Data in tables with relationships | PostgreSQL, MySQL |\n| **NoSQL** | Flexible structure | MongoDB, Redis |\n| **Data Warehouse** | Optimized for analytics | Snowflake, BigQuery |\n\nIn this course, we focus on **SQL databases** - the most common type for business data.\n\n---\n\n## üéØ Your Task\n\nWrite a SELECT statement that returns the text 'Hello, SQL!'.\n",
    "starter_code": "-- Your first SQL query!\n-- SELECT is used to retrieve data\n\nSELECT 'Hello, SQL!';",
    "solution_code": "SELECT 'Hello, SQL!';",
    "expected_output": "Hello, SQL!",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1002": {
    "id": 1002,
    "title": "Tables, Rows, and Columns",
    "content": "# üìä Tables, Rows, and Columns\n\n## The Building Blocks\n\nSQL databases organize data into **tables**:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ              employees                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ id   ‚îÇ name       ‚îÇ role    ‚îÇ salary    ‚îÇ  ‚Üê Column names\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1    ‚îÇ Alice      ‚îÇ Analyst ‚îÇ 75000     ‚îÇ  ‚Üê Row 1\n‚îÇ 2    ‚îÇ Bob        ‚îÇ Manager ‚îÇ 90000     ‚îÇ  ‚Üê Row 2\n‚îÇ 3    ‚îÇ Charlie    ‚îÇ Analyst ‚îÇ 72000     ‚îÇ  ‚Üê Row 3\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Key Terminology\n\n| Term | Also Called | Definition |\n| --- | --- | --- |\n| **Table** | Relation | A collection of related data |\n| **Row** | Record, Tuple | One complete entry |\n| **Column** | Field, Attribute | A specific piece of data |\n\n## Important Rules\n\n1. Each column has a **data type** (text, number, date)\n2. Each row should be **unique** (identified by a key)\n3. Column names should be **descriptive**\n4. Tables should represent **one concept** (employees, orders, products)\n\n---\n\n## üéØ Your Task\n\nSelect all columns from the employees table using SELECT *.\n",
    "starter_code": "-- The employees table has: id, name, department, salary\n-- Use SELECT * to get all columns\n\n",
    "solution_code": "SELECT * FROM employees;",
    "expected_output": "id | name    | department | salary\n1  | Alice   | Sales      | 75000\n2  | Bob     | Marketing  | 90000\n3  | Charlie | Sales      | 72000",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1003": {
    "id": 1003,
    "title": "Primary Keys",
    "content": "# üîë Primary Keys\n\n## What is a Primary Key?\n\nA **primary key** is a column (or combination of columns) that uniquely identifies each row in a table. No two rows can have the same primary key value.\n\n## Why Are Primary Keys Essential?\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ id   ‚îÇ name       ‚îÇ email       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1    ‚îÇ Alice      ‚îÇ a@mail.com  ‚îÇ  ‚Üê id=1 is unique!\n‚îÇ 2    ‚îÇ Alice      ‚îÇ b@mail.com  ‚îÇ  ‚Üê Same name, different id\n‚îÇ 3    ‚îÇ Bob        ‚îÇ c@mail.com  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  ‚Üë\n  Primary Key (PK)\n```\n\n## Primary Key Rules\n\n1. **Unique**: No duplicate values allowed\n2. **Not NULL**: Must always have a value\n3. **Immutable**: Should rarely change\n4. **Simple**: Often an auto-incrementing integer\n\n## Common Primary Key Patterns\n\n| Pattern | Example | Use Case |\n| --- | --- | --- |\n| Auto-increment | 1, 2, 3, 4... | Most common |\n| UUID | 'a1b2c3d4-...' | Distributed systems |\n| Natural key | 'USA', 'CAN' | Country codes |\n\n---\n\n## üéØ Your Task\n\nSelect the id and name columns from customers table.\n",
    "starter_code": "-- customers table has: id (primary key), name, email, city\n-- Select just id and name\n\n",
    "solution_code": "SELECT id, name FROM customers;",
    "expected_output": "id | name\n1  | Alice\n2  | Bob\n3  | Charlie",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1004": {
    "id": 1004,
    "title": "Foreign Keys & Relationships",
    "content": "# üîó Foreign Keys & Relationships\n\n## Connecting Tables Together\n\nA **foreign key** is a column that references the primary key of another table. This creates a **relationship** between tables.\n\n```\n‚îå‚îÄ customers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ orders ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ id ‚îÇ name          ‚îÇ       ‚îÇ id ‚îÇ customer_id ‚îÇ amount  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1  ‚îÇ Alice         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ 1  ‚îÇ 1           ‚îÇ 99.99   ‚îÇ\n‚îÇ 2  ‚îÇ Bob           ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ 2  ‚îÇ 1           ‚îÇ 149.99  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ 3  ‚îÇ 2           ‚îÇ 49.99   ‚îÇ\n  PK                    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                        ‚îÇ              FK\n                        ‚îî‚îÄ‚îÄ References customers.id\n```\n\n## Why Use Foreign Keys?\n\n1. **Avoid duplication**: Store customer info once, reference it many times\n2. **Data integrity**: Can't create an order for a non-existent customer\n3. **Enable JOINs**: Combine data from multiple tables\n\n## Relationship Types\n\n| Type | Example |\n| --- | --- |\n| **One-to-Many** | One customer has many orders |\n| **One-to-One** | One user has one profile |\n| **Many-to-Many** | Students enrolled in courses |\n\n---\n\n## üéØ Your Task\n\nSelect order_id and customer_id from the orders table.\n",
    "starter_code": "-- orders table has: order_id, customer_id (FK), product, amount\n-- Select order_id and customer_id\n\n",
    "solution_code": "SELECT order_id, customer_id FROM orders;",
    "expected_output": "order_id | customer_id\n1        | 101\n2        | 101\n3        | 102",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1005": {
    "id": 1005,
    "title": "SQL is Declarative",
    "content": "# üí° SQL is Declarative\n\n## Declarative vs Imperative\n\n| Imperative (How) | Declarative (What) |\n| --- | --- |\n| Step-by-step instructions | Describe desired result |\n| Python, Java, JavaScript | SQL, HTML, CSS |\n| You control the loop | Database figures it out |\n\n## Example Comparison\n\n**Imperative (Python):**\n```python\nresults = []\nfor row in table:\n    if row['salary'] > 50000:\n        results.append(row['name'])\n```\n\n**Declarative (SQL):**\n```sql\nSELECT name FROM employees WHERE salary > 50000;\n```\n\nIn SQL, you say WHAT you want, and the database engine figures out HOW to get it!\n\n## Why This Matters\n\n1. **Simpler code**: Less boilerplate\n2. **Optimized execution**: Database chooses the best approach\n3. **Portable**: Same query works across databases\n4. **Focus on logic**: Not on implementation details\n\n---\n\n## üéØ Your Task\n\nWrite a declarative SQL query to get names where salary > 60000.\n",
    "starter_code": "-- Get employee names with salary above 60000\n-- Remember: SQL is declarative - say WHAT you want!\n\n",
    "solution_code": "SELECT name FROM employees WHERE salary > 60000;",
    "expected_output": "name\nAlice\nBob",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1006": {
    "id": 1006,
    "title": "Result Sets",
    "content": "# üìã Result Sets\n\n## What is a Result Set?\n\nWhen you run a SELECT query, the database returns a **result set** - a temporary table containing your requested data.\n\n```sql\nSELECT name, salary FROM employees WHERE department = 'Sales';\n```\n\nReturns:\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ name    ‚îÇ salary ‚îÇ  ‚Üê Result Set\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Alice   ‚îÇ 75000  ‚îÇ\n‚îÇ Charlie ‚îÇ 72000  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Result Set Properties\n\n- **Temporary**: Only exists while query runs\n- **Read-only**: You can't directly edit it\n- **Ordered**: Can be sorted with ORDER BY\n- **Subset**: Can be limited with LIMIT\n\n## Empty Result Sets\n\nIf no rows match your criteria:\n```sql\nSELECT * FROM employees WHERE salary > 1000000;\n-- Returns 0 rows (an empty result set)\n```\n\nThis is NOT an error - it's a valid result meaning \"no matches found.\"\n\n---\n\n## üéØ Your Task\n\nSelect name and department from employees.\n",
    "starter_code": "-- Get name and department columns\n\n",
    "solution_code": "SELECT name, department FROM employees;",
    "expected_output": "name    | department\nAlice   | Sales\nBob     | Marketing\nCharlie | Sales",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1007": {
    "id": 1007,
    "title": "NULL Basics",
    "content": "# ‚ùì NULL Basics\n\n## What is NULL?\n\n**NULL** represents missing or unknown data. It is NOT:\n- Zero (0)\n- An empty string ('')\n- The word 'null'\n\nIt literally means \"we don't know this value.\"\n\n## Examples of NULL\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ id ‚îÇ name    ‚îÇ phone     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1  ‚îÇ Alice   ‚îÇ 555-1234  ‚îÇ\n‚îÇ 2  ‚îÇ Bob     ‚îÇ NULL      ‚îÇ  ‚Üê Bob hasn't provided a phone\n‚îÇ 3  ‚îÇ Charlie ‚îÇ 555-5678  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## NULL Behavior\n\n- `NULL = NULL` is **not TRUE** (it's NULL!)\n- Any math with NULL gives NULL: `5 + NULL = NULL`\n- Use `IS NULL` to check for NULL values\n\n## Why NULL Matters for Data Science\n\n- Missing data is common in real datasets\n- Ignoring NULL can lead to wrong calculations\n- Different aggregations handle NULL differently\n\n---\n\n## üéØ Your Task\n\nSelect all columns from products where price IS NULL.\n",
    "starter_code": "-- Find products with missing prices\n-- Use IS NULL (not = NULL)\n\n",
    "solution_code": "SELECT * FROM products WHERE price IS NULL;",
    "expected_output": "id | name       | price\n3  | Widget C   | NULL",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1008": {
    "id": 1008,
    "title": "Schema Organization",
    "content": "# üèõÔ∏è Schema Organization\n\n## What is a Schema?\n\nA **schema** is a logical container that groups related tables together. Think of it like folders organizing files.\n\n```\nDatabase: company_db\n‚îú‚îÄ‚îÄ public (schema)\n‚îÇ   ‚îú‚îÄ‚îÄ employees\n‚îÇ   ‚îî‚îÄ‚îÄ departments\n‚îú‚îÄ‚îÄ sales (schema)\n‚îÇ   ‚îú‚îÄ‚îÄ orders\n‚îÇ   ‚îî‚îÄ‚îÄ customers\n‚îî‚îÄ‚îÄ analytics (schema)\n    ‚îú‚îÄ‚îÄ daily_metrics\n    ‚îî‚îÄ‚îÄ user_events\n```\n\n## Referencing Tables with Schemas\n\n```sql\n-- Without schema (uses default)\nSELECT * FROM employees;\n\n-- With schema prefix\nSELECT * FROM public.employees;\nSELECT * FROM sales.orders;\n```\n\n## Common Schema Patterns\n\n| Schema | Purpose |\n| --- | --- |\n| **public** | Default tables |\n| **raw** | Unprocessed source data |\n| **staging** | Cleaned intermediate data |\n| **analytics** | Final reporting tables |\n\n---\n\n## üéØ Your Task\n\nSelect all from the sales.orders table using schema prefix.\n",
    "starter_code": "-- Select from sales schema, orders table\n\n",
    "solution_code": "SELECT * FROM sales.orders;",
    "expected_output": "order_id | customer_id | total\n1        | 101         | 99.99\n2        | 102         | 149.99",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1009": {
    "id": 1009,
    "title": "Query Execution Order",
    "content": "# üîÑ Query Execution Order\n\n## SQL Doesn't Run Top-to-Bottom!\n\nThe order you WRITE a query differs from how it EXECUTES:\n\n**Writing Order:**\n```sql\nSELECT name, SUM(amount)    -- 1st written\nFROM orders                  -- 2nd written\nWHERE status = 'completed'   -- 3rd written\nGROUP BY name                -- 4th written\nHAVING SUM(amount) > 100     -- 5th written\nORDER BY SUM(amount) DESC    -- 6th written\nLIMIT 10;                    -- 7th written\n```\n\n**Execution Order:**\n1. **FROM** - Which table?\n2. **WHERE** - Filter rows\n3. **GROUP BY** - Group rows\n4. **HAVING** - Filter groups\n5. **SELECT** - Choose columns\n6. **ORDER BY** - Sort results\n7. **LIMIT** - Limit rows\n\n## Why This Matters\n\n- You can't use a SELECT alias in WHERE\n- HAVING filters AFTER grouping\n- ORDER BY runs almost last\n\n---\n\n## üéØ Your Task\n\nWrite a query with FROM, WHERE, and SELECT in correct syntax order.\n",
    "starter_code": "-- Get name from employees where department = 'Sales'\n-- Remember: SELECT comes first in syntax\n\n",
    "solution_code": "SELECT name FROM employees WHERE department = 'Sales';",
    "expected_output": "name\nAlice\nCharlie",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1010": {
    "id": 1010,
    "title": "Your First SELECT",
    "content": "# üéØ Your First SELECT: Retrieve Data from Tables\n\n## The Foundation of SQL\n\nSELECT is the most fundamental SQL command. It retrieves data from tables.\n\n```sql\nSELECT column_name FROM table_name;\n```\n\n## Your First Query\n\n```sql\nSELECT name FROM employees;\n```\n\nThis retrieves all values from the `name` column.\n\n## How Data is Stored\n\nTables have:\n- **Columns**: Define what data is stored (name, age, salary)\n- **Rows**: Individual records (one employee per row)\n\n```\nemployees table:\n+----+-------+--------+\n| id | name  | salary |\n+----+-------+--------+\n| 1  | Alice | 75000  |\n| 2  | Bob   | 65000  |\n+----+-------+--------+\n```\n\n## Query Execution\n\n1. FROM - identify the table\n2. SELECT - choose columns to return\n\n## Case Insensitivity\n\n```sql\nSELECT name FROM employees;\nselect name from employees;\nSELECT Name FROM Employees;\n-- All equivalent!\n```\n\nConvention: UPPERCASE for keywords, lowercase for names.\n\n---\n\n## üéØ Your Task\n\nWrite a SELECT statement to retrieve the `name` column from the products table.",
    "starter_code": "-- Select the email column from users\n\n",
    "solution_code": "SELECT email FROM users;",
    "expected_output": "email\nalice@example.com\nbob@example.com\ncharlie@example.com",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1011": {
    "id": 1011,
    "title": "Selecting Multiple Columns",
    "content": "# üìä Selecting Multiple Columns\n\n## Beyond Single Columns\n\nSeparate column names with commas:\n\n```sql\nSELECT name, salary, department\nFROM employees;\n```\n\n## Column Order Matters\n\nThe order you list columns is the order they appear in results:\n\n```sql\nSELECT department, name FROM employees;\n-- Returns department first, then name\n\nSELECT name, department FROM employees;\n-- Returns name first, then department\n```\n\n## Example Output\n\n```sql\nSELECT name, department, salary FROM employees;\n```\n\nResult:\n```\nname    | department | salary\n--------|------------|--------\nAlice   | Engineering| 75000\nBob     | Sales      | 65000\n```\n\n## Best Practice\n\nOnly select columns you need - it's faster and cleaner:\n\n```sql\n-- Good: Specific columns\nSELECT name, email FROM customers;\n\n-- Avoid: Selecting everything when you don't need it\nSELECT * FROM customers;\n```\n\n---\n\n## üéØ Your Task\n\nSelect the name, price, and category columns from the products table.",
    "starter_code": "-- Select name, department, and salary\n\n",
    "solution_code": "SELECT name, department, salary FROM employees;",
    "expected_output": "name    | department | salary\nAlice   | Sales      | 75000\nBob     | Marketing  | 90000\nCharlie | Sales      | 72000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1012": {
    "id": 1012,
    "title": "SELECT All Columns",
    "content": "# üìã SELECT * : Get All Columns\n\n## The Star Operator\n\nUse `*` to select every column in a table:\n\n```sql\nSELECT * FROM employees;\n```\n\n## When to Use SELECT *\n\n‚úÖ **Good uses:**\n- Quick exploration of table structure\n- Debugging and testing\n- Small tables in development\n\n‚ùå **Avoid in production:**\n- Wastes bandwidth on unneeded columns\n- Breaks if table structure changes\n- Harder to maintain code\n\n## Example\n\n```sql\nSELECT * FROM products;\n```\n\nResult includes ALL columns:\n```\nid | name   | price | category | stock | created_at\n---|--------|-------|----------|-------|------------\n1  | Widget | 29.99 | Tools    | 150   | 2024-01-15\n2  | Gadget | 49.99 | Tech     | 75    | 2024-01-20\n```\n\n## Better Alternative\n\n```sql\n-- Explicit column listing\nSELECT id, name, price, category, stock, created_at \nFROM products;\n```\n\nThis is more maintainable and self-documenting.\n\n---\n\n## üéØ Your Task\n\nUse SELECT * to view all columns from the orders table.",
    "starter_code": "-- Select all columns from products\n\n",
    "solution_code": "SELECT * FROM products;",
    "expected_output": "id | name     | price | category\n1  | Widget A | 29.99 | Tools\n2  | Widget B | 49.99 | Tools\n3  | Gadget C | 99.99 | Electronics",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1013": {
    "id": 1013,
    "title": "Column Aliases with AS",
    "content": "# üè∑Ô∏è Column Aliases with AS\n\n## Renaming Columns in Output\n\nUse `AS` to give columns friendlier names in your result:\n\n```sql\nSELECT employee_name AS name FROM employees;\n```\n\n## Why Use Aliases?\n\n1. **Readability**: `first_name` ‚Üí `First Name`\n2. **Calculations**: Name computed values\n3. **Joins**: Distinguish duplicate column names\n\n## Examples\n\n```sql\n-- Simple rename\nSELECT email AS contact_email FROM users;\n\n-- With calculations\nSELECT price, price * 0.9 AS discounted_price FROM products;\n\n-- Spaces in alias (use quotes)\nSELECT name AS \"Employee Name\" FROM employees;\n```\n\n## AS is Optional\n\n```sql\n-- These are equivalent:\nSELECT name AS employee_name FROM employees;\nSELECT name employee_name FROM employees;\n```\n\nBut using `AS` is clearer!\n\n---\n\n## üéØ Your Task\n\nSelect name as 'employee_name' and salary as 'annual_salary'.\n",
    "starter_code": "-- Select with aliases\n\n",
    "solution_code": "SELECT name AS employee_name, salary AS annual_salary FROM employees;",
    "expected_output": "employee_name | annual_salary\nAlice         | 75000\nBob           | 90000\nCharlie       | 72000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1014": {
    "id": 1014,
    "title": "Table Aliases",
    "content": "# üè∑Ô∏è Table Aliases: Shorter Names\n\n## Why Aliases?\n\nLong table names make queries hard to read. Aliases create shortcuts:\n\n```sql\n-- Without alias\nSELECT employees.name, employees.department \nFROM employees;\n\n-- With alias\nSELECT e.name, e.department \nFROM employees e;\n```\n\n## AS Keyword (Optional)\n\n```sql\n-- Explicit AS\nFROM employees AS e\n\n-- Implied (same result)\nFROM employees e\n```\n\nBoth work identically!\n\n## When Aliases Are Essential\n\n### Multiple Tables\n```sql\nSELECT e.name, d.name\nFROM employees e\nJOIN departments d ON e.dept_id = d.id;\n```\n\n### Self Joins\n```sql\nSELECT a.name, b.name\nFROM employees a\nJOIN employees b ON a.manager_id = b.id;\n```\n\n## Column Aliases Too\n\n```sql\nSELECT \n    name AS employee_name,\n    salary * 12 AS annual_salary\nFROM employees;\n```\n\n---\n\n## üéØ Your Task\n\nRewrite a query using table aliases to make it more readable.",
    "starter_code": "-- Use table alias 'e' for employees\n\n",
    "solution_code": "SELECT e.name, e.department FROM employees AS e;",
    "expected_output": "name    | department\nAlice   | Sales\nBob     | Marketing\nCharlie | Sales",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1015": {
    "id": 1015,
    "title": "LIMIT Clause",
    "content": "# üî¢ LIMIT: Control Result Size\n\n## Why Limit Results?\n\n- Preview data without loading millions of rows\n- Pagination (show 10 results per page)\n- Top-N queries (top 5 customers)\n\n## Basic Syntax\n\n```sql\nSELECT * FROM products LIMIT 10;\n```\n\nReturns only the first 10 rows.\n\n## With ORDER BY\n\nLIMIT is most useful with ORDER BY:\n\n```sql\n-- Top 5 highest paid employees\nSELECT name, salary \nFROM employees \nORDER BY salary DESC \nLIMIT 5;\n```\n\n## OFFSET for Pagination\n\nSkip rows before limiting:\n\n```sql\n-- Page 1: rows 1-10\nSELECT * FROM products LIMIT 10 OFFSET 0;\n\n-- Page 2: rows 11-20\nSELECT * FROM products LIMIT 10 OFFSET 10;\n\n-- Page 3: rows 21-30\nSELECT * FROM products LIMIT 10 OFFSET 20;\n```\n\n## Database Variations\n\n| Database | Syntax |\n|----------|--------|\n| PostgreSQL, MySQL | LIMIT n |\n| SQL Server | TOP n |\n| Oracle | FETCH FIRST n ROWS |\n\n---\n\n## üéØ Your Task\n\nGet the 10 most expensive products using LIMIT and ORDER BY.",
    "starter_code": "-- Get first 3 orders\n\n",
    "solution_code": "SELECT * FROM orders LIMIT 3;",
    "expected_output": "order_id | customer_id | amount\n1        | 101         | 99.99\n2        | 101         | 149.99\n3        | 102         | 49.99",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1016": {
    "id": 1016,
    "title": "WHERE Basics",
    "content": "# üîç WHERE: Filter Your Results\n\n## The Filter Clause\n\nWHERE restricts which rows are returned:\n\n```sql\nSELECT * FROM employees\nWHERE department = 'Engineering';\n```\n\nOnly returns engineers!\n\n## Comparison Operators\n\n| Operator | Meaning |\n|----------|---------|\n| = | Equal |\n| != or <> | Not equal |\n| > | Greater than |\n| < | Less than |\n| >= | Greater or equal |\n| <= | Less or equal |\n\n## Examples\n\n```sql\n-- Exact match\nWHERE status = 'active'\n\n-- Numeric comparison\nWHERE salary > 50000\n\n-- Not equal\nWHERE department != 'Sales'\n```\n\n## Text Values Need Quotes\n\n```sql\n-- String: use quotes\nWHERE name = 'Alice'\n\n-- Number: no quotes\nWHERE age = 30\n```\n\n## Execution Order\n\n1. FROM - identify table\n2. **WHERE - filter rows**\n3. SELECT - choose columns\n\nFiltering happens before selection!\n\n---\n\n## üéØ Your Task\n\nWrite a query to find all products where price is greater than 100.",
    "starter_code": "-- Filter employees with salary > 70000\n\n",
    "solution_code": "SELECT name, salary FROM employees WHERE salary > 70000;",
    "expected_output": "name  | salary\nAlice | 75000\nBob   | 90000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1017": {
    "id": 1017,
    "title": "Comparison Operators",
    "content": "# ‚öñÔ∏è Comparison Operators\n\n## All Comparison Operators\n\n| Operator | Meaning | Example |\n| --- | --- | --- |\n| `=` | Equal | `price = 100` |\n| `<>` or `!=` | Not equal | `status <> 'done'` |\n| `<` | Less than | `age < 18` |\n| `>` | Greater than | `score > 90` |\n| `<=` | Less than or equal | `qty <= 10` |\n| `>=` | Greater than or equal | `rating >= 4` |\n\n## Multiple Conditions\n\nCombine with AND / OR:\n\n```sql\n-- Both must be true\nWHERE salary > 50000 AND department = 'Sales'\n\n-- Either can be true\nWHERE department = 'Sales' OR department = 'Marketing'\n\n-- Use parentheses for complex logic\nWHERE (age > 21 OR has_permission = true) AND status = 'active'\n```\n\n## NOT Operator\n\n```sql\nWHERE NOT status = 'cancelled'\nWHERE NOT (price > 100 AND qty < 5)\n```\n\n---\n\n## üéØ Your Task\n\nFind employees in Sales department with salary >= 75000.\n",
    "starter_code": "-- Use AND to combine two conditions\n\n",
    "solution_code": "SELECT * FROM employees WHERE department = 'Sales' AND salary >= 75000;",
    "expected_output": "id | name  | department | salary\n1  | Alice | Sales      | 75000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1018": {
    "id": 1018,
    "title": "IN Operator",
    "content": "# üìã IN Operator: Match Multiple Values\n\n## The Problem\n\nWithout IN, you need multiple ORs:\n\n```sql\nWHERE department = 'Sales' \n   OR department = 'Marketing' \n   OR department = 'Support'\n```\n\n## IN Makes It Clean\n\n```sql\nWHERE department IN ('Sales', 'Marketing', 'Support')\n```\n\nSame result, much cleaner!\n\n## How IN Works\n\nIN checks if a value matches ANY item in the list:\n\n```sql\nSELECT * FROM products\nWHERE category IN ('Electronics', 'Computers', 'Phones');\n```\n\n## NOT IN\n\nExclude multiple values:\n\n```sql\nSELECT * FROM employees\nWHERE department NOT IN ('HR', 'Finance');\n```\n\n## With Numbers\n\n```sql\nSELECT * FROM orders\nWHERE status_id IN (1, 2, 5);\n```\n\n## Performance Tip\n\nFor large lists, consider using a subquery or temporary table instead:\n\n```sql\nWHERE id IN (SELECT id FROM active_users)\n```\n\n---\n\n## üéØ Your Task\n\nFind all orders with status 'pending', 'processing', or 'shipped'.",
    "starter_code": "-- Use IN to match multiple categories\n\n",
    "solution_code": "SELECT * FROM products WHERE category IN ('Electronics', 'Tools');",
    "expected_output": "id | name     | price | category\n1  | Widget A | 29.99 | Tools\n2  | Widget B | 49.99 | Tools\n3  | Gadget C | 99.99 | Electronics",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1019": {
    "id": 1019,
    "title": "BETWEEN Operator",
    "content": "# üìè BETWEEN: Range Filtering\n\n## The Range Filter\n\nBETWEEN filters values within a range (inclusive):\n\n```sql\nWHERE price BETWEEN 10 AND 50\n-- Same as: price >= 10 AND price <= 50\n```\n\n## Key Point: Inclusive!\n\nBETWEEN includes BOTH boundary values:\n\n```sql\nWHERE age BETWEEN 18 AND 65\n-- Includes 18 AND 65\n```\n\n## With Dates\n\n```sql\nWHERE order_date BETWEEN '2024-01-01' AND '2024-12-31'\n```\n\n## NOT BETWEEN\n\nExclude a range:\n\n```sql\nWHERE temperature NOT BETWEEN 32 AND 100\n-- Either < 32 or > 100\n```\n\n## Gotcha with Timestamps\n\nBETWEEN '2024-01-01' AND '2024-01-31' might miss times on Jan 31!\n\n```sql\n-- Better for timestamps:\nWHERE date >= '2024-01-01' AND date < '2024-02-01'\n```\n\n## Common Uses\n\n- Age ranges\n- Price ranges\n- Date ranges\n- Score ranges\n\n---\n\n## üéØ Your Task\n\nFind all products with prices between 25 and 75 dollars.",
    "starter_code": "-- Find salaries in range 70000-80000\n\n",
    "solution_code": "SELECT * FROM employees WHERE salary BETWEEN 70000 AND 80000;",
    "expected_output": "id | name    | department | salary\n1  | Alice   | Sales      | 75000\n3  | Charlie | Sales      | 72000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1020": {
    "id": 1020,
    "title": "LIKE Pattern Matching",
    "content": "# üîé LIKE: Pattern Matching\n\n## Wildcards\n\n| Symbol | Meaning |\n|--------|---------|\n| % | Any characters (0 or more) |\n| _ | Exactly one character |\n\n## Examples\n\n```sql\n-- Starts with 'A'\nWHERE name LIKE 'A%'\n\n-- Ends with 'son'\nWHERE name LIKE '%son'\n\n-- Contains 'data'\nWHERE title LIKE '%data%'\n\n-- Exactly 5 characters\nWHERE code LIKE '_____'\n```\n\n## Case Sensitivity\n\nLIKE is case-sensitive in most databases!\n\n```sql\n-- Use ILIKE for case-insensitive (PostgreSQL)\nWHERE name ILIKE '%john%'\n\n-- Or use LOWER()\nWHERE LOWER(name) LIKE '%john%'\n```\n\n## NOT LIKE\n\n```sql\n-- Names NOT starting with 'A'\nWHERE name NOT LIKE 'A%'\n```\n\n## Escape Special Characters\n\n```sql\n-- Find literal %\nWHERE text LIKE '%\\%%' ESCAPE '\\\\'\n```\n\n## Performance Warning\n\nLeading wildcards are slow:\n```sql\nWHERE name LIKE '%smith'  -- Cannot use index!\nWHERE name LIKE 'smith%'  -- Can use index\n```\n\n---\n\n## üéØ Your Task\n\nFind all customers whose email ends with '@gmail.com'.",
    "starter_code": "-- Select names starting with 'A'\n\n",
    "solution_code": "SELECT * FROM employees WHERE name LIKE 'A%';",
    "expected_output": "id | name  | department | salary\n1  | Alice | Sales      | 75000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1021": {
    "id": 1021,
    "title": "ORDER BY Sorting",
    "content": "# üîÑ ORDER BY Sorting\n\n## Sorting Results\n\nORDER BY arranges rows in a specific order:\n\n```sql\nSELECT * FROM employees ORDER BY salary;\n```\n\n## ASC and DESC\n\n| Direction | Meaning | Default? |\n| --- | --- | --- |\n| `ASC` | Ascending (A‚ÜíZ, 1‚Üí9) | Yes |\n| `DESC` | Descending (Z‚ÜíA, 9‚Üí1) | No |\n\n```sql\n-- Highest salary first\nSELECT * FROM employees ORDER BY salary DESC;\n\n-- Alphabetically\nSELECT * FROM employees ORDER BY name ASC;\n```\n\n## Multiple Columns\n\n```sql\n-- Sort by department, then by salary within each dept\nSELECT * FROM employees\nORDER BY department ASC, salary DESC;\n```\n\n## NULL Sorting\n\nNULLs typically sort to the end (depends on database).\n\n```sql\n-- Control NULL position\nORDER BY column NULLS FIRST\nORDER BY column NULLS LAST\n```\n\n---\n\n## üéØ Your Task\n\nSelect all employees ordered by salary descending.\n",
    "starter_code": "-- Order by salary from highest to lowest\n\n",
    "solution_code": "SELECT * FROM employees ORDER BY salary DESC;",
    "expected_output": "id | name    | department | salary\n2  | Bob     | Marketing  | 90000\n1  | Alice   | Sales      | 75000\n3  | Charlie | Sales      | 72000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1022": {
    "id": 1022,
    "title": "Integer Types",
    "content": "# üî¢ Integer Types: Whole Numbers in SQL\n\n## Common Integer Types\n\n| Type | Range | Storage |\n|------|-------|---------|\n| SMALLINT | -32,768 to 32,767 | 2 bytes |\n| INTEGER | -2.1 billion to 2.1 billion | 4 bytes |\n| BIGINT | Huge range | 8 bytes |\n\n## Choosing the Right Type\n\n```sql\n-- User age: SMALLINT (0-120)\nage SMALLINT\n\n-- Order count: INTEGER (millions)\norder_count INTEGER\n\n-- Huge IDs: BIGINT\nglobal_id BIGINT\n```\n\n## Auto-Incrementing IDs\n\n```sql\n-- PostgreSQL\nid SERIAL PRIMARY KEY\n\n-- Creates an auto-incrementing integer\n```\n\n## Integer Arithmetic\n\n```sql\nSELECT 10 / 3;  -- Returns 3 (integer division!)\nSELECT 10.0 / 3;  -- Returns 3.333... (decimal)\n```\n\n---\n\n## üéØ Your Task\n\nCreate a table with appropriate integer types for different columns.",
    "starter_code": "-- inventory has: id (INT), product_name, quantity (INT)\n\n",
    "solution_code": "SELECT id, quantity FROM inventory WHERE quantity > 100;",
    "expected_output": "id | quantity\n1  | 150\n3  | 200",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1023": {
    "id": 1023,
    "title": "Numeric and Decimal Types",
    "content": "# üí∞ Numeric Types: Precise Decimals\n\n## When Precision Matters\n\nFor money and measurements, use NUMERIC/DECIMAL:\n\n```sql\nprice NUMERIC(10, 2)  -- Up to 99,999,999.99\n```\n\n- First number: total digits\n- Second number: decimal places\n\n## FLOAT vs NUMERIC\n\n```sql\n-- FLOAT: Approximate (faster, less accurate)\ntemperature FLOAT\n\n-- NUMERIC: Exact (slower, precise)\nprice NUMERIC(10, 2)\n```\n\n## Money Example\n\n```sql\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100),\n    price NUMERIC(10, 2),  -- Never use FLOAT for money!\n    weight FLOAT           -- OK for measurements\n);\n```\n\n## Why Not FLOAT for Money?\n\n```sql\n-- FLOAT can have rounding errors\n0.1 + 0.2 = 0.30000000000000004  -- FLOAT\n0.1 + 0.2 = 0.30                  -- NUMERIC\n```\n\n---\n\n## üéØ Your Task\n\nCreate a table for financial transactions with proper numeric types.",
    "starter_code": "-- products has: id, product_name, price (NUMERIC)\n\n",
    "solution_code": "SELECT product_name, price FROM products WHERE price > 50.00;",
    "expected_output": "product_name | price\nWidget Pro   | 99.99\nGadget Plus  | 149.50",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1024": {
    "id": 1024,
    "title": "Text Types",
    "content": "# üìù Text Types: Storing Strings\n\n## Common Text Types\n\n| Type | Description | Max Length |\n|------|-------------|------------|\n| CHAR(n) | Fixed length | n characters |\n| VARCHAR(n) | Variable length | Up to n |\n| TEXT | Unlimited | No limit |\n\n## When to Use Each\n\n```sql\n-- Fixed codes (always same length)\ncountry_code CHAR(2)  -- 'US', 'UK'\n\n-- Variable strings (email, name)\nemail VARCHAR(255)\nname VARCHAR(100)\n\n-- Long text (descriptions, articles)\ndescription TEXT\n```\n\n## CHAR vs VARCHAR\n\n```sql\nCHAR(10) with 'hello':   'hello     ' (padded)\nVARCHAR(10) with 'hello': 'hello'     (no padding)\n```\n\n## Case Sensitivity\n\nText comparisons are usually case-sensitive:\n\n```sql\nWHERE name = 'Alice'  -- Won't match 'alice'\nWHERE LOWER(name) = 'alice'  -- Matches both\n```\n\n---\n\n## üéØ Your Task\n\nDesign a users table with appropriate text types for name, email, and bio.",
    "starter_code": "-- Find Gmail users\n\n",
    "solution_code": "SELECT name, email FROM users WHERE email LIKE '%@gmail.com';",
    "expected_output": "name  | email\nAlice | alice@gmail.com\nBob   | bob@gmail.com",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1025": {
    "id": 1025,
    "title": "Date and Timestamp Types",
    "content": "# üìÖ Date/Time Types: Temporal Data\n\n## Common Types\n\n| Type | Stores | Example |\n|------|--------|---------|\n| DATE | Date only | '2024-03-15' |\n| TIME | Time only | '14:30:00' |\n| TIMESTAMP | Both | '2024-03-15 14:30:00' |\n| TIMESTAMPTZ | With timezone | '2024-03-15 14:30:00+00' |\n\n## Creating Date/Time Columns\n\n```sql\nCREATE TABLE events (\n    id SERIAL PRIMARY KEY,\n    event_date DATE,\n    start_time TIME,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n```\n\n## Current Date/Time\n\n```sql\nSELECT CURRENT_DATE;      -- 2024-03-15\nSELECT CURRENT_TIME;      -- 14:30:00\nSELECT CURRENT_TIMESTAMP; -- 2024-03-15 14:30:00\nSELECT NOW();             -- Same as CURRENT_TIMESTAMP\n```\n\n## Date Arithmetic\n\n```sql\nSELECT order_date + INTERVAL '7 days';\nSELECT NOW() - created_at;  -- Time difference\n```\n\n---\n\n## üéØ Your Task\n\nCreate an orders table with date and timestamp columns.",
    "starter_code": "-- Find orders after January 1, 2024\n\n",
    "solution_code": "SELECT order_id, order_date FROM orders WHERE order_date > '2024-01-01';",
    "expected_output": "order_id | order_date\n5        | 2024-01-15\n6        | 2024-02-01\n7        | 2024-03-10",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1026": {
    "id": 1026,
    "title": "Boolean Type",
    "content": "# ‚úÖ Boolean Type: True or False\n\n## Boolean Values\n\n```sql\nis_active BOOLEAN DEFAULT true\n```\n\nValid values:\n- TRUE, true, 't', 'yes', 'on', 1\n- FALSE, false, 'f', 'no', 'off', 0\n\n## Using Booleans\n\n```sql\n-- Insert\nINSERT INTO users (name, is_active) VALUES ('Alice', true);\n\n-- Filter\nSELECT * FROM users WHERE is_active = true;\nSELECT * FROM users WHERE is_active;  -- Same thing!\n\n-- Negate\nSELECT * FROM users WHERE NOT is_active;\n```\n\n## Common Boolean Columns\n\n```sql\nis_active BOOLEAN DEFAULT true\nis_verified BOOLEAN DEFAULT false\nis_admin BOOLEAN DEFAULT false\nhas_premium BOOLEAN DEFAULT false\n```\n\n## With Three-Valued Logic\n\nRemember: NULL is neither TRUE nor FALSE!\n\n```sql\nWHERE is_active = true   -- Only true\nWHERE is_active IS true  -- Only true\nWHERE is_active IS NOT false  -- true OR NULL\n```\n\n---\n\n## üéØ Your Task\n\nAdd boolean columns to track user status and preferences.",
    "starter_code": "-- Find active users\n\n",
    "solution_code": "SELECT name FROM users WHERE is_active;",
    "expected_output": "name\nAlice\nCharlie",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1027": {
    "id": 1027,
    "title": "Understanding NULL",
    "content": "# ‚ùì Understanding NULL: The Unknown Value\n\n## What is NULL?\n\nNULL means \"no value\" or \"unknown\". It's NOT:\n- Zero\n- Empty string\n- False\n\n## NULL Comparisons\n\nNULL cannot be compared with =:\n\n```sql\n-- WRONG: Never matches anything!\nWHERE salary = NULL\n\n-- RIGHT: Use IS NULL\nWHERE salary IS NULL\n```\n\n## NULL in Expressions\n\nAny operation with NULL returns NULL:\n\n```sql\n5 + NULL = NULL\n'hello' || NULL = NULL\nNULL = NULL  -- Returns NULL, not TRUE!\n```\n\n## NULL in Aggregates\n\n```sql\n-- values: 10, NULL, 20\nSUM() = 30   -- NULL ignored\nAVG() = 15   -- (10+20)/2, not /3\nCOUNT(col) = 2  -- NULL not counted\nCOUNT(*) = 3    -- Counts rows\n```\n\n## Checking for NULL\n\n```sql\nWHERE column IS NULL      -- Has no value\nWHERE column IS NOT NULL  -- Has a value\n```\n\n---\n\n## üéØ Your Task\n\nWrite queries to find and handle NULL values in a table.",
    "starter_code": "-- Find employees with no manager\n\n",
    "solution_code": "SELECT name FROM employees WHERE manager_id IS NULL;",
    "expected_output": "name\nSarah",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1028": {
    "id": 1028,
    "title": "IS NULL and IS NOT NULL",
    "content": "# üîç IS NULL: Finding Missing Values\n\n## The Syntax\n\n```sql\n-- Find rows WITH missing data\nSELECT * FROM customers\nWHERE phone IS NULL;\n\n-- Find rows WITHOUT missing data\nSELECT * FROM customers\nWHERE phone IS NOT NULL;\n```\n\n## Common Use Cases\n\n### Find Incomplete Records\n```sql\nSELECT * FROM users\nWHERE email IS NULL OR phone IS NULL;\n```\n\n### Count Missing Values\n```sql\nSELECT \n    COUNT(*) as total,\n    COUNT(email) as has_email,\n    COUNT(*) - COUNT(email) as missing_email\nFROM customers;\n```\n\n### Filter Valid Data\n```sql\nSELECT AVG(salary)\nFROM employees\nWHERE salary IS NOT NULL;\n```\n\n## Why Not Use = NULL?\n\n```sql\nWHERE phone = NULL   -- WRONG! Returns no rows\nWHERE phone IS NULL  -- RIGHT!\n```\n\nNULL = NULL is NULL (unknown), not TRUE!\n\n---\n\n## üéØ Your Task\n\nFind all orders that are missing a shipping address.",
    "starter_code": "-- Find products that have descriptions\n\n",
    "solution_code": "SELECT product_name FROM products WHERE description IS NOT NULL;",
    "expected_output": "product_name\nWidget A\nGadget Pro",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1029": {
    "id": 1029,
    "title": "Three-Valued Logic",
    "content": "# üîÄ Three-Valued Logic: TRUE, FALSE, NULL\n\n## SQL Has Three Truth Values\n\nUnlike regular programming:\n- TRUE\n- FALSE\n- **NULL** (unknown)\n\n## Truth Tables with NULL\n\n| A | NOT A |\n|---|-------|\n| TRUE | FALSE |\n| FALSE | TRUE |\n| NULL | NULL |\n\n| A | B | A AND B |\n|---|---|---------|\n| TRUE | NULL | NULL |\n| FALSE | NULL | FALSE |\n| NULL | NULL | NULL |\n\n| A | B | A OR B |\n|---|---|--------|\n| TRUE | NULL | TRUE |\n| FALSE | NULL | NULL |\n| NULL | NULL | NULL |\n\n## Practical Impact\n\n```sql\n-- If status is NULL, NEITHER query returns the row!\nWHERE status = 'active'      -- FALSE (not returned)\nWHERE status != 'active'     -- NULL (not returned!)\n\n-- To include NULL:\nWHERE status != 'active' OR status IS NULL\n```\n\n## COALESCE for Safety\n\n```sql\nWHERE COALESCE(status, 'unknown') != 'active'\n```\n\n---\n\n## üéØ Your Task\n\nHandle three-valued logic correctly when filtering data.",
    "starter_code": "-- Find adults or users with unknown age\n\n",
    "solution_code": "SELECT name FROM users WHERE age > 18 OR age IS NULL;",
    "expected_output": "name\nAlice\nBob\nDiana",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1030": {
    "id": 1030,
    "title": "COALESCE Function",
    "content": "# üîÑ COALESCE: Replace NULL with Default\n\n## The Problem\n\nNULL values cause display and calculation issues.\n\n## The Solution\n\nCOALESCE returns the first non-NULL value:\n\n```sql\nSELECT COALESCE(phone, 'No phone') FROM customers;\n```\n\n## Multiple Fallbacks\n\n```sql\nCOALESCE(mobile_phone, home_phone, work_phone, 'No number')\n```\n\nReturns the first non-NULL from the list!\n\n## Common Uses\n\n### Display Defaults\n```sql\nSELECT \n    name,\n    COALESCE(nickname, name) as display_name\nFROM users;\n```\n\n### Safe Calculations\n```sql\nSELECT \n    price * COALESCE(discount, 0) as discount_amount\nFROM products;\n```\n\n### Aggregation Safety\n```sql\nSELECT COALESCE(SUM(amount), 0) as total\nFROM orders\nWHERE customer_id = 999;  -- Might be no orders\n```\n\n---\n\n## üéØ Your Task\n\nUse COALESCE to display 'N/A' for missing phone numbers.",
    "starter_code": "-- Show 'N/A' for missing phone numbers\n\n",
    "solution_code": "SELECT name, COALESCE(phone, 'N/A') AS phone FROM contacts;",
    "expected_output": "name    | phone\nAlice   | 555-1234\nBob     | N/A\nCharlie | 555-5678",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1031": {
    "id": 1031,
    "title": "NULLIF Function",
    "content": "# ‚öñÔ∏è NULLIF: Convert Values to NULL\n\n## The Problem\n\nSometimes you need to TURN a value into NULL:\n- Divide by zero\n- Empty strings that should be NULL\n- Sentinel values like -1 or 'N/A'\n\n## NULLIF Syntax\n\n```sql\nNULLIF(expression1, expression2)\n-- Returns NULL if equal, otherwise returns expression1\n```\n\n## Safe Division\n\n```sql\n-- Without NULLIF: might divide by zero!\nSELECT revenue / cost\n\n-- With NULLIF: returns NULL instead of error\nSELECT revenue / NULLIF(cost, 0)\n```\n\n## Convert Empty Strings\n\n```sql\n-- Turn '' into NULL\nSELECT NULLIF(phone, '') FROM customers;\n```\n\n## Common Patterns\n\n```sql\n-- Avoid division by zero\ntotal / NULLIF(count, 0)\n\n-- Treat 0 as missing\nNULLIF(score, 0)\n\n-- Treat placeholder as NULL\nNULLIF(status, 'N/A')\n```\n\n---\n\n## üéØ Your Task\n\nUse NULLIF to safely calculate averages avoiding division by zero.",
    "starter_code": "-- Calculate average, avoid divide by zero\n\n",
    "solution_code": "SELECT total / NULLIF(count, 0) AS average FROM stats;",
    "expected_output": "average\n25.5\nNULL\n30.0",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1032": {
    "id": 1032,
    "title": "CAST and Type Conversion",
    "content": "# ÔøΩÔøΩ CAST: Convert Between Types\n\n## Why Convert Types?\n\nData often comes in the wrong format:\n- '123' string ‚Üí 123 number\n- '2024-01-15' string ‚Üí date\n\n## CAST Syntax\n\n```sql\nCAST(expression AS target_type)\n\n-- Or using ::\nexpression::target_type  -- PostgreSQL\n```\n\n## Common Conversions\n\n```sql\n-- String to Integer\nCAST('123' AS INTEGER)\n'123'::INTEGER\n\n-- String to Date\nCAST('2024-01-15' AS DATE)\n\n-- Number to String\nCAST(price AS VARCHAR)\n\n-- Date to String\nTO_CHAR(date, 'YYYY-MM-DD')\n```\n\n## In Calculations\n\n```sql\n-- Integer division gives integer\nSELECT 5 / 2;  -- Returns 2\n\n-- Cast to get decimal\nSELECT CAST(5 AS DECIMAL) / 2;  -- Returns 2.5\n```\n\n---\n\n## üéØ Your Task\n\nConvert string representations to proper dates and numbers.",
    "starter_code": "-- Convert price to integer (truncates decimals)\n\n",
    "solution_code": "SELECT product_id, CAST(price AS INTEGER) AS price_rounded FROM products;",
    "expected_output": "product_id | price_rounded\n1          | 29\n2          | 49\n3          | 99",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1033": {
    "id": 1033,
    "title": "Safe Division with NULLIF",
    "content": "# ‚ûó Safe Division: Avoid Divide by Zero\n\n## The Problem\n\nDivision by zero causes errors:\n\n```sql\nSELECT 100 / 0;  -- ERROR!\n```\n\n## NULLIF Solution\n\n```sql\nSELECT 100 / NULLIF(divisor, 0)\n-- Returns NULL instead of error\n```\n\n## Complete Pattern\n\n```sql\nSELECT \n    total_revenue,\n    total_orders,\n    CASE \n        WHEN total_orders = 0 THEN 0\n        ELSE total_revenue / total_orders\n    END as avg_order\nFROM metrics;\n\n-- Or simpler with COALESCE\nSELECT \n    COALESCE(total_revenue / NULLIF(total_orders, 0), 0) as avg_order\nFROM metrics;\n```\n\n## Real Example\n\n```sql\nSELECT \n    department,\n    budget,\n    expenses,\n    ROUND(100.0 * expenses / NULLIF(budget, 0), 2) as pct_used\nFROM department_budgets;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate percentages safely without division by zero errors.",
    "starter_code": "-- Calculate average order value safely\n\n",
    "solution_code": "SELECT category, sales / NULLIF(orders, 0) AS avg_order FROM sales;",
    "expected_output": "category  | avg_order\nBooks     | 25.50\nGames     | NULL\nToys      | 15.00",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1034": {
    "id": 1034,
    "title": "COUNT Function",
    "content": "# üî¢ COUNT: Count Rows\n\n## Basic COUNT\n\n```sql\n-- Count all rows\nSELECT COUNT(*) FROM employees;\n\n-- Count non-NULL values in column\nSELECT COUNT(email) FROM employees;\n```\n\n## COUNT(*) vs COUNT(column)\n\n```sql\n-- COUNT(*): Counts all rows\nCOUNT(*)  -- 100 rows\n\n-- COUNT(column): Only non-NULL values\nCOUNT(email)  -- 95 (5 have NULL email)\n```\n\n## With WHERE\n\n```sql\nSELECT COUNT(*) \nFROM orders \nWHERE status = 'completed';\n```\n\n## With GROUP BY\n\n```sql\nSELECT \n    department,\n    COUNT(*) as employee_count\nFROM employees\nGROUP BY department;\n```\n\n## Count Different Things\n\n```sql\nSELECT \n    COUNT(*) as total_orders,\n    COUNT(discount) as orders_with_discount,\n    COUNT(*) - COUNT(discount) as orders_without_discount\nFROM orders;\n```\n\n---\n\n## üéØ Your Task\n\nCount orders by status and find how many have discounts.",
    "starter_code": "-- Count all customers\n\n",
    "solution_code": "SELECT COUNT(*) FROM customers;",
    "expected_output": "count\n25",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1035": {
    "id": 1035,
    "title": "COUNT DISTINCT",
    "content": "# üéØ COUNT DISTINCT: Unique Values\n\n## The Problem\n\nCOUNT(*) includes duplicates. To count unique values:\n\n```sql\nSELECT COUNT(DISTINCT customer_id) FROM orders;\n```\n\n## Example\n\n```sql\n-- All orders (may have repeats)\nSELECT COUNT(customer_id) FROM orders;  -- 1000\n\n-- Unique customers\nSELECT COUNT(DISTINCT customer_id) FROM orders;  -- 350\n```\n\n## Multiple DISTINCT Counts\n\n```sql\nSELECT \n    COUNT(*) as total_orders,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    COUNT(DISTINCT product_id) as unique_products,\n    COUNT(DISTINCT date) as days_with_orders\nFROM orders;\n```\n\n## With GROUP BY\n\n```sql\nSELECT \n    category,\n    COUNT(*) as total_sales,\n    COUNT(DISTINCT customer_id) as unique_buyers\nFROM sales\nGROUP BY category;\n```\n\n---\n\n## üéØ Your Task\n\nFind the number of unique customers who made purchases each month.",
    "starter_code": "-- Count unique categories\n\n",
    "solution_code": "SELECT COUNT(DISTINCT category) FROM products;",
    "expected_output": "count\n5",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1036": {
    "id": 1036,
    "title": "SUM Function",
    "content": "# ‚ûï SUM: Add Up Values\n\n## Basic SUM\n\n```sql\nSELECT SUM(amount) FROM orders;\n```\n\n## SUM with WHERE\n\n```sql\nSELECT SUM(amount) \nFROM orders \nWHERE status = 'completed';\n```\n\n## SUM with GROUP BY\n\n```sql\nSELECT \n    category,\n    SUM(amount) as total_sales\nFROM sales\nGROUP BY category;\n```\n\n## Multiple SUMs\n\n```sql\nSELECT \n    SUM(quantity) as total_items,\n    SUM(price * quantity) as total_revenue,\n    SUM(cost * quantity) as total_cost,\n    SUM(price * quantity) - SUM(cost * quantity) as profit\nFROM order_items;\n```\n\n## SUM with NULL\n\nNULL values are ignored:\n\n```sql\n-- values: 10, NULL, 20\nSUM() = 30  -- NULL not included\n```\n\n---\n\n## üéØ Your Task\n\nCalculate total revenue by product category.",
    "starter_code": "-- Sum all order amounts\n\n",
    "solution_code": "SELECT SUM(amount) FROM orders;",
    "expected_output": "sum\n15750.00",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1037": {
    "id": 1037,
    "title": "AVG Function",
    "content": "# üìä AVG: Calculate Average\n\n## Basic AVG\n\n```sql\nSELECT AVG(salary) FROM employees;\n```\n\n## AVG with WHERE\n\n```sql\nSELECT AVG(salary) \nFROM employees \nWHERE department = 'Engineering';\n```\n\n## AVG with GROUP BY\n\n```sql\nSELECT \n    department,\n    AVG(salary) as avg_salary,\n    COUNT(*) as employee_count\nFROM employees\nGROUP BY department;\n```\n\n## Rounding Averages\n\n```sql\nSELECT ROUND(AVG(price), 2) as avg_price\nFROM products;\n```\n\n## AVG Ignores NULL\n\n```sql\n-- values: 10, NULL, 20\nAVG() = 15  -- (10+20)/2, not /3\n```\n\n## Average vs Weighted Average\n\n```sql\n-- Simple average\nAVG(rating)\n\n-- Weighted by count\nSUM(rating * review_count) / SUM(review_count)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate average order value by customer segment.",
    "starter_code": "-- Get average product price\n\n",
    "solution_code": "SELECT AVG(price) FROM products;",
    "expected_output": "avg\n45.99",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1038": {
    "id": 1038,
    "title": "MIN and MAX",
    "content": "# üìà MIN/MAX: Find Extremes\n\n## Basic Usage\n\n```sql\nSELECT \n    MIN(price) as cheapest,\n    MAX(price) as most_expensive\nFROM products;\n```\n\n## With Dates\n\n```sql\nSELECT \n    MIN(order_date) as first_order,\n    MAX(order_date) as last_order\nFROM orders;\n```\n\n## With GROUP BY\n\n```sql\nSELECT \n    category,\n    MIN(price) as min_price,\n    MAX(price) as max_price,\n    MAX(price) - MIN(price) as price_range\nFROM products\nGROUP BY category;\n```\n\n## Finding the Record\n\nMIN/MAX give you the value, not the row:\n\n```sql\n-- Get the actual most expensive product\nSELECT * FROM products\nWHERE price = (SELECT MAX(price) FROM products);\n\n-- Or use ORDER BY + LIMIT\nSELECT * FROM products\nORDER BY price DESC\nLIMIT 1;\n```\n\n---\n\n## üéØ Your Task\n\nFind the earliest and latest order dates for each customer.",
    "starter_code": "-- Get salary range\n\n",
    "solution_code": "SELECT MIN(salary), MAX(salary) FROM employees;",
    "expected_output": "min   | max\n45000 | 150000",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1039": {
    "id": 1039,
    "title": "GROUP BY Basics",
    "content": "# üìä GROUP BY: Aggregate by Category\n\n## The Problem\n\nYou want results \"per category\":\n- Total sales per region\n- Average salary per department\n- Count of orders per customer\n\n## GROUP BY Syntax\n\n```sql\nSELECT category, SUM(price)\nFROM products\nGROUP BY category;\n```\n\n## The Rule\n\nEvery non-aggregate column in SELECT must be in GROUP BY:\n\n```sql\n-- WRONG\nSELECT category, name, SUM(price)\nFROM products\nGROUP BY category;  -- 'name' missing!\n\n-- RIGHT\nSELECT category, SUM(price)\nFROM products\nGROUP BY category;\n```\n\n## Multiple Groups\n\n```sql\nSELECT region, category, SUM(sales)\nFROM sales\nGROUP BY region, category;\n```\n\n## Execution Order\n\n1. FROM\n2. WHERE (filter rows)\n3. **GROUP BY** (aggregate)\n4. HAVING (filter groups)\n5. SELECT\n6. ORDER BY\n\n---\n\n## üéØ Your Task\n\nGroup orders by month and calculate total revenue per month.",
    "starter_code": "-- Group orders by status\n\n",
    "solution_code": "SELECT status, COUNT(*) FROM orders GROUP BY status;",
    "expected_output": "status    | count\ncompleted | 45\npending   | 12\ncancelled | 3",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1040": {
    "id": 1040,
    "title": "GROUP BY Multiple Columns",
    "content": "# üìä Multi-Column GROUP BY\n\n## Grouping by Multiple Dimensions\n\n```sql\nSELECT \n    region,\n    product_category,\n    SUM(sales) as total_sales\nFROM sales\nGROUP BY region, product_category;\n```\n\nCreates one row for each unique (region, category) combination.\n\n## Example Output\n\n```\nregion | product_category | total_sales\n-------|------------------|------------\nNorth  | Electronics      | 45000\nNorth  | Clothing         | 32000\nSouth  | Electronics      | 38000\nSouth  | Clothing         | 41000\n```\n\n## With Time Periods\n\n```sql\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    customer_segment,\n    COUNT(*) as orders,\n    SUM(amount) as revenue\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date), customer_segment\nORDER BY month, customer_segment;\n```\n\n## Column Order Matters for Readability\n\n```sql\nGROUP BY year, quarter, month  -- Hierarchy: broad ‚Üí specific\n```\n\n---\n\n## üéØ Your Task\n\nGroup sales by year and region, calculating count and sum.",
    "starter_code": "-- Sales by category and year\n\n",
    "solution_code": "SELECT category, year, SUM(sales) FROM products GROUP BY category, year;",
    "expected_output": "category | year | sum\nBooks    | 2023 | 5000\nBooks    | 2024 | 7500\nGames    | 2023 | 3200",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1041": {
    "id": 1041,
    "title": "HAVING Clause",
    "content": "# ÔøΩÔøΩ HAVING: Filter Aggregated Results\n\n## WHERE vs HAVING\n\n| Clause | Filters | When |\n|--------|---------|------|\n| WHERE | Individual rows | Before GROUP BY |\n| HAVING | Groups/aggregates | After GROUP BY |\n\n## Basic HAVING\n\n```sql\nSELECT customer_id, COUNT(*) as orders\nFROM orders\nGROUP BY customer_id\nHAVING COUNT(*) >= 5;  -- Only customers with 5+ orders\n```\n\n## HAVING with WHERE\n\n```sql\nSELECT customer_id, SUM(amount) as total\nFROM orders\nWHERE status = 'completed'  -- Row filter\nGROUP BY customer_id\nHAVING SUM(amount) > 1000;  -- Group filter\n```\n\n## Common Patterns\n\n```sql\n-- High-volume customers\nHAVING COUNT(*) > 10\n\n-- High-value customers\nHAVING SUM(amount) > 1000\n\n-- Above-average groups\nHAVING AVG(salary) > 75000\n```\n\n## Can't Use Aliases in HAVING\n\n```sql\n-- WRONG\nSELECT customer_id, COUNT(*) as order_count\nHAVING order_count >= 5\n\n-- RIGHT\nHAVING COUNT(*) >= 5\n```\n\n---\n\n## üéØ Your Task\n\nFind departments where average salary exceeds $70,000.",
    "starter_code": "-- Categories with count > 5\n\n",
    "solution_code": "SELECT category, COUNT(*) FROM products GROUP BY category HAVING COUNT(*) > 5;",
    "expected_output": "category    | count\nElectronics | 12\nClothing    | 8",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1042": {
    "id": 1042,
    "title": "WHERE vs HAVING",
    "content": "# ‚öñÔ∏è WHERE vs HAVING\n\n## Key Difference\n\n| Clause | Filters | When |\n| --- | --- | --- |\n| WHERE | Individual rows | Before grouping |\n| HAVING | Groups | After grouping |\n\n## Example\n\n```sql\n-- Find departments where active employees average > $60k\nSELECT department, AVG(salary)\nFROM employees\nWHERE is_active = true     -- Filter rows first\nGROUP BY department\nHAVING AVG(salary) > 60000; -- Then filter groups\n```\n\n## Performance Tip\n\nFilter with WHERE whenever possible - it reduces data before grouping:\n\n```sql\n-- ‚úÖ More efficient: filter early\nWHERE status = 'active'\nGROUP BY category\n\n-- ‚ùå Less efficient: group all, filter late\nGROUP BY category\nHAVING status = 'active'  -- This doesn't even work!\n```\n\n---\n\n## üéØ Your Task\n\nFind customers with total orders > 1000, only counting completed orders.\n",
    "starter_code": "-- Filter completed orders first, then group\n\n",
    "solution_code": "SELECT customer_id, SUM(amount) FROM orders WHERE status = 'completed' GROUP BY customer_id HAVING SUM(amount) > 1000;",
    "expected_output": "customer_id | sum\n101         | 2500\n103         | 1200",
    "chapter_id": 203,
    "chapter_title": "Aggregations & Grouping"
  },
  "1043": {
    "id": 1043,
    "title": "Why We Need Joins",
    "content": "# üîó Why We Need Joins: Relational Data\n\n## The Problem with One Table\n\nImagine storing everything in one spreadsheet:\n\n| OrderID | Customer | Address | Product | Price |\n|---------|----------|---------|---------|-------|\n| 1       | Alice    | 123 Main| Widget  | 10.00 |\n| 2       | Alice    | 123 Main| Gadget  | 20.00 |\n\n**Issues:**\n1. **Redundancy**: Alice's address is stored twice.\n2. **Inconsistency**: What if we update one row but not the other?\n3. **Space**: Wastes storage.\n\n## The Solution: Normalized Tables\n\nSplit data into logical tables:\n\n**Customers Table:**\n| ID | Name  | Address |\n|----|-------|---------|\n| 1  | Alice | 123 Main|\n\n**Orders Table:**\n| ID | CustomerID | Product |\n|----|------------|---------|\n| 1  | 1          | Widget  |\n| 2  | 1          | Gadget  |\n\n## Joining Them Back Together\n\nTo get the full picture, we **JOIN** them:\n\n```sql\nSELECT c.Name, o.Product\nFROM Customers c\nJOIN Orders o ON c.ID = o.CustomerID;\n```\n\n## Benefits\n- **Efficiency**: Update address in one place.\n- **Integrity**: Database enforces relationships.\n- **Flexibility**: Query data in many ways.\n\n---\n\n## üéØ Your Task\n\nWrite a query that effectively joins two tables to reconstruct the data.",
    "starter_code": "-- Join orders with customers\n\n",
    "solution_code": "SELECT orders.order_id, customers.name FROM orders JOIN customers ON orders.customer_id = customers.id;",
    "expected_output": "order_id | name\n1        | Alice\n2        | Alice\n3        | Bob",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1044": {
    "id": 1044,
    "title": "INNER JOIN Basics",
    "content": "# ü§ù INNER JOIN: Finding Matches\n\n## The Most Common Join\n\nINNER JOIN returns rows ONLY when there is a match in BOTH tables.\n\n## Syntax\n\n```sql\nSELECT columns\nFROM TableA\nINNER JOIN TableB\nON TableA.key = TableB.key;\n```\n\n## Visualizing it\n\n( A ( MATCH ) B )\n\nOnly the overlapping part (intersection) is returned.\n\n## Example\n\n**Students:**\n| ID | Name |\n|----|------|\n| 1  | Ali  |\n| 2  | Bob  |\n| 3  | Cat  |\n\n**Enrolled:**\n| StudentID | Course |\n|-----------|--------|\n| 1         | Math   |\n| 2         | Art    |\n\n```sql\nSELECT s.Name, e.Course\nFROM Students s\nINNER JOIN Enrolled e ON s.ID = e.StudentID;\n```\n\n**Result:**\n- Ali (Math)\n- Bob (Art)\n- Cat is EXCLUDED (no enrollment)\n\n## Ambiguous Columns\n\nIf both tables have a `name` column, you MUST specify the table:\n\n```sql\n-- Error: Ambiguous column name\nSELECT name FROM Students JOIN Enrolled ...\n\n-- Correct\nSELECT Students.name FROM ...\n-- Better (with alias)\nSELECT s.name FROM Students s JOIN ...\n```\n\n---\n\n## üéØ Your Task\n\nPerform an INNER JOIN to find employees and their departments.",
    "starter_code": "-- Inner join products with categories\n\n",
    "solution_code": "SELECT p.product_name, c.category_name FROM products p INNER JOIN categories c ON p.category_id = c.id;",
    "expected_output": "product_name | category_name\nWidget A     | Electronics\nWidget B     | Electronics\nGadget C     | Tools",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1045": {
    "id": 1045,
    "title": "INNER JOIN Multiple Tables",
    "content": "# üîó INNER JOIN Multiple Tables\n\n## Chaining Joins\n\nJoin more than two tables by chaining:\n\n```sql\nSELECT o.id, c.name, p.product_name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN products p ON o.product_id = p.id;\n```\n\n## Execution Order\n\nJoins execute left to right:\n1. orders JOIN customers ‚Üí creates temporary result\n2. temporary result JOIN products ‚Üí final result\n\n## Practical Example\n\n```sql\n-- Order details with customer and product info\nSELECT \n  o.order_date,\n  c.name AS customer,\n  p.name AS product,\n  oi.quantity,\n  oi.price\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN order_items oi ON o.id = oi.order_id\nJOIN products p ON oi.product_id = p.id;\n```\n\n---\n\n## üéØ Your Task\n\nJoin orders, customers, and products to get order_id, customer name, and product name.\n",
    "starter_code": "-- Three-table join\n\n",
    "solution_code": "SELECT o.order_id, c.name, p.product_name FROM orders o JOIN customers c ON o.customer_id = c.id JOIN products p ON o.product_id = p.id;",
    "expected_output": "order_id | name  | product_name\n1        | Alice | Widget A\n2        | Bob   | Gadget B",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1046": {
    "id": 1046,
    "title": "LEFT JOIN Basics",
    "content": "# ‚¨ÖÔ∏è LEFT JOIN Basics\n\n## Keep All Left Rows\n\nLEFT JOIN returns all rows from the left table, even if no match:\n\n```sql\nSELECT c.name, o.id AS order_id\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id;\n```\n\n## How LEFT JOIN Works\n\n```\ncustomers:   orders:          Result:\nid|name      id|cust_id       name   |order_id\n1 |Alice     1 | 1       ‚Üí    Alice  | 1\n2 |Bob       2 | 1            Alice  | 2\n3 |Charlie                    Bob    | NULL\n                              Charlie| NULL\n```\n\nBob and Charlie have no orders, but they're still included with NULL.\n\n## Use Cases\n\n- Find customers who haven't ordered\n- Include all products, even unsold ones\n- Show all employees, even without assignments\n\n---\n\n## üéØ Your Task\n\nLeft join customers with orders to show all customers (even those without orders).\n",
    "starter_code": "-- Show all customers with their orders (or NULL)\n\n",
    "solution_code": "SELECT c.name, o.order_id FROM customers c LEFT JOIN orders o ON c.id = o.customer_id;",
    "expected_output": "name    | order_id\nAlice   | 1\nAlice   | 2\nBob     | 3\nCharlie | NULL",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1047": {
    "id": 1047,
    "title": "LEFT JOIN for Missing Data",
    "content": "# ‚¨ÖÔ∏è LEFT JOIN: Keep All Left Rows\n\n## The \"Optional\" Match\n\nLEFT JOIN returns ALL rows from the left table, and matching rows from the right. If no match, the right side is NULL.\n\n## Syntax\n\n```sql\nSELECT columns\nFROM LeftTable\nLEFT JOIN RightTable\nON LeftTable.key = RightTable.key;\n```\n\n## Visualizing it\n\n( A ( MATCH ) B )\n^ Includes all of A\n\n## Example\n\n**Students:**\n| ID | Name |\n|----|------|\n| 1  | Ali  |\n| 2  | Bob  |\n| 3  | Cat  |\n\n**Enrolled:**\n| StudentID | Course |\n|-----------|--------|\n| 1         | Math   |\n\n```sql\nSELECT s.Name, e.Course\nFROM Students s\nLEFT JOIN Enrolled e ON s.ID = e.StudentID;\n```\n\n**Result:**\n| Name | Course |\n|------|--------|\n| Ali  | Math   |\n| Bob  | NULL   |\n| Cat  | NULL   |\n\n## Finding Non-Matches\n\nThis is the best way to find items WITHOUT a match:\n\n```sql\n-- Find students taking NO courses\nSELECT s.Name\nFROM Students s\nLEFT JOIN Enrolled e ON s.ID = e.StudentID\nWHERE e.Course IS NULL;\n```\n\n---\n\n## üéØ Your Task\n\nUse a LEFT JOIN to list all customers, including those who haven't placed an order.",
    "starter_code": "-- Find customers without any orders\n\n",
    "solution_code": "SELECT c.name FROM customers c LEFT JOIN orders o ON c.id = o.customer_id WHERE o.order_id IS NULL;",
    "expected_output": "name\nCharlie\nDiana",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1048": {
    "id": 1048,
    "title": "RIGHT JOIN",
    "content": "# ‚û°Ô∏è RIGHT JOIN: Keep All Right Rows\n\n## The Mirror of LEFT JOIN\n\nRIGHT JOIN returns ALL rows from the right table, and matching rows from the left.\n\n## Syntax\n\n```sql\nSELECT columns\nFROM TableA\nRIGHT JOIN TableB\nON TableA.key = TableB.key;\n```\n\n## Why is it rare?\n\nRIGHT JOIN is logically equivalent to a LEFT JOIN with tables swapped.\n\n```sql\n-- These are usually the same result structure:\nSELECT * FROM A RIGHT JOIN B ON A.id = B.id;\nSELECT * FROM B LEFT JOIN A ON A.id = B.id;\n```\n\nMost developers prefer to read \"Top to Bottom\" or \"Left to Right\", so they stick to LEFT JOIN and just swap table order.\n\n## When to use?\n\nSometimes useful in complex multi-joins where you want to keep the data flow clear without reordering everything.\n\n---\n\n## üéØ Your Task\n\nRewrite a RIGHT JOIN query using LEFT JOIN to achieve the same result.",
    "starter_code": "-- Right join employees with departments\n\n",
    "solution_code": "SELECT e.name, d.department_name FROM employees e RIGHT JOIN departments d ON e.department_id = d.id;",
    "expected_output": "name  | department_name\nAlice | Sales\nBob   | Sales\nNULL  | Research",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1049": {
    "id": 1049,
    "title": "FULL OUTER JOIN",
    "content": "# ‚ÜîÔ∏è FULL OUTER JOIN: Everything from Everywhere\n\n## The \"Everything\" Join\n\nFULL JOIN returns ALL rows from BOTH tables.\n- Matches are linked.\n- Non-matches from Left are preserved (Right is NULL).\n- Non-matches from Right are preserved (Left is NULL).\n\n## Syntax\n\n```sql\nSELECT columns\nFROM TableA\nFULL OUTER JOIN TableB\nON TableA.key = TableB.key;\n```\n\n## Use Cases\n\n1. **Comparing Lists**: Compare Month 1 customers vs Month 2 customers.\n2. **Data Reconciliation**: Find records present in System A but not B, and vice versa.\n\n## Example\n\n**A:** {1, 2}\n**B:** {2, 3}\n\n**A FULL JOIN B:**\n1 - NULL (Only in A)\n2 - 2    (In Both)\nNULL - 3 (Only in B)\n\n---\n\n## üéØ Your Task\n\nUse FULL OUTER JOIN to find discrepancies between two inventory lists.",
    "starter_code": "-- Full outer join\n\n",
    "solution_code": "SELECT c.name, o.order_id FROM customers c FULL OUTER JOIN orders o ON c.id = o.customer_id;",
    "expected_output": "name    | order_id\nAlice   | 1\nBob     | 2\nNULL    | 3\nCharlie | NULL",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1050": {
    "id": 1050,
    "title": "CROSS JOIN",
    "content": "# ‚úñÔ∏è CROSS JOIN\n\n## Cartesian Product\n\nCROSS JOIN produces every combination of rows:\n\n```sql\nSELECT * FROM colors CROSS JOIN sizes;\n```\n\n## Result\n\n```\ncolors:    sizes:         Result:\ncolor      size           color|size\nRed        S         ‚Üí    Red  | S\nBlue       M              Red  | M\n           L              Blue | S\n                          Blue | M\n                          ... (6 rows total)\n```\n\n## Use Cases\n\n```sql\n-- Generate all date/product combinations for reports\nSELECT d.date, p.product_id\nFROM dates d\nCROSS JOIN products p;\n\n-- Create a multiplication table\nSELECT a.num, b.num, a.num * b.num AS product\nFROM numbers a CROSS JOIN numbers b;\n```\n\n## ‚ö†Ô∏è Warning: Row Explosion!\n\n100 rows √ó 100 rows = 10,000 result rows!\n\n---\n\n## üéØ Your Task\n\nCross join colors and sizes tables to get all combinations.\n",
    "starter_code": "-- All color/size combinations\n\n",
    "solution_code": "SELECT colors.color, sizes.size FROM colors CROSS JOIN sizes;",
    "expected_output": "color | size\nRed   | S\nRed   | M\nBlue  | S\nBlue  | M",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1051": {
    "id": 1051,
    "title": "Join Cardinality",
    "content": "# üî¢ Join Cardinality: One-to-One, One-to-Many\n\n## Understanding Rows Returned\n\nJoins don't just \"look up\" values‚Äîthey can multiply rows!\n\n## 1:1 (One-to-One)\nOne row in A matches exactly one in B.\n- **Result Rows**: Same as A (if INNER JOIN).\n\n## 1:N (One-to-Many)\nOne row in A matches multiple rows in B.\n- **Result Rows**: A rows are duplicated for each match in B.\n- **Example**: Customer (1) JOIN Orders (5) -> Returns 5 rows.\n\n## N:N (Many-to-Many)\nMultiple rows in A match multiple rows in B.\n- **Result Rows**: Cartesian product of the matches.\n- **Example**: Students (3) JOIN Classes (3) via Enrollment.\n\n## The Explosion Risk\nIf you join two tables with duplicates on the join key (Many-to-Many accidentally), you get a massive result set!\n\n`100 rows` JOIN `100 rows` could become `10,000 rows`!\n\n---\n\n## üéØ Your Task\n\nPredict the number of rows returned by joining a Customers table with an Orders table.",
    "starter_code": "-- Customers with multiple orders\n\n",
    "solution_code": "SELECT customer_id, COUNT(*) as order_count FROM orders GROUP BY customer_id HAVING COUNT(*) > 2;",
    "expected_output": "customer_id | order_count\n101         | 5\n103         | 3",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1052": {
    "id": 1052,
    "title": "Duplicate Rows in Joins",
    "content": "# üëØ Duplicate Rows in Joins\n\n## The Causes\n\nDuplicates often appear unexpectedly after a join.\n\n**Scenario:**\nTable A:\nID: 1\n\nTable B:\nID: 1, Value: X\nID: 1, Value: Y\n\n**Query:**\n```sql\nSELECT A.ID \nFROM A \nJOIN B ON A.ID = B.ID;\n```\n\n**Result:**\n1\n1\n\nThe ID '1' appears twice because it matched two rows in B!\n\n## Fixing Duplicates\n\n1. **DISTINCT**: Force uniqueness (can be slow).\n   ```sql\n   SELECT DISTINCT A.ID ...\n   ```\n\n2. **Aggregation**: Collapse the many side.\n   ```sql\n   SELECT A.ID, COUNT(B.Value)\n   FROM A JOIN B ...\n   GROUP BY A.ID\n   ```\n\n3. **Check Keys**: Ensure you are joining on a unique key if you expect 1:1.\n\n---\n\n## üéØ Your Task\n\nIdentify and fix a query that is returning duplicate customer records.",
    "starter_code": "-- Count orders per customer using join\n\n",
    "solution_code": "SELECT c.name, COUNT(DISTINCT o.order_id) as orders FROM customers c JOIN orders o ON c.id = o.customer_id GROUP BY c.name;",
    "expected_output": "name  | orders\nAlice | 3\nBob   | 2",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1053": {
    "id": 1053,
    "title": "Self Joins",
    "content": "# ü™û Self Joins: Joining a Table to Itself\n\n## Why Join to Self?\n\nHierarchical data is the #1 reason.\n\n**Employees Table:**\n| ID | Name | ManagerID |\n|----|------|-----------|\n| 1  | Boss | NULL      |\n| 2  | Ali  | 1         |\n| 3  | Bob  | 1         |\n\nTo get \"Employee Name\" and \"Manager Name\", we treat the table as two different lists: one for employees, one for managers.\n\n## Syntax\n\nYou MUST use aliases!\n\n```sql\nSELECT \n    e.Name as Employee_Name,\n    m.Name as Manager_Name\nFROM Employees e\nLEFT JOIN Employees m\nON e.ManagerID = m.ID;\n```\n\n## Other Uses\n\n- Finding pairs: \"Find students with the same grade\".\n- Sequential events: \"Find events that happened right after another event for the same user\".\n\n---\n\n## üéØ Your Task\n\nWrite a self-join to list employees alongside their manager's name.",
    "starter_code": "-- Employee and their manager\n\n",
    "solution_code": "SELECT e.name AS employee, m.name AS manager FROM employees e LEFT JOIN employees m ON e.manager_id = m.id;",
    "expected_output": "employee | manager\nAlice    | NULL\nBob      | Alice\nCharlie  | Alice",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1054": {
    "id": 1054,
    "title": "Join Best Practices",
    "content": "# ‚úÖ Join Best Practices\n\n## 1. Use Explicit JOIN Syntax\n\n```sql\n-- ‚úÖ Explicit (clear and modern)\nSELECT * FROM orders o\nJOIN customers c ON o.customer_id = c.id;\n\n-- ‚ùå Implicit (old style, harder to read)\nSELECT * FROM orders o, customers c\nWHERE o.customer_id = c.id;\n```\n\n## 2. Always Qualify Column Names\n\n```sql\n-- ‚úÖ Clear which table each column comes from\nSELECT o.id, c.name, o.amount\n\n-- ‚ùå Ambiguous when both tables have 'id'\nSELECT id, name, amount\n```\n\n## 3. Use Consistent Alias Conventions\n\n```sql\nFROM customers c      -- First letter\nJOIN orders o\nJOIN order_items oi   -- Two letters for longer names\n```\n\n## 4. Filter Early\n\n```sql\n-- ‚úÖ Filter before join (faster)\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.status = 'completed'\n```\n\n---\n\n## üéØ Your Task\n\nWrite a clean join query for orders and customers using proper aliases and explicit syntax.\n",
    "starter_code": "-- Clean, readable join\n\n",
    "solution_code": "SELECT o.order_id, o.amount, c.name AS customer_name FROM orders o INNER JOIN customers c ON o.customer_id = c.id WHERE o.status = 'completed';",
    "expected_output": "order_id | amount | customer_name\n1        | 99.99  | Alice\n3        | 149.99 | Bob",
    "chapter_id": 204,
    "chapter_title": "Joins Like a Pro"
  },
  "1055": {
    "id": 1055,
    "title": "üèõÔ∏è Query Architect Challenge",
    "content": "# üèõÔ∏è Challenge: Query Architect\n\n## The Scenario\n\nYou are the Lead Data Architect for a library system. You have:\n\n- **Books** (Current Inventory)\n- **Rentals** (Active and Past Rentals)\n- **Members** (Library Card Holders)\n\n## The Requirement\n\nManagement wants a comprehensive report showing:\n1. All Members (even those who never rented).\n2. Their total number of active rentals.\n3. Their favorite Genre (based on past rentals).\n\n## Constraints\n\n- Use LEFT JOINs to ensure no members are dropped.\n- You'll need Aggregation for the total rentals.\n- You might need a Mode (most frequent) calculation or a subquery for the favorite genre.\n\n## Go!\n\nConstruct the query piece by piece. Start with Members.\n\n---\n\n## üéØ Your Task\n\nBuild the ultimate library report query using multiple joins and aggregation.",
    "starter_code": "-- The Query Architect Challenge\n-- Show customer name, order count, total spent, and average\n-- Only customers with total > 200\n-- Sort by total descending\n\n",
    "solution_code": "SELECT \n  c.name,\n  COUNT(o.order_id) AS order_count,\n  SUM(o.amount) AS total_spent,\n  ROUND(AVG(o.amount), 2) AS avg_order\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nGROUP BY c.name\nHAVING SUM(o.amount) > 200\nORDER BY total_spent DESC;",
    "expected_output": "name  | order_count | total_spent | avg_order\nBob   | 5           | 750.00      | 150.00\nAlice | 3           | 450.00      | 150.00",
    "chapter_id": 250,
    "chapter_title": "Query Architect Boss"
  },
  "1056": {
    "id": 1056,
    "title": "Subquery in WHERE",
    "content": "# üîç Subquery in WHERE: Filtering by a List\n\n## Concept\n\nUse the result of one query to filter another.\n\n## Example: \"Who bought 'Widget'?\"\n\nInstead of two queries:\n1. Get ID of 'Widget' (say, 99).\n2. Get orders with ProductID 99.\n\nDo it in one:\n\n```sql\nSELECT * FROM Orders\nWHERE ProductID = (\n    SELECT ID FROM Products WHERE Name = 'Widget'\n);\n```\n\n## Scalar Subquery\nThe inner query returns ONE value.\n\n## List Subquery (IN)\nThe inner query returns a LIST of values.\n\n```sql\nSELECT * FROM Orders\nWHERE ProductID IN (\n    SELECT ID FROM Products WHERE Category = 'Electronics'\n);\n```\n\n---\n\n## üéØ Your Task\n\nFind customers who have placed an order in the last 30 days using a subquery.",
    "starter_code": "-- Products above average price\n\n",
    "solution_code": "SELECT * FROM products WHERE price > (SELECT AVG(price) FROM products);",
    "expected_output": "id | name     | price\n2  | Widget B | 99.99\n4  | Premium  | 149.99",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1057": {
    "id": 1057,
    "title": "IN with Subquery",
    "content": "# üìã IN with Subquery\n\n## Dynamic Lists\n\nHardcoding lists is brittle:\n`WHERE ID IN (1, 2, 5)`\n\nWhat if the list changes? Use a subquery!\n\n```sql\nWHERE UserID IN (\n    SELECT ID FROM Users WHERE Status = 'Banned'\n)\n```\n\n## How it works\n\n1. Database runs the inner query first (conceptually).\n2. Generates the list of Banned IDs.\n3. Filters the outer query against that list.\n\n## Caution: NULLs\n\nIf the subquery returns ANY `NULL` values, `NOT IN` will fail to match anything.\n**Always filter out NULLs in subqueries intended for IN/NOT IN.**\n\n```sql\nWHERE ID NOT IN (\n    SELECT ManagerID FROM Employees WHERE ManagerID IS NOT NULL\n)\n```\n\n---\n\n## üéØ Your Task\n\nFind all products that have NEVER been ordered.",
    "starter_code": "-- Customers with orders\n\n",
    "solution_code": "SELECT * FROM customers WHERE id IN (SELECT customer_id FROM orders);",
    "expected_output": "id  | name  | email\n101 | Alice | a@mail.com\n102 | Bob   | b@mail.com",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1058": {
    "id": 1058,
    "title": "EXISTS Operator",
    "content": "# ‚àÉ EXHIBITS: Checking for Existence\n\n## What is EXISTS?\n\nIt returns TRUE if the subquery returns **at least one row**.\n\n```sql\nSELECT Name\nFROM Customers c\nWHERE EXISTS (\n    SELECT 1 FROM Orders o\n    WHERE o.CustomerID = c.ID\n);\n```\n\n## Why not just use IN?\n\n- **Performance**: EXISTS stops searching as soon as it finds ONE match. IN might scan the whole list.\n- **Semantics**: EXISTS is often cleaner for \"Correlated Subqueries\" (where inner refers to outer).\n\n## \"SELECT 1\"\n\nBy convention, we `SELECT 1` in the subquery because the actual data returned doesn't matter, only *that* rows exist.\n\n---\n\n## üéØ Your Task\n\nUse EXISTS to find suppliers that supply at least one active product.",
    "starter_code": "-- Departments with employees\n\n",
    "solution_code": "SELECT * FROM departments d WHERE EXISTS (SELECT 1 FROM employees e WHERE e.department_id = d.id);",
    "expected_output": "id | name\n1  | Sales\n2  | Engineering",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1059": {
    "id": 1059,
    "title": "NOT EXISTS",
    "content": "# üö´ NOT EXISTS: finding the \"Antimatter\"\n\n## The \"Everything Else\" Filter\n\nFinding things that DO NOT exist is a classic SQL problem.\n\n- Customers with NO orders.\n- Products with NO sales.\n\n## The Pattern\n\n```sql\nSELECT Name\nFROM Customers c\nWHERE NOT EXISTS (\n    SELECT 1 FROM Orders o\n    WHERE o.CustomerID = c.ID\n);\n```\n\n## Versus \"LEFT JOIN ... IS NULL\"\n\nBoth do the same work!\n1. **NOT EXISTS**: Often easier to read.\n2. **LEFT JOIN / IS NULL**: Sometimes faster depending on the optimizer.\n\n```sql\nSELECT c.Name\nFROM Customers c\nLEFT JOIN Orders o ON c.ID = o.CustomerID\nWHERE o.ID IS NULL;\n```\n\n---\n\n## üéØ Your Task\n\nFind employees who have no assigned manager using NOT EXISTS.",
    "starter_code": "-- Products never ordered\n\n",
    "solution_code": "SELECT * FROM products p WHERE NOT EXISTS (SELECT 1 FROM order_items oi WHERE oi.product_id = p.id);",
    "expected_output": "id | name     | price\n5  | Widget E | 29.99",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1060": {
    "id": 1060,
    "title": "Scalar Subqueries",
    "content": "# üî¢ Scalar Subqueries\n\n## Returning a Single Value\n\nA **scalar subquery** returns exactly one value (one row, one column):\n\n```sql\nSELECT \n  name,\n  salary,\n  salary - (SELECT AVG(salary) FROM employees) AS diff_from_avg\nFROM employees;\n```\n\n## Where to Use Scalar Subqueries\n\n```sql\n-- In SELECT\nSELECT name, (SELECT MAX(salary) FROM employees) AS max_salary\n\n-- In WHERE\nWHERE price = (SELECT MAX(price) FROM products)\n\n-- In calculations\nSELECT salary / (SELECT AVG(salary) FROM employees) AS salary_ratio\n```\n\n## ‚ö†Ô∏è Error If Not Scalar\n\nIf subquery returns multiple rows, you get an error!\n\n```sql\n-- ERROR: subquery returned more than one row\nWHERE id = (SELECT id FROM users WHERE active = true)\n```\n\n---\n\n## üéØ Your Task\n\nShow each product with its price and the max price across all products.\n",
    "starter_code": "-- Product with max price comparison\n\n",
    "solution_code": "SELECT name, price, (SELECT MAX(price) FROM products) AS max_price FROM products;",
    "expected_output": "name     | price | max_price\nWidget A | 29.99 | 149.99\nWidget B | 99.99 | 149.99\nPremium  | 149.99| 149.99",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1061": {
    "id": 1061,
    "title": "Correlated Subqueries",
    "content": "# üîó Correlated Subqueries: Loop-de-Loop\n\n## The \"For Loop\" of SQL\n\nA standard subquery runs ONCE.\nA **Correlated Subquery** runs ONCE PER ROW of the outer query.\n\n## Syntax\n\nNotice the inner query uses `e.DepartmentID` from the outer query!\n\n```sql\n-- Find employees who earn more than their department average\nSELECT Name, Salary, DepartmentID\nFROM Employees e\nWHERE Salary > (\n    SELECT AVG(Salary)\n    FROM Employees sub\n    WHERE sub.DepartmentID = e.DepartmentID\n);\n```\n\n## How it executes\n\n1. Pick first employee (Alice, Dept 1, $80k).\n2. Run inner: Calc avg for Dept 1 ($60k).\n3. Check: 80 > 60? Yes. Keep Alice.\n4. Pick next employee...\n\n## Performance Warning\n\nSince it runs per row, it can be slow on large tables.\n\n---\n\n## üéØ Your Task\n\nWrite a correlated subquery to find orders that are larger than the customer's average order.",
    "starter_code": "-- Above department average\n\n",
    "solution_code": "SELECT * FROM employees e WHERE salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id);",
    "expected_output": "id | name  | department_id | salary\n1  | Alice | 1             | 90000\n4  | Diana | 2             | 85000",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1062": {
    "id": 1062,
    "title": "UNION Operator",
    "content": "# ‚à™ UNION: Combining Sets\n\n## Vertical Combination\n\nJoins combine tables **horizontally** (adding columns).\nUnions combine tables **vertically** (adding rows).\n\n## The UNION Operator\n\nCombines result sets and **removes duplicates**.\n\n```sql\nSELECT City FROM Customers\nUNION\nSELECT City FROM Suppliers;\n```\n\n**Result:** A unique list of all cities where we have either a customer or supplier.\n\n## Requirements\n\n1. Same number of columns.\n2. Compatible data types in each column.\n3. Column names in results come from the First query.\n\n---\n\n## üéØ Your Task\n\nCreate a unified list of all contacts (Customers and Employees) using UNION.",
    "starter_code": "-- All names from both tables\n\n",
    "solution_code": "SELECT name FROM customers UNION SELECT name FROM suppliers;",
    "expected_output": "name\nAlice\nBob\nAcme Corp\nGlobal Inc",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1063": {
    "id": 1063,
    "title": "UNION ALL",
    "content": "# ‚à™+ UNION ALL: Combining Sets (Keep Duplicates)\n\n## The Fast Union\n\n`UNION ALL` is like UNION, but it **keeps duplicates**.\n\n```sql\nSELECT City FROM Customers\nUNION ALL\nSELECT City FROM Suppliers;\n```\n\n## Why use UNION ALL?\n\n1. **Performance**: Database doesn't have to sort and remove duplicates. It's much faster.\n2. **Logic**: Sometimes you want to know if a value appeared twice (e.g., counting total transaction volume from two tables).\n\n## Example\n\n**A:** {1, 2}\n**B:** {2, 3}\n\n**UNION:** {1, 2, 3}\n**UNION ALL:** {1, 2, 2, 3}\n\n---\n\n## üéØ Your Task\n\nCombine two lists of numbers using UNION ALL to preserve counts.",
    "starter_code": "-- All orders from both years\n\n",
    "solution_code": "SELECT * FROM orders WHERE EXTRACT(YEAR FROM order_date) = 2023 UNION ALL SELECT * FROM orders WHERE EXTRACT(YEAR FROM order_date) = 2024;",
    "expected_output": "order_id | amount | order_date\n1        | 100.00 | 2023-03-15\n2        | 150.00 | 2023-06-20\n3        | 200.00 | 2024-01-10",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1064": {
    "id": 1064,
    "title": "INTERSECT and EXCEPT",
    "content": "# ‚à© INTERSECT and - EXCEPT\n\n## Set Theory in SQL\n\n- **UNION**: A OR B (All items)\n- **INTERSECT**: A AND B (Overlapping items)\n- **EXCEPT** (or MINUS): A NOT B (Items in A but not B)\n\n## INTERSECT\n\n\"Find rows that are in BOTH lists.\"\n\n```sql\n-- Customers who are ALSO Employees\nSELECT Name FROM Customers\nINTERSECT\nSELECT Name FROM Employees;\n```\n\n## EXCEPT\n\n\"Find rows in the first list but NOT the second.\"\n\n```sql\n-- Products ordered in 2023 but NOT 2024\nSELECT ProductID FROM Orders2023\nEXCEPT\nSELECT ProductID FROM Orders2024;\n```\n\n*Note: MySQL does not natively support INTERSECT/EXCEPT (use Joins/Subqueries), but PostgreSQL and SQL Server do.*\n\n---\n\n## üéØ Your Task\n\nUse EXCEPT to find books that are missing from the inventory audit.",
    "starter_code": "-- Customers who ordered both years\n\n",
    "solution_code": "SELECT customer_id FROM orders_2023 INTERSECT SELECT customer_id FROM orders_2024;",
    "expected_output": "customer_id\n101\n103",
    "chapter_id": 205,
    "chapter_title": "Subqueries & Set Operations"
  },
  "1065": {
    "id": 1065,
    "title": "Introduction to CTEs",
    "content": "# üèóÔ∏è Intro to CTEs: Common Table Expressions\n\n## Making SQL Readable\n\nSubqueries are powerful but messy. They force you to read \"inside out\".\n\n**Subquery Style (Messy):**\n```sql\nSELECT * FROM (\n    SELECT * FROM (\n        SELECT ...\n    )\n)\n```\n\n**CTE Style (Readable):**\n```sql\nWITH Step1 AS (\n    SELECT ...\n),\nStep2 AS (\n    SELECT ... FROM Step1\n)\nSELECT * FROM Step2;\n```\n\nA CTE is like a named temporary table that exists just for one query.\n\n---\n\n## üéØ Your Task\n\nConvert a simple subquery into a CTE.",
    "starter_code": "-- CTE for 2024 orders\n\n",
    "solution_code": "WITH recent_orders AS (\n  SELECT * FROM orders WHERE order_date >= '2024-01-01'\n)\nSELECT * FROM recent_orders;",
    "expected_output": "order_id | customer_id | amount | order_date\n5        | 101         | 150.00 | 2024-01-15\n6        | 102         | 200.00 | 2024-02-20",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1066": {
    "id": 1066,
    "title": "Basic CTE Syntax",
    "content": "# üìù Basic CTE Syntax\n\n## The WITH Clause\n\nDefine CTEs at the top, THEN write your main SELECT.\n\n```sql\nWITH HighValueOrders AS (\n    SELECT *\n    FROM Orders\n    WHERE Total > 500\n)\nSELECT CustomerID, COUNT(*)\nFROM HighValueOrders\nGROUP BY CustomerID;\n```\n\n## Naming\n\n- Give meaningful names like `MonthlySales`, `ActiveUsers`, `FlaggedTransactions`.\n- The name is valid only for the query immediately following.\n\n## Scope\n\nA CTE dies immediately after the main query runs. You can't reference it in a later query.\n\n---\n\n## üéØ Your Task\n\nDefine a CTE named `RecentLogins` and select from it.",
    "starter_code": "-- CTE for active users\n\n",
    "solution_code": "WITH active_users AS (\n  SELECT * FROM users WHERE is_active = true\n)\nSELECT name, email FROM active_users;",
    "expected_output": "name  | email\nAlice | alice@mail.com\nBob   | bob@mail.com",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1067": {
    "id": 1067,
    "title": "CTEs vs Subqueries",
    "content": "# ‚öñÔ∏è CTEs vs Subqueries\n\n## When to use what?\n\n**Use CTEs when:**\n1. You need to reuse the same result set multiple times in the main query.\n2. The logic is complex and needs to be broken into steps for readability.\n3. You need recursion (Advanced!).\n\n**Use Subqueries when:**\n1. It's a simple one-off filter (e.g., `WHERE ID IN (SELECT ...)`).\n2. You are doing a quick check `EXISTS (...)`.\n\n## Readability Wins\n\nCode is read more often than it is written. CTEs read top-to-bottom, like a story.\n\n---\n\n## üéØ Your Task\n\nRefactor a complex subquery into a clean CTE.",
    "starter_code": "-- Convert subquery to CTE\n\n",
    "solution_code": "WITH expensive AS (\n  SELECT * FROM products WHERE price > 50\n)\nSELECT * FROM expensive;",
    "expected_output": "id | name     | price\n2  | Widget B | 99.99\n3  | Premium  | 149.99",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1068": {
    "id": 1068,
    "title": "Multiple CTEs",
    "content": "# ‚õìÔ∏è Multiple CTEs\n\n## Defining a Sequence\n\nYou can define multiple CTEs in one `WITH` clause, separated by commas.\n\n```sql\nWITH \n    Sales AS (\n        SELECT * FROM Orders\n    ),\n    Returns AS (\n        SELECT * FROM Returns\n    ),\n    NetSales AS (\n        SELECT s.Total - r.Amount\n        FROM Sales s\n        JOIN Returns r ON s.ID = r.OrderID\n    )\nSELECT * FROM NetSales;\n```\n\nNotice: Only **one** `WITH` keyword at the start!\n\n---\n\n## üéØ Your Task\n\nCreate three CTEs: `Revenue`, `Costs`, and `Profits`, then query the final one.",
    "starter_code": "-- Two CTEs\n\n",
    "solution_code": "WITH \n  sales_dept AS (\n    SELECT * FROM employees WHERE department = 'Sales'\n  ),\n  high_salary AS (\n    SELECT * FROM sales_dept WHERE salary > 60000\n  )\nSELECT name, salary FROM high_salary;",
    "expected_output": "name  | salary\nAlice | 75000\nBob   | 80000",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1069": {
    "id": 1069,
    "title": "Chained CTEs",
    "content": "# üîó Chained CTEs\n\n## Building Blocks\n\nA CTE can reference previous CTEs defined in the same block.\n\n```sql\nWITH \n    RawData AS (\n        SELECT * FROM Events\n    ),\n    CleanedData AS (\n        SELECT UserID, UPPER(Event) as Event\n        FROM RawData   -- < Reference strict predecessor!\n        WHERE Event IS NOT NULL\n    ),\n    Summary AS (\n        SELECT Event, COUNT(*)\n        FROM CleanedData\n        GROUP BY Event\n    )\nSELECT * FROM Summary;\n```\n\nThis creates a \"Pipeline\" of data transformation.\n\n---\n\n## üéØ Your Task\n\nBuild a data pipeline using 3 chained CTEs.",
    "starter_code": "-- Chain CTEs\n\n",
    "solution_code": "WITH \n  big_orders AS (\n    SELECT * FROM orders WHERE amount > 100\n  ),\n  customer_totals AS (\n    SELECT customer_id, SUM(amount) AS total FROM big_orders GROUP BY customer_id\n  )\nSELECT * FROM customer_totals;",
    "expected_output": "customer_id | total\n101         | 500.00\n102         | 350.00",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1070": {
    "id": 1070,
    "title": "CTEs for Readability",
    "content": "# üìñ CTEs for Readability\n\n## Before: Hard to Read\n\n```sql\nSELECT c.name, ot.total\nFROM customers c\nJOIN (\n  SELECT customer_id, SUM(amount) AS total\n  FROM (\n    SELECT * FROM orders WHERE status = 'completed'\n  ) completed_orders\n  GROUP BY customer_id\n) ot ON c.id = ot.customer_id\nWHERE ot.total > 1000;\n```\n\n## After: Clear and Readable\n\n```sql\nWITH \n  completed_orders AS (\n    SELECT * FROM orders WHERE status = 'completed'\n  ),\n  order_totals AS (\n    SELECT customer_id, SUM(amount) AS total\n    FROM completed_orders\n    GROUP BY customer_id\n  ),\n  high_value_customers AS (\n    SELECT * FROM order_totals WHERE total > 1000\n  )\nSELECT c.name, h.total\nFROM customers c\nJOIN high_value_customers h ON c.id = h.customer_id;\n```\n\n---\n\n## üéØ Your Task\n\nRefactor into CTEs: filter active products, then get category counts.\n",
    "starter_code": "-- Make readable with CTEs\n\n",
    "solution_code": "WITH \n  active_products AS (\n    SELECT * FROM products WHERE is_active = true\n  ),\n  category_counts AS (\n    SELECT category, COUNT(*) AS count FROM active_products GROUP BY category\n  )\nSELECT * FROM category_counts ORDER BY count DESC;",
    "expected_output": "category    | count\nElectronics | 15\nClothing    | 10\nBooks       | 5",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1071": {
    "id": 1071,
    "title": "Reusing CTEs",
    "content": "# ‚ôªÔ∏è Reusing CTEs\n\n## Don't Repeat Yourself (DRY)\n\nThe killer feature of CTEs: Define once, use many times.\n\n**Scenario**: Compare \"Active Users\" this month vs \"Active Users\" last month.\n\n```sql\nWITH ActiveUsers AS (\n    SELECT UserID, Month\n    FROM Logins\n    GROUP BY UserID, Month\n)\nSELECT *\nFROM ActiveUsers ThisMonth\nJOIN ActiveUsers LastMonth \n  ON ThisMonth.UserID = LastMonth.UserID;\n```\n\nIf you used subqueries, you'd paste the code block twice!\n\n---\n\n## üéØ Your Task\n\nUse a single CTE twice in a Self-Join query.",
    "starter_code": "-- Compare to average using CTE\n\n",
    "solution_code": "WITH stats AS (\n  SELECT AVG(salary) AS avg_salary FROM employees\n)\nSELECT e.name, e.salary, s.avg_salary, e.salary - s.avg_salary AS diff\nFROM employees e, stats s;",
    "expected_output": "name    | salary | avg_salary | diff\nAlice   | 75000  | 70000      | 5000\nBob     | 65000  | 70000      | -5000",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1072": {
    "id": 1072,
    "title": "Recursive CTEs Intro",
    "content": "# üåÄ Recursive CTEs: The Loop\n\n## What is Recursion?\n\nA Recursive CTE calls itself. It's used for **Hierarchies** (Org Charts) or **Graphs** (Flights).\n\n## Structure\n\n1. **Anchor Member**: The baseline result (e.g., the CEO).\n2. **UNION ALL**\n3. **Recursive Member**: Queries the CTE itself to find the next level.\n\n## Example: Counting to 5\n\n```sql\nWITH RECURSIVE Counter AS (\n    -- Anchor\n    SELECT 1 as n\n    UNION ALL\n    -- Recursive Step\n    SELECT n + 1 FROM Counter WHERE n < 5\n)\nSELECT * FROM Counter;\n```\n\n**Result**: 1, 2, 3, 4, 5\n\n---\n\n## üéØ Your Task\n\nWrite a simple recursive CTE to generate valid dates for the current month.",
    "starter_code": "-- Simple CTE as recursive intro\n\n",
    "solution_code": "WITH emp_list AS (\n  SELECT id, name, manager_id FROM employees\n)\nSELECT * FROM emp_list;",
    "expected_output": "id | name    | manager_id\n1  | Sarah   | NULL\n2  | Alice   | 1\n3  | Bob     | 1",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1073": {
    "id": 1073,
    "title": "CTE Best Practices",
    "content": "# üåü CTE Best Practices\n\n## Tips for Success\n\n1. **Naming**: Use descriptive names (`MonthlyRevenue`, not `CTE1`).\n2. **Limit Columns**: Only select columns you need in the CTE. Carrying blobs is expensive.\n3. **Start Simple**: Build one CTE, test it with `SELECT * FROM CTE`. Then add the next.\n4. **Don't Overdo It**: If you have 20 CTEs, maybe you need a temporary table or an ETL process.\n\n## Debugging\n\nSince you can't see \"inside\" a CTE during the final query, debug by replacing the final `SELECT` with `SELECT * FROM ProblematicCTE`.\n\n---\n\n## üéØ Your Task\n\nOptimize an existing query by moving repeatedly used logic into a CTE.",
    "starter_code": "-- Clear, well-named CTEs\n\n",
    "solution_code": "WITH \n  significant_orders AS (\n    SELECT * FROM orders WHERE amount > 50\n  ),\n  customer_revenue AS (\n    SELECT customer_id, SUM(amount) AS total\n    FROM significant_orders\n    GROUP BY customer_id\n  )\nSELECT * FROM customer_revenue ORDER BY total DESC;",
    "expected_output": "customer_id | total\n101         | 450.00\n102         | 300.00",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1074": {
    "id": 1074,
    "title": "What are Window Functions?",
    "content": "# ü™ü What are Window Functions?\n\n## Aggregate Without Grouping\n\nWindow functions let you calculate across related rows WITHOUT collapsing them:\n\n```sql\nSELECT \n  name,\n  department,\n  salary,\n  AVG(salary) OVER (PARTITION BY department) AS dept_avg\nFROM employees;\n```\n\n## Output\n\n```\nname    | department | salary | dept_avg\nAlice   | Sales      | 60000  | 65000\nBob     | Sales      | 70000  | 65000\nCharlie | Eng        | 80000  | 85000\nDiana   | Eng        | 90000  | 85000\n```\n\nNotice: All rows are kept, but each shows its department's average!\n\n## Window vs Regular Aggregates\n\n| Regular Aggregate | Window Function |\n| --- | --- |\n| Collapses rows | Keeps all rows |\n| Uses GROUP BY | Uses OVER |\n| One row per group | Original row count |\n\n---\n\n## üéØ Your Task\n\nShow each employee with the overall average salary using window function.\n",
    "starter_code": "-- Overall average alongside each row\n\n",
    "solution_code": "SELECT name, salary, AVG(salary) OVER () AS overall_avg FROM employees;",
    "expected_output": "name    | salary | overall_avg\nAlice   | 60000  | 75000\nBob     | 70000  | 75000\nCharlie | 80000  | 75000",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1075": {
    "id": 1075,
    "title": "OVER Clause Basics",
    "content": "# ü™ü The OVER Clause: Opening the Window\n\n## Aggregates vs Windows\n\n- **GROUP BY** collapses rows (100 rows -> 1 row).\n- **Window Functions** keep all rows but add context (100 rows -> 100 rows + new column).\n\n## Syntax\n\n```sql\nSELECT \n    Name, \n    Salary, \n    AVG(Salary) OVER () as GlobalAvg\nFROM Employees;\n```\n\nHere, `OVER ()` means \"Apply this aggregate to the ENTIRE result set\".\nEvery row will see the same `GlobalAvg`.\n\nThis is powerful! You can compare an individual to the group in one line:\n`Salary - AVG(Salary) OVER () as DiffFromAvg`\n\n---\n\n## üéØ Your Task\n\nCalculate the total company salary and show it alongside every employee's individual salary.",
    "starter_code": "-- Total using window function\n\n",
    "solution_code": "SELECT order_id, amount, SUM(amount) OVER () AS total FROM orders;",
    "expected_output": "order_id | amount | total\n1        | 100    | 450\n2        | 150    | 450\n3        | 200    | 450",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1076": {
    "id": 1076,
    "title": "PARTITION BY",
    "content": "# üç∞ PARTITION BY: Slicing the Window\n\n## Context Aware Windows\n\n`OVER ()` sees the whole table.\n`OVER (PARTITION BY Department)` splits the table into groups.\n\nThe function restarts calculation for each group!\n\n```sql\nSELECT \n    Name, \n    Department, \n    Salary, \n    AVG(Salary) OVER (PARTITION BY Department) as DeptAvg\nFROM Employees;\n```\n\n**Result:**\n- Alice (Eng) sees Engineering Avg.\n- Bob (Sales) sees Sales Avg.\n\nThis is like a localized GROUP BY that doesn't hide rows.\n\n---\n\n## üéØ Your Task\n\nDetermine the maximum score in each game category and display it next to each player's score.",
    "starter_code": "-- Customer totals with PARTITION BY\n\n",
    "solution_code": "SELECT order_id, customer_id, amount, SUM(amount) OVER (PARTITION BY customer_id) AS customer_total FROM orders;",
    "expected_output": "order_id | customer_id | amount | customer_total\n1        | 101         | 100    | 250\n2        | 101         | 150    | 250\n3        | 102         | 200    | 200",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1077": {
    "id": 1077,
    "title": "ORDER BY in Windows",
    "content": "# üì∂ ORDER BY in Windows: Cumulative logic\n\n## Giving Direction\n\nSome functions need order (like Rank or Running Total).\n\n```sql\nRANK() OVER (ORDER BY Score DESC)\n```\n\nThis ranks players from High score to Low.\n\n## Window vs Main Query Order\n\n- `OVER (ORDER BY ...)` controls how the function calculates.\n- The `ORDER BY` at the very end of the query controls how rows are displayed.\n\nThey are independent!\n\n---\n\n## üéØ Your Task\n\nAssign a row number to each order based on the date it was placed.",
    "starter_code": "-- Running total by date\n\n",
    "solution_code": "SELECT order_date, amount, SUM(amount) OVER (ORDER BY order_date) AS running_total FROM orders;",
    "expected_output": "order_date | amount | running_total\n2024-01-01 | 100    | 100\n2024-01-02 | 150    | 250\n2024-01-03 | 200    | 450",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1078": {
    "id": 1078,
    "title": "ROW_NUMBER",
    "content": "# 1Ô∏è‚É£ ROW_NUMBER: Unique ID per Row\n\n## What is it?\n\nIt assigns a sequential integer `1, 2, 3...` to rows within a window.\n\n## Usage\n\n```sql\nROW_NUMBER() OVER (ORDER BY Date)\n```\n\n## Deduplication (Killer Feature)\n\nIf you `PARTITION BY` user and `ORDER BY` date, `ROW_NUMBER = 1` identifies the **first** action per user.\n\n```sql\nSELECT * FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY UserID ORDER BY Date) as rn\n    FROM Logins\n) sub\nWHERE rn = 1;\n```\nThis finds the first login for every user!\n\n---\n\n## üéØ Your Task\n\nFind the most recent order for every customer using ROW_NUMBER.",
    "starter_code": "-- Row numbers by salary\n\n",
    "solution_code": "SELECT name, salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num FROM employees;",
    "expected_output": "name  | salary | row_num\nBob   | 90000  | 1\nAlice | 75000  | 2\nTom   | 60000  | 3",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1079": {
    "id": 1079,
    "title": "RANK and DENSE_RANK",
    "content": "# üèÖ RANK vs DENSE_RANK\n\n## Handling Ties\n\nWhat happens when two people tie for 1st place?\n\n1. **ROW_NUMBER**: Ties broken arbitrarily (1, 2).\n2. **RANK**: Ties share rank, next number skipped (1, 1, 3). \"Olympic Style\".\n3. **DENSE_RANK**: Ties share rank, next number used (1, 1, 2).\n\n## Example\n\nScores: 100, 100, 90\n\n| Function | Values | Notes |\n|----------|--------|-------|\n| ROW_NUMBER | 1, 2, 3 | No ties allowed |\n| RANK | 1, 1, 3 | Gap after ties |\n| DENSE_RANK | 1, 1, 2 | No gaps |\n\n---\n\n## üéØ Your Task\n\nUse DENSE_RANK to assign leaderboard positions allowing for ties.",
    "starter_code": "-- Dense rank by price\n\n",
    "solution_code": "SELECT name, price, DENSE_RANK() OVER (ORDER BY price DESC) AS price_rank FROM products;",
    "expected_output": "name     | price  | price_rank\nPremium  | 149.99 | 1\nWidget B | 99.99  | 2\nWidget A | 99.99  | 2\nBasic    | 29.99  | 3",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1080": {
    "id": 1080,
    "title": "Running Totals with SUM",
    "content": "# üìà Running Totals: Cumulative Sum\n\n## The \"ORDER BY\" Effect in Aggregates\n\nWhen you add `ORDER BY` to `SUM()`, it becomes a cumulative sum!\n\n```sql\nSELECT \n    Date, \n    DailyRevenue,\n    SUM(DailyRevenue) OVER (ORDER BY Date) as RunningTotal\nFROM Sales;\n```\n\n**Result:**\nDay 1: 100 -> Total 100\nDay 2: 50 -> Total 150\nDay 3: 20 -> Total 170\n\n## Why?\n\nThe default window frame is \"Start of partition up to current row\". So it adds up everything before it.\n\n---\n\n## üéØ Your Task\n\nCalculate the cumulative number of users signed up over time.",
    "starter_code": "-- Cumulative sales\n\n",
    "solution_code": "SELECT order_date, amount, SUM(amount) OVER (ORDER BY order_date) AS cumulative_amount FROM orders;",
    "expected_output": "order_date | amount | cumulative_amount\n2024-01-01 | 100.00 | 100.00\n2024-01-02 | 150.00 | 250.00\n2024-01-03 | 200.00 | 450.00",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1081": {
    "id": 1081,
    "title": "Moving Averages",
    "content": "# üåä Moving Averages: Smooth the Noise\n\n## Defined Frame\n\nTo do a \"7-Day Moving Average\", we need to explicitly set the window frame.\n\n```sql\nAVG(Sales) OVER (\n    ORDER BY Date\n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n)\n```\n\n- **6 PRECEDING**: Look back 6 rows.\n- **CURRENT ROW**: Include today.\n- Total: 7 days.\n\n## Visualizing\n\nIt's a sliding window that moves down one row at a time, recalculating the average.\n\n---\n\n## üéØ Your Task\n\nCalculate a 3-day moving average of stock prices.",
    "starter_code": "-- 3-day moving average\n\n",
    "solution_code": "SELECT date, revenue, AVG(revenue) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_avg FROM daily_sales;",
    "expected_output": "date       | revenue | moving_avg\n2024-01-01 | 1000    | 1000.00\n2024-01-02 | 1500    | 1250.00\n2024-01-03 | 800     | 1100.00",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1082": {
    "id": 1082,
    "title": "LAG Function",
    "content": "# üîô LAG: Looking Backward\n\n## Access Previous Rows\n\n`LAG(Column, n)` lets you peek at the row `n` steps before the current one.\n\n## Use Case: Growth Calculation\n\nCompare Today vs Yesterday.\n\n```sql\nSELECT \n    Date,\n    Revenue,\n    LAG(Revenue) OVER (ORDER BY Date) as YesterdayRev\nFROM Sales;\n```\n\nNow you can do: `(Revenue - YesterdayRev) / YesterdayRev` to get % growth!\n\n## Default Values\n\n`LAG(Col, 1, 0)` -> If no previous row exists (first row), return 0.\n\n---\n\n## üéØ Your Task\n\nCalculate the difference in sales between the current day and the previous day.",
    "starter_code": "-- Previous order amount\n\n",
    "solution_code": "SELECT order_id, amount, LAG(amount) OVER (ORDER BY order_id) AS prev_amount FROM orders;",
    "expected_output": "order_id | amount | prev_amount\n1        | 100    | NULL\n2        | 150    | 100\n3        | 200    | 150",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1083": {
    "id": 1083,
    "title": "LEAD Function",
    "content": "# üîú LEAD: Looking Forward\n\n## Access Future Rows\n\n`LEAD(Column, n)` lets you peek at the row `n` steps AHEAD.\n\n## Use Case: Duration Calculation\n\nIf you have a `StartTime` log, how long did the session last?\nDuration = `NextStartTime - CurrentStartTime`.\n\n```sql\nSELECT \n    EventTime,\n    LEAD(EventTime) OVER (ORDER BY EventTime) as NextEvent\nFROM Logs;\n```\n\n---\n\n## üéØ Your Task\n\nFind the number of days until the *next* purchase for each customer.",
    "starter_code": "-- Next salary in ranking\n\n",
    "solution_code": "SELECT name, salary, LEAD(salary) OVER (ORDER BY salary) AS next_salary FROM employees;",
    "expected_output": "name    | salary | next_salary\nTom     | 60000  | 75000\nAlice   | 75000  | 90000\nBob     | 90000  | NULL",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1084": {
    "id": 1084,
    "title": "FIRST_VALUE and LAST_VALUE",
    "content": "# ü•á FIRST_VALUE & LAST_VALUE\n\n## Anchoring to Extremes\n\n- **FIRST_VALUE**: Returns the value from the first row in the window.\n- **LAST_VALUE**: Returns the value from the last row.\n\n## Use Case: Compare to Baseline\n\n\"How does my salary compare to the entry-level salary?\"\n\n```sql\nSELECT \n    Salary,\n    FIRST_VALUE(Salary) OVER (ORDER BY HireDate) as StartingSalary\nFROM Employees;\n```\n\n## Warning on LAST_VALUE\n\nThe default frame ends at `CURRENT ROW`. So `LAST_VALUE` usually just returns... the current value!\nTo fix:\n`ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`.\n\n---\n\n## üéØ Your Task\n\nCalculate the price change of a product relative to its initial launch price.",
    "starter_code": "-- Highest priced product on each row\n\n",
    "solution_code": "SELECT name, price, FIRST_VALUE(name) OVER (ORDER BY price DESC) AS most_expensive FROM products;",
    "expected_output": "name     | price  | most_expensive\nPremium  | 149.99 | Premium\nWidget B | 99.99  | Premium\nBasic    | 29.99  | Premium",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1085": {
    "id": 1085,
    "title": "NTILE for Bucketing",
    "content": "# ü™£ NTILE: Bucketing Data\n\n## Splitting into Groups\n\n`NTILE(n)` splits the data into `n` approximately equal buckets.\n\n- **NTILE(4)**: Quartiles (Top 25%, Next 25%...)\n- **NTILE(10)**: Deciles\n- **NTILE(100)**: Percentiles\n\n## Use Case: Customer Segmentation\n\n\"Group customers into 3 tiers: Gold, Silver, Bronze\".\n\n```sql\nSELECT \n    Name, \n    NTILE(3) OVER (ORDER BY Spend DESC) as Tier\nFROM Customers;\n```\n\nTier 1 = High spenders\nTier 3 = Low spenders\n\n---\n\n## üéØ Your Task\n\nDivide products into 4 price quartiles using NTILE.",
    "starter_code": "-- 3 price tiers\n\n",
    "solution_code": "SELECT name, price, NTILE(3) OVER (ORDER BY price DESC) AS price_tier FROM products;",
    "expected_output": "name     | price  | price_tier\nPremium  | 149.99 | 1\nWidget B | 99.99  | 1\nWidget A | 49.99  | 2\nBasic    | 29.99  | 3",
    "chapter_id": 207,
    "chapter_title": "Window Functions"
  },
  "1086": {
    "id": 1086,
    "title": "Date Truncation",
    "content": "# ‚úÇÔ∏è Date Truncation: Reducing Granularity\n\n## Rounding Down Time\n\nTruncation means \"shortening\" or \"rounding down\" a timestamp to a specific precision (Day, Month, Year).\n\n## Concept\n\n- **Timestamp**: 2024-03-15 14:30:45\n- **Truncate to Day**: 2024-03-15 00:00:00\n- **Truncate to Month**: 2024-03-01 00:00:00\n- **Truncate to Year**: 2024-01-01 00:00:00\n\n## Why use it?\n\nIt's the primary way to **Group By Time**.\n\"How many sales did we have per month?\" requires truncating individual sales times to the start of their respective months.\n\n---\n\n## üéØ Your Task\n\nTruncate a list of timestamps to the hour.",
    "starter_code": "-- Monthly order counts\n\n",
    "solution_code": "SELECT DATE_TRUNC('month', order_date) AS month, COUNT(*) FROM orders GROUP BY 1 ORDER BY 1;",
    "expected_output": "month      | count\n2024-01-01 | 15\n2024-02-01 | 22\n2024-03-01 | 18",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1087": {
    "id": 1087,
    "title": "DATE_TRUNC Function",
    "content": "# üìÖ DATE_TRUNC: The Power Tool\n\n## Syntax (PostgreSQL)\n\n```sql\nDATE_TRUNC('field', timestamp)\n```\n\nFields: 'microseconds', 'milliseconds', 'second', 'minute', 'hour', 'day', 'week', 'month', 'quarter', 'year', 'decade', 'century', 'millennium'.\n\n## Examples\n\n```sql\nSELECT DATE_TRUNC('month', NOW());\n-- Returns: Start of current month\n\nSELECT DATE_TRUNC('hour', TIMESTAMP '2024-03-15 14:35:00');\n-- Returns: 2024-03-15 14:00:00\n```\n\n## Other Databases\n\n- **MySQL**: `DATE_FORMAT(date, '%Y-%m-01')` or `STR_TO_DATE`\n- **SQL Server**: `DATETRUNC` (newer versions) or `DATEADD/DATEDIFF` tricks.\n\n---\n\n## üéØ Your Task\n\nGroup sales data by month using DATE_TRUNC.",
    "starter_code": "-- Weekly sales\n\n",
    "solution_code": "SELECT DATE_TRUNC('week', sale_date) AS week, SUM(amount) AS total FROM sales GROUP BY 1 ORDER BY 1;",
    "expected_output": "week       | total\n2024-01-01 | 5000\n2024-01-08 | 7500",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1088": {
    "id": 1088,
    "title": "Grouping by Time Periods",
    "content": "# üìä Grouping by Time: Aggregating Trends\n\n## The Pattern\n\n1. Truncate timestamp to desired period (e.g., Week).\n2. Group By that truncated timestamp.\n3. Order By time.\n\n## Weekly Sales Report\n\n```sql\nSELECT \n    DATE_TRUNC('week', order_date) as week_start,\n    COUNT(*) as total_orders,\n    SUM(amount) as revenue\nFROM orders\nGROUP BY 1\nORDER BY 1;\n```\n\n## Why GROUP BY 1?\n\nIt refers to the **first column** in the SELECT list. It's a common shortcut when grouping by expressions like `DATE_TRUNC`.\n\n## Filling Gaps\n\nStandard SQL grouping hides periods with ZERO data.\n(If no sales in Feb, Feb doesn't appear).\nTo fix this, you need **Calendar Tables** (covered later!).\n\n---\n\n## üéØ Your Task\n\nGenerate a daily report of active users.",
    "starter_code": "-- Quarterly revenue\n\n",
    "solution_code": "SELECT DATE_TRUNC('quarter', order_date) AS quarter, SUM(amount) AS total FROM orders GROUP BY 1 ORDER BY 1;",
    "expected_output": "quarter    | total\n2024-01-01 | 45000\n2024-04-01 | 52000",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1089": {
    "id": 1089,
    "title": "Date Arithmetic",
    "content": "# ‚ûï Date Arithmetic: Adding and Subtracting Time\n\n## Intervals\n\nSQL understands \"Intervals\" of time.\n\n```sql\nSELECT NOW() + INTERVAL '1 day';\nSELECT NOW() - INTERVAL '2 months';\n```\n\n## Finding the Difference\n\nSubtracting two timestamps usually gives an **Interval** or **Days** depending on the DB.\n\n```sql\nSELECT '2024-01-01'::date - '2023-12-31'::date;\n-- Returns 1 (integer days) in Postgres\n```\n\n## Calculating Age\n\n```sql\nSELECT AGE('2024-01-01', '1990-05-15');\n-- Returns '33 years 7 mons 17 days'\n```\n\n## Expiration Dates\n\n```sql\nWHERE expiration_date < NOW() + INTERVAL '30 days'\n```\n\n---\n\n## üéØ Your Task\n\nFind orders that are more than 7 days overdue (shipped_date is null and order_date is older than 7 days).",
    "starter_code": "-- Recent orders\n\n",
    "solution_code": "SELECT * FROM orders WHERE order_date >= CURRENT_DATE - INTERVAL '7 days';",
    "expected_output": "order_id | customer_id | amount | order_date\n45       | 101         | 150.00 | 2024-01-14\n46       | 102         | 200.00 | 2024-01-15",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1090": {
    "id": 1090,
    "title": "EXTRACT Function",
    "content": "# üß™ EXTRACT: Getting Components\n\n## Pulling parts of a date\n\nUnlike Truncate (which leaves a timestamp), EXTRACT returns a **Number**.\n\n```sql\nEXTRACT(field FROM source)\n```\n\n## Examples\n\n```sql\nSELECT EXTRACT(YEAR FROM NOW());    -- 2024\nSELECT EXTRACT(MONTH FROM NOW());   -- 3\nSELECT EXTRACT(DOW FROM NOW());     -- Day of Week (0=Sun, 6=Sat)\n```\n\n## Use Cases\n\n- \"Find sales in December\" -> `WHERE EXTRACT(MONTH FROM date) = 12`\n- \"Analyze weekend traffic\" -> `WHERE EXTRACT(DOW FROM date) IN (0, 6)`\n- \"Revenue by hour of day\" -> `GROUP BY EXTRACT(HOUR FROM date)`\n\n---\n\n## üéØ Your Task\n\nFind the average number of logins for each day of the week (Sun-Sat).",
    "starter_code": "-- Year/month breakdown\n\n",
    "solution_code": "SELECT EXTRACT(YEAR FROM order_date) AS year, EXTRACT(MONTH FROM order_date) AS month, COUNT(*) FROM orders GROUP BY 1, 2;",
    "expected_output": "year | month | count\n2024 | 1     | 35\n2024 | 2     | 28",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1091": {
    "id": 1091,
    "title": "Cohort Analysis Basics",
    "content": "# üë• Cohort Analysis: Grouping Users by History\n\n## What is a Cohort?\n\nA group of users who shared a common experience in a specific time period.\n- **Acquisition Cohort**: Users who signed up in Jan 2024.\n- **Behavior Cohort**: Users who bought 'Pro Plan'.\n\n## Why?\n\n\"Are users we stuck in January staying longer than users we got in February?\"\n\n## Step 1: Define the Cohort\n\nAssign every user a \"Cohort Date\" (usually their first action).\n\n```sql\nSELECT \n    UserID, \n    DATE_TRUNC('month', MIN(JoinDate)) as CohortMonth\nFROM Users\nGROUP BY UserID;\n```\n\n---\n\n## üéØ Your Task\n\nAssign every customer to a cohort bucket based on their first purchase month.",
    "starter_code": "-- Monthly cohorts\n\n",
    "solution_code": "SELECT DATE_TRUNC('month', signup_date) AS cohort, COUNT(*) AS size FROM users GROUP BY 1 ORDER BY 1;",
    "expected_output": "cohort     | size\n2024-01-01 | 150\n2024-02-01 | 180\n2024-03-01 | 165",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1092": {
    "id": 1092,
    "title": "Retention Tables",
    "content": "# üß≤ Retention Tables: Avoiding the Churn\n\n## The Classic Heatmap\n\nA Retention Table answers: \"Of the users who joined in Month 0, what % came back in Month 1, Month 2...?\"\n\n## Calculating It\n\n1. **Cohort**: Get user's starting month.\n2. **Activity**: Get all months user was active.\n3. **Offset**: `ActivityMonth - CohortMonth` (Months Since Join).\n4. **Group**: Count users per Cohort + Offset.\n\n## Example Query Structure\n\n```sql\nWITH Cohorts AS (...),\nActivity AS (...)\nSELECT \n    c.CohortMonth,\n    age(a.ActivityMonth, c.CohortMonth) as Gap,\n    COUNT(DISTINCT c.UserID)\nFROM Cohorts c\nJOIN Activity a ON c.UserID = a.UserID\nGROUP BY 1, 2;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate Month 1 retention: How many users from Jan returned in Feb?",
    "starter_code": "-- Retention by cohort\n\n",
    "solution_code": "SELECT DATE_TRUNC('month', u.signup_date) AS cohort, DATE_TRUNC('month', e.event_date) AS month, COUNT(DISTINCT u.user_id) AS active FROM users u JOIN events e ON u.user_id = e.user_id GROUP BY 1, 2;",
    "expected_output": "cohort     | month      | active\n2024-01-01 | 2024-01-01 | 150\n2024-01-01 | 2024-02-01 | 90",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1093": {
    "id": 1093,
    "title": "Session Analysis",
    "content": "# ‚è±Ô∏è Sessionization: Grouping Clicks into Visits\n\n## What is a Session?\n\nA distinct period of activity. If I click, wait 30 minutes, and click again... that's usually a **New Session**.\n\n## The 30 Minute Rule\n\nStandard industry definition: A session ends after 30 minutes of inactivity.\n\n## Finding Sessions in SQL\n\nUse `LAG()` to find gaps larger than 30 mins!\n\n```sql\nSELECT \n    timestamp,\n    CASE \n        WHEN timestamp - LAG(timestamp) OVER (ORDER BY timestamp) > INTERVAL '30 min' \n        THEN 1 ELSE 0 \n    END as NewSessionFlag\nFROM Clicks;\n```\n\nThen do a **Running Sum** of the flags to create Session IDs!\n\n---\n\n## üéØ Your Task\n\nIdentify distinct sessions for a user based on 30-minute gaps.",
    "starter_code": "-- Time gaps between events\n\n",
    "solution_code": "SELECT user_id, event_time, event_time - LAG(event_time) OVER (PARTITION BY user_id ORDER BY event_time) AS time_since_prev FROM events;",
    "expected_output": "user_id | event_time          | time_since_prev\n1       | 2024-01-15 10:00:00 | NULL\n1       | 2024-01-15 10:05:00 | 00:05:00",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1094": {
    "id": 1094,
    "title": "Conversion Funnels",
    "content": "# üìâ Conversion Funnels: Tracking Drop-off\n\n## Visualizing the Flow\n\nHomepage -> Product Page -> Cart -> Checkout -> Thanks.\n\nAt each step, we lose people.\n\n## Fixed Step Approach\n\nCount users who did Step 1. Then Step 2. Then Step 3.\n\n```sql\nSELECT \n    COUNT(CASE WHEN Event = 'Home' THEN 1 END) as Step1,\n    COUNT(CASE WHEN Event = 'Cart' THEN 1 END) as Step2\nFROM Events;\n```\n\n## Advanced: Ordered Steps\n\nEnsuring they did Step 1 **BEFORE** Step 2 requires Self-Joins or specialized Window measures (or `match_recognize` in advanced DBs).\n\n---\n\n## üéØ Your Task\n\nCalculate the conversion rate from 'View' to 'Purchase'.",
    "starter_code": "-- Funnel stages\n\n",
    "solution_code": "SELECT COUNT(DISTINCT CASE WHEN event_type = 'view' THEN user_id END) AS views, COUNT(DISTINCT CASE WHEN event_type = 'click' THEN user_id END) AS clicks, COUNT(DISTINCT CASE WHEN event_type = 'purchase' THEN user_id END) AS purchases FROM events;",
    "expected_output": "views | clicks | purchases\n1000  | 350    | 75",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1095": {
    "id": 1095,
    "title": "Gap Analysis",
    "content": "# üï≥Ô∏è Gap Analysis: Finding Missing Data\n\n## Where are the holes?\n\n- Finding missing sequence numbers (Invoice 1, 2, 4... where is 3?)\n- Finding days with zero sales.\n\n## Sequence Gaps\n\n```sql\nSELECT t1.id + 1 as missing_id\nFROM table t1\nLEFT JOIN table t2 ON t1.id + 1 = t2.id\nWHERE t2.id IS NULL;\n```\n\nChecks if \"Next ID\" exists. If not, we found a gap!\n\n---\n\n## üéØ Your Task\n\nFind any missing days in the daily login logs.",
    "starter_code": "-- Dates with sales\n\n",
    "solution_code": "SELECT sale_date, COUNT(*) AS num_sales FROM sales GROUP BY sale_date ORDER BY sale_date;",
    "expected_output": "sale_date  | num_sales\n2024-01-01 | 5\n2024-01-02 | 8\n2024-01-04 | 3",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1096": {
    "id": 1096,
    "title": "Calendar Tables",
    "content": "# üìÖ Calendar Tables: Analysis Infrastructure\n\n## Never depend on your data for dates!\n\nIf you group `Orders` by date, days with 0 orders disappear.\nYou need a **Calendar Table** (or Date Dimension) that contains EVERY date.\n\n## Usage\n\n**LEFT JOIN** from Calendar TO your Data.\n\n```sql\nSELECT \n    c.Date,\n    COUNT(o.ID) as Orders  -- Returns 0 if null\nFROM Calendar c\nLEFT JOIN Orders o ON c.Date = o.OrderDate\nWHERE c.Date BETWEEN '2024-01-01' AND '2024-01-31'\nGROUP BY c.Date;\n```\n\n## Generating One\n\n```sql\nSELECT generate_series(\n    '2020-01-01'::date, \n    '2030-12-31'::date, \n    '1 day'\n);\n```\n\n---\n\n## üéØ Your Task\n\nUse a generated date series to report sales for every day in January, filling 0s.",
    "starter_code": "-- Join with calendar\n\n",
    "solution_code": "SELECT c.year, c.month, COALESCE(SUM(o.amount), 0) AS total FROM calendar c LEFT JOIN orders o ON c.date = o.order_date GROUP BY 1, 2 ORDER BY 1, 2;",
    "expected_output": "year | month | total\n2024 | 1     | 45000\n2024 | 2     | 52000",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1097": {
    "id": 1097,
    "title": "Year-over-Year Comparisons",
    "content": "# üìä DoD, WoW, MoM, YoY: Period Comparisons\n\n## Concept\n\n\"How does today compare to the same day last year?\"\n\n## Using LAG with Offsets\n\nIf you have daily data, Last Year is 365 rows ago.\n`LAG(Value, 365) OVER (ORDER BY Date)`\n\n## Self Join Strategy\n\nOften more robust (handles leap years).\n\n```sql\nSELECT \n    ThisYear.Year, \n    ThisYear.Revenue,\n    LastYear.Revenue as PrevRevenue\nFROM AnnualStats ThisYear\nJOIN AnnualStats LastYear \n  ON ThisYear.Year = LastYear.Year + 1;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate the Year-over-Year growth percentage for annual revenue.",
    "starter_code": "-- Month over month\n\n",
    "solution_code": "SELECT DATE_TRUNC('month', order_date) AS month, SUM(amount) AS revenue, LAG(SUM(amount)) OVER (ORDER BY DATE_TRUNC('month', order_date)) AS prev_month FROM orders GROUP BY 1 ORDER BY 1;",
    "expected_output": "month      | revenue | prev_month\n2024-01-01 | 45000   | NULL\n2024-02-01 | 52000   | 45000\n2024-03-01 | 48000   | 52000",
    "chapter_id": 208,
    "chapter_title": "Time-Series SQL"
  },
  "1098": {
    "id": 1098,
    "title": "üßô Analytics Wizard Challenge",
    "content": "# üßô Challenge: The Grand Dashboard\n\n## The Scenario\n\nYou are the Head of Data for an E-commerce giant. The CEO needs a comprehensive monthly dashboard.\n\n## The Requirements\n\nProduce a single result set showing for each month:\n1. **Total Revenue**\n2. **New Users** (First time purchasers)\n3. **Repeat Users** (Purchased before)\n4. **AOV** (Average Order Value)\n\n## Hints\n\n- You'll likely need a CTE to determine \"First Order Date\" for every user.\n- Then join that back to your main Sales table.\n- Group everything by Month.\n\n---\n\n## üéØ Your Task\n\nConstruct this master report using CTEs and Aggregation.",
    "starter_code": "-- The Analytics Wizard Challenge\n-- Monthly revenue with:\n-- - Running total (YTD)\n-- - MoM change\n-- - Rank by revenue\n\n",
    "solution_code": "WITH monthly AS (\n  SELECT \n    DATE_TRUNC('month', order_date) AS month,\n    SUM(amount) AS revenue\n  FROM orders\n  GROUP BY 1\n)\nSELECT \n  month,\n  revenue,\n  SUM(revenue) OVER (ORDER BY month) AS ytd_total,\n  revenue - LAG(revenue) OVER (ORDER BY month) AS mom_change,\n  RANK() OVER (ORDER BY revenue DESC) AS revenue_rank\nFROM monthly\nORDER BY month;",
    "expected_output": "month      | revenue | ytd_total | mom_change | revenue_rank\n2024-01-01 | 45000   | 45000     | NULL       | 3\n2024-02-01 | 52000   | 97000     | 7000       | 1\n2024-03-01 | 48000   | 145000    | -4000      | 2",
    "chapter_id": 251,
    "chapter_title": "Analytics Wizard Boss"
  },
  "1099": {
    "id": 1099,
    "title": "Identifying Duplicates",
    "content": "# üëØ Identifying Duplicates: Finding the Clones\n\n## Why Duplicates Happen?\n\n- Double submission on forms.\n- Retry logic in APIs.\n- Bad data merges.\n\n## How to Find Them\n\nGroup by the columns that *should* be unique, and check `COUNT(*) > 1`.\n\n```sql\nSELECT Email, COUNT(*)\nFROM Users\nGROUP BY Email\nHAVING COUNT(*) > 1;\n```\n\nThis returns any email appearing more than once.\n\n## Finding Entire Duplicate Rows\n\n```sql\nSELECT Col1, Col2, Col3, COUNT(*)\nFROM Table\nGROUP BY Col1, Col2, Col3\nHAVING COUNT(*) > 1;\n```\n\n---\n\n## üéØ Your Task\n\nIdentify all emails that are registered to more than one user ID.",
    "starter_code": "-- Find products with duplicate names\n-- Show the name and how many times it appears\n\n",
    "solution_code": "SELECT name, COUNT(*) as count\nFROM products\nGROUP BY name\nHAVING COUNT(*) > 1;",
    "expected_output": "name       | count\nWidget A   | 2\nGadget Pro | 3",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1100": {
    "id": 1100,
    "title": "Deduplication with ROW_NUMBER",
    "content": "# üßπ Deduplication: The Clean Sweep\n\n## The Best Way to Remove Duplicates\n\nUsing `ROW_NUMBER()` is the standard pattern for deduplication.\n\n## The Strategy\n\n1. Partition by the columns that define duplicates.\n2. Order by a preference (e.g., Keep the Latest one).\n3. Delete where `RowNumber > 1`.\n\n## Example\n\n```sql\nWITH Ranked AS (\n    SELECT \n        ID, \n        ROW_NUMBER() OVER (\n            PARTITION BY Email \n            ORDER BY CreatedAt DESC\n        ) as rn\n    FROM Users\n)\nDELETE FROM Users\nWHERE ID IN (\n    SELECT ID FROM Ranked WHERE rn > 1\n);\n```\n\nThis keeps the *most recent* account for each email and deletes the older ones.\n\n---\n\n## üéØ Your Task\n\nSimulate deduplication by selecting only the latest login for each user.",
    "starter_code": "-- Deduplicate orders\n-- Keep the most recent order per customer\n\n",
    "solution_code": "WITH ranked AS (\n  SELECT *,\n    ROW_NUMBER() OVER (\n      PARTITION BY customer_id\n      ORDER BY order_date DESC\n    ) as rn\n  FROM orders\n)\nSELECT * FROM ranked WHERE rn = 1;",
    "expected_output": "customer_id | order_date | amount | rn\n1           | 2024-03-15 | 150    | 1\n2           | 2024-03-10 | 200    | 1",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1101": {
    "id": 1101,
    "title": "Keeping First/Last Record",
    "content": "# ü•á Keeping First/Last: The Survivor\n\n## Scenarios\n\n- \"Current Status\" (Last update)\n- \"Acquisition Source\" (First click)\n- \"Initial Price\"\n\n## Using DISTINCT ON (PostgreSQL)\n\n```sql\nSELECT DISTINCT ON (UserID) \n    UserID, Status, UpdatedAt\nFROM StatusHistory\nORDER BY UserID, UpdatedAt DESC;\n```\n\n## Using Window Functions (Universal)\n\nSame as deduplication: Partition by User, Order by Date, filter `rn = 1`.\n\n---\n\n## üéØ Your Task\n\nFind the very first product ever purchased by each customer.",
    "starter_code": "-- Find first purchase per customer\n-- Return customer_id and their first order_date\n\n",
    "solution_code": "SELECT customer_id,\n  MIN(order_date) as first_purchase\nFROM orders\nGROUP BY customer_id;",
    "expected_output": "customer_id | first_purchase\n1           | 2024-01-05\n2           | 2024-01-12\n3           | 2024-02-01",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1102": {
    "id": 1102,
    "title": "Detecting Outliers",
    "content": "# üö® Detecting Outliers: Finding Anomalies\n\n## What is an Outlier?\n\nData points that are far from the norm.\n- $1M purchase on a $10 item.\n- 200 year old user.\n\n## Method 1: Mean + StdDev (Z-Score)\n\nIf data is normal (Bell Curve):\n`Value > Avg + (3 * StdDev)` is extremely rare (99.7% threshold).\n\n## Method 2: Interquartile Range (IQR)\n\nFocus on the middle 50% of data. Anything far outside that box is suspicious.\n\n---\n\n## üéØ Your Task\n\nFind orders that are more than 3 standard deviations above the average amount.",
    "starter_code": "-- Find outlier orders\n-- Amount > 3x the average\n\n",
    "solution_code": "SELECT *\nFROM orders\nWHERE amount > 3 * (SELECT AVG(amount) FROM orders);",
    "expected_output": "order_id | amount\n42       | 15000\n67       | 12500",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1103": {
    "id": 1103,
    "title": "Percentile Calculations",
    "content": "# üìä Percentiles: Understanding Distribution\n\n## Averages Lay!\n\n\"Average income is $1M\" (Bill Gates walked into the bar).\n**Median** (50th Percentile) tells the true story.\n\n## Calculating Percentiles\n\nFunctions: `PERCENTILE_CONT` (Continuous) or `PERCENTILE_DISC` (Discrete).\n\n```sql\nSELECT \n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY Salary) as Median,\n    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY Salary) as Top10Percent\nFROM Employees;\n```\n\n(Note: Syntax varies heavily across DBs. some use `APPROX_PERCENTILE`).\n\n---\n\n## ÔøΩÔøΩ Your Task\n\nCalculate the median order value (50th percentile).",
    "starter_code": "-- Find the median (50th percentile) order amount\n\n",
    "solution_code": "SELECT\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) as median_amount\nFROM orders;",
    "expected_output": "median_amount\n125.50",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1104": {
    "id": 1104,
    "title": "Handling Outliers",
    "content": "# üß§ Handling Outliers: Fix or Filter?\n\n## Options\n\n1. **Delete**: If it's technically impossible (Age = 200).\n2. **Cap/Floor**: \"If Age > 100, set to 100\". (Winsorizing)\n3. **Exclude**: Keep in DB, filter from analysis.\n4. **Investigate**: It might be your \"Whale\" customer!\n\n## Capping Calculation\n\n```sql\nSELECT \n    LEAST(Amount, 1000) as CappedAmount\nFROM Orders;\n```\nAny order > 1000 becomes 1000.\n\n---\n\n## üéØ Your Task\n\nCalculate average order value, excluding the top 1% of outliers.",
    "starter_code": "-- Cap order amounts at 1000\n-- Show order_id and capped_amount\n\n",
    "solution_code": "SELECT order_id,\n  LEAST(amount, 1000) as capped_amount\nFROM orders;",
    "expected_output": "order_id | capped_amount\n1        | 500\n2        | 1000\n3        | 750",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1105": {
    "id": 1105,
    "title": "Metric Definitions Matter",
    "content": "# üìè Metric Definitions: Context is King\n\n## \"How many users do we have?\"\n\n1. Registered users?\n2. Active users (logged in today)?\n3. Paying users?\n4. Users who haven't deleted their account?\n\n## The Lesson\n\nNever just write SQL. **Define the metric first.**\n\nBad: `SELECT COUNT(*) FROM Users`\nGood: `SELECT COUNT(*) FROM Users WHERE Status = 'Active' AND LastLogin > NOW() - INTERVAL '30 days'`\n\n---\n\n## üéØ Your Task\n\nCalculate \"Churned Users\" defined as \"Paid previously, but subscription expired > 30 days ago\".",
    "starter_code": "-- Calculate net revenue\n-- Exclude orders with status = 'refunded'\n\n",
    "solution_code": "SELECT SUM(amount) as net_revenue\nFROM orders\nWHERE status != 'refunded';",
    "expected_output": "net_revenue\n45000",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1106": {
    "id": 1106,
    "title": "Active User Definitions",
    "content": "# üèÉ Active Users: DAU / MAU\n\n## DAU (Daily Active Users)\n\nUnique users who performed a \"meaningful action\" today.\n\n## MAU (Monthly Active Users)\n\nUnique users in the last 30 days.\n\n## Stickiness Ratio\n\nDAU / MAU.\nIf 100 people use it monthly, and 50 use it daily, stickiness = 50%.\nHigh stickiness = Habit forming product (Facebook).\nLow stickiness = Utility product (Tax software).\n\n---\n\n## üéØ Your Task\n\nCalculate MAU (Active in last 30 days) for the current date.",
    "starter_code": "-- Calculate DAU\n-- Count unique users per day\n\n",
    "solution_code": "SELECT\n  DATE(event_time) as date,\n  COUNT(DISTINCT user_id) as dau\nFROM events\nGROUP BY DATE(event_time)\nORDER BY date;",
    "expected_output": "date       | dau\n2024-03-01 | 150\n2024-03-02 | 175\n2024-03-03 | 160",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1107": {
    "id": 1107,
    "title": "Avoiding Double Counting",
    "content": "# ‚ö†Ô∏è Avoiding Double Counting\n\n## Common Mistakes\n\n1. **Joining without deduping** - inflates counts\n2. **Missing DISTINCT** - counts rows not unique values\n3. **Wrong aggregation level** - summing at wrong grain\n\n## Bad: Double Counting\n\n```sql\n-- If user has multiple orders, counted multiple times!\nSELECT COUNT(*) FROM users u\nJOIN orders o ON u.id = o.user_id;\n```\n\n## Good: Distinct Count\n\n```sql\nSELECT COUNT(DISTINCT u.id) as unique_customers\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n```\n\n## Watch for Many-to-Many\n\n```sql\n-- Pre-aggregate to avoid explosion\nWITH user_orders AS (\n  SELECT user_id, SUM(amount) as total\n  FROM orders GROUP BY user_id\n)\nSELECT * FROM users u\nJOIN user_orders uo ON u.id = uo.user_id;\n```\n\n---\n\n## üéØ Your Task\n\nCount unique customers who have placed orders.\n",
    "starter_code": "-- Count unique customers with orders\n-- Don't double count!\n\n",
    "solution_code": "SELECT COUNT(DISTINCT customer_id) as unique_customers\nFROM orders;",
    "expected_output": "unique_customers\n250",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1108": {
    "id": 1108,
    "title": "Data Quality Checks",
    "content": "# ‚úÖ Data Quality Checks: Trust but Verify\n\n## Garbage In, Garbage Out\n\nBefore doing analytics, run health checks.\n\n1. **Uniqueness**: Are primary keys actually unique?\n2. **Completeness**: Are critical fields (Price) NULL?\n3. **Validity**: Are dates in the future? Is age negative?\n4. **Consistency**: Does `Total` match `Quantity * Price`?\n\n## Querying for Issues\n\n```sql\nSELECT * FROM Orders\nWHERE OrderDate > ShipDate; -- Impossible! Shipped before ordered?\n```\n\n---\n\n## üéØ Your Task\n\nFind records where the start time is AFTER the end time.",
    "starter_code": "-- Count products with NULL category_id\n\n",
    "solution_code": "SELECT COUNT(*) as missing_category\nFROM products\nWHERE category_id IS NULL;",
    "expected_output": "missing_category\n15",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1109": {
    "id": 1109,
    "title": "Assertion Queries",
    "content": "# üõ°Ô∏è Assertion Queries: Automated Testing\n\n## Continuous Monitoring\n\nTurn your quality checks into assertions that run daily.\n\n```sql\n-- Should return 0 rows. If > 0, alert the team.\nSELECT ID \nFROM Payments \nWHERE Amount < 0;\n```\n\n## Dashboard Health\n\nAdd a \"Data Health\" tab to dashboards.\n\"Rows with Missing Country: 0.5% (Acceptable)\"\n\n---\n\n## üéØ Your Task\n\nWrite an assertion query that returns any product with a negative stock count.",
    "starter_code": "-- Find orders with invalid (negative) amounts\n-- Should return 0 rows if data is clean\n\n",
    "solution_code": "SELECT * FROM orders\nWHERE amount < 0;",
    "expected_output": "order_id | amount\n(no rows)",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1110": {
    "id": 1110,
    "title": "Metric Validation",
    "content": "# üìä Metric Validation\n\n## Trust But Verify\n\nAlways sanity-check your metrics:\n\n```sql\n-- Does total match sum of parts?\nSELECT \n  SUM(amount) as total_revenue,\n  SUM(CASE WHEN channel = 'web' THEN amount ELSE 0 END) as web,\n  SUM(CASE WHEN channel = 'mobile' THEN amount ELSE 0 END) as mobile\nFROM orders;\n-- web + mobile should equal total!\n```\n\n## Year-over-Year Sense Check\n\n```sql\nSELECT\n  EXTRACT(YEAR FROM order_date) as year,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders\nGROUP BY EXTRACT(YEAR FROM order_date)\nORDER BY year;\n-- Wild swings? Investigate!\n```\n\n## Cross-Table Validation\n\n```sql\n-- Order count should match\nSELECT 'orders' as source, COUNT(*) FROM orders\nUNION ALL\nSELECT 'order_items', COUNT(DISTINCT order_id) FROM order_items;\n```\n\n---\n\n## üéØ Your Task\n\nCompare total revenue against the sum of web and mobile channels.\n",
    "starter_code": "-- Validate revenue by channel\n-- Total should equal web + mobile\n\n",
    "solution_code": "SELECT\n  SUM(amount) as total_revenue,\n  SUM(CASE WHEN channel = 'web' THEN amount ELSE 0 END) as web_revenue,\n  SUM(CASE WHEN channel = 'mobile' THEN amount ELSE 0 END) as mobile_revenue\nFROM orders;",
    "expected_output": "total_revenue | web_revenue | mobile_revenue\n50000         | 30000       | 20000",
    "chapter_id": 209,
    "chapter_title": "Data Cleaning & Metrics"
  },
  "1111": {
    "id": 1111,
    "title": "Why Tables Split",
    "content": "# üî® Why Tables Split: The Separation of Concerns\n\n## The Mega-Table Problem\n\nImagine a table `CompanyData` with columns:\n`EmployeeName`, `EmployeePhone`, `ProjectName`, `ProjectDeadline`, `OfficeLocation`.\n\n**Problems:**\n1. **Repetition**: If 50 people work in \"New York\", we store \"New York\" 50 times.\n2. **Update Anomaly**: If the deadline changes, we update 50 rows.\n3. **Deletion Anomaly**: If everyone leaves a project, we lose the project data!\n\n## Splitting Logic\n\nWe split based on **Entity** (Thing):\n- **Employees** table.\n- **Projects** table.\n- **Offices** table.\n\nThen we link them with IDs.\n\n---\n\n## üéØ Your Task\n\nIdentify columns that belong in a separate 'Department' table versus the 'Employee' table.",
    "starter_code": "-- Count orders per customer\n\n",
    "solution_code": "SELECT customer_id, COUNT(*) as total_orders\nFROM orders\nGROUP BY customer_id;",
    "expected_output": "customer_id | total_orders\n1           | 5\n2           | 3",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1112": {
    "id": 1112,
    "title": "Normalization Basics",
    "content": "# üìè Normalization: Organizing Data\n\n## The Goal\n\nReduce redundancy and improve data integrity.\n\n## The Forms (Levels)\n\n1. **1NF (First Normal Form)**: Atomic values (no lists in cells).\n2. **2NF**: No partial dependencies (everything depends on the whole key).\n3. **3NF**: No transitive dependencies (only depend on the key, nothing else).\n\n\"The Key, the Whole Key, and Nothing but the Key (so help me Codd).\"\n\n## Trade-off\n\nHighly normalized DBs are great for **writing** (OLTP) but can be slow for **reading** (requires many joins).\n\n---\n\n## üéØ Your Task\n\nStructuring a table to avoid repeating customer address data for every order.",
    "starter_code": "-- Select distinct emails\n\n",
    "solution_code": "SELECT DISTINCT email\nFROM customers;",
    "expected_output": "email\nalice@mail.com\nbob@mail.com",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1113": {
    "id": 1113,
    "title": "First Normal Form",
    "content": "# 1Ô∏è‚É£ First Normal Form (1NF)\n\n## Rule: Atomicity\n\nEvery cell must contain a **single value**.\n\n**Bad (Not 1NF):**\n| ID | Name | PhoneNumbers |\n|----|------|--------------|\n| 1  | Ali  | 555-1234, 555-9876 |\n\n**Good (1NF):**\n| ID | Name | Phone |\n|----|------|-------|\n| 1  | Ali  | 555-1234 |\n| 1  | Ali  | 555-9876 |\n\n*(Or better, a separate Phone table!)*\n\n## Rule: Unique Rows\n\nNo duplicate rows allowed. Primary Key required.\n\n---\n\n## üéØ Your Task\n\nFix a schema where multiple tags are stored in a single comma-separated string.",
    "starter_code": "-- Join users with their phones\n\n",
    "solution_code": "SELECT u.name, p.phone\nFROM users u\nJOIN user_phones p ON u.id = p.user_id;",
    "expected_output": "name  | phone\nAlice | 555-1234\nAlice | 555-5678",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1114": {
    "id": 1114,
    "title": "Second and Third Normal Form",
    "content": "# 2Ô∏è‚É£3Ô∏è‚É£ Second & Third Normal Form\n\n## 2NF: The Whole Key\n\nApplies to composite keys (keys made of 2+ columns).\nEvery non-key column must depend on BOTH parts of the key.\n\n**Bad**: `OrderItem(OrderID, ProductID, OrderDate)`\nOrderDate depends only on OrderID, not ProductID! Move it to Orders table.\n\n## 3NF: Nothing But The Key\n\nNo transitive dependencies.\nA non-key column shouldn't depend on another non-key column.\n\n**Bad**: `Employee(ID, ZipCode, City)`\nCity describes ZipCode, not Employee directly.\n**Fix**: `ZipCodes(Zip, City)` and `Employee(ID, Zip)`.\n\n---\n\n## üéØ Your Task\n\nIdentify the violation of 3NF in a table containing State codes and State names.",
    "starter_code": "-- Join employees and departments\n\n",
    "solution_code": "SELECT e.name, d.name as department\nFROM employees e\nJOIN departments d ON e.department_id = d.id;",
    "expected_output": "name  | department\nAlice | Sales\nBob   | Marketing",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1115": {
    "id": 1115,
    "title": "Fact Tables",
    "content": "# üìä Fact Tables: The Center of Analytics\n\n## What is a Fact Table?\n\nIn Data Warehousing (Star Schema), the Fact Table holds the **Measurements** (Numbers).\n\n- **Examples**: `Sales`, `Clicks`, `TemperatureReadings`.\n- **Structure**: Mostly Foreign Keys (`ProductID`, `DateID`, `StoreID`) and Metrics (`Amount`, `Qty`).\n- **Characteristics**: Very long (billions of rows), very narrow (few columns).\n\n## The \"Verb\"\n\nFact tables usually represent an **Event** or Action. \"A sale happened.\"\n\n---\n\n## üéØ Your Task\n\nDesign the columns for a 'PageViews' fact table.",
    "starter_code": "-- Sum all order amounts\n\n",
    "solution_code": "SELECT SUM(amount) as total_revenue\nFROM orders;",
    "expected_output": "total_revenue\n125000",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1116": {
    "id": 1116,
    "title": "Dimension Tables",
    "content": "# üè∑Ô∏è Dimension Tables: The Context\n\n## What is a Dimension?\n\nDimensions describe the **Who, What, Where, When** of the facts.\n\n- **Examples**: `Products`, `Customers`, `Dates`, `Stores`.\n- **Structure**: Primary Key and many descriptive text columns (`Color`, `Size`, `Manager`, `Region`).\n- **Characteristics**: Short (fewer rows), very wide (many attributes).\n\n## The \"Noun\"\n\nDimensions represent the **Entities** involved in the event.\n\n---\n\n## üéØ Your Task\n\nSelect appropriate attributes for a 'Product' dimension table.",
    "starter_code": "-- Get all customer attributes\n\n",
    "solution_code": "SELECT * FROM customers;",
    "expected_output": "id | name  | email          | segment\n1  | Alice | alice@mail.com | Premium",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1117": {
    "id": 1117,
    "title": "Star Schema",
    "content": "# ‚≠ê Star Schema: Optimized for Reads\n\n## The Constellation\n\nA design pattern for Analytics (OLAP).\n\n- **Center**: One Fact Table (Sales).\n- **Points**: Dimension Tables (Time, Product, Customer) linked to the center.\n\n## Why \"Star\"?\n\nVisualizing the diagram looks like a star.\n\n## Benefits\n\n- **Performance**: Joins are simple (Fact -> Dim). Finding data is fast.\n- **Understanding**: Easy for business users to query (\"Drag Time and Revenue\").\n\n## Contrast with Snowflake\n\nSnowflake schema normalizes dimensions (Product -> Brand -> Category). Star keeps them denormalized (Product table has Brand and Category columns).\n\n---\n\n## üéØ Your Task\n\nWrite a query joining a Fact table to two Dimension tables.",
    "starter_code": "-- Star join: orders + customers + products\n\n",
    "solution_code": "SELECT c.name as customer, p.name as product, o.amount\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN products p ON o.product_id = p.id;",
    "expected_output": "customer | product | amount\nAlice    | Widget  | 50",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1118": {
    "id": 1118,
    "title": "Primary Key Constraints",
    "content": "# üîë Primary Key: The Unique ID\n\n## The Golden Rule\n\nEvery table should have a Primary Key (PK).\n- **Unique**: No two rows can have the same ID.\n- **Not Null**: Cannot be empty.\n- **Stable**: Shouldn't change over time (don't use Email, people change emails!).\n\n## Natural vs Surrogate\n\n- **Natural**: `SSN`, `ISBN`. Real-world uniqueness.\n- **Surrogate**: `1, 2, 3...` (Auto-increment). Database generated.\n*Surrogate is usually safer.*\n\n## Syntax\n\n```sql\nCREATE TABLE Users (\n    ID SERIAL PRIMARY KEY,\n    Name TEXT\n);\n```\n\n---\n\n## üéØ Your Task\n\nIdentify which column is best suited to be the Primary Key.",
    "starter_code": "-- Check for duplicate IDs\n\n",
    "solution_code": "SELECT id, COUNT(*)\nFROM users\nGROUP BY id\nHAVING COUNT(*) > 1;",
    "expected_output": "id | count\n(no rows)",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1119": {
    "id": 1119,
    "title": "Foreign Key Constraints",
    "content": "# üîó Foreign Key: The Link\n\n## Enforcing Relationships\n\nA Foreign Key (FK) in one table points to a Primary Key in another.\n\n## Referential Integrity\n\nThe DB ensures you can't have an \"Orphan\":\n- You can't add an `Order` for a `CustomerID` that doesn't exist.\n- You can't delete a `Customer` if they still have `Orders` (unless you CASCADE).\n\n## Syntax\n\n```sql\nCREATE TABLE Orders (\n    ID SERIAL PRIMARY KEY,\n    CustomerID INT REFERENCES Customers(ID)\n);\n```\n\n---\n\n## üéØ Your Task\n\nTry to insert a record with an invalid Foreign Key and catch the error.",
    "starter_code": "-- Find orphan orders\n\n",
    "solution_code": "SELECT o.*\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL;",
    "expected_output": "order_id | customer_id\n(no rows)",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1120": {
    "id": 1120,
    "title": "Unique Constraints",
    "content": "# ü¶Ñ Unique Constraints: One of a Kind\n\n## Beyond Primary Keys\n\nYou can enforce uniqueness on columns that aren't the ID.\n\n- **Emails**: ID is PK, but Email must also be unique.\n- **Usernames**.\n- **SKU Codes**.\n\n## Composite Unique\n\n\"A user can only like a post once.\"\n`UNIQUE (UserID, PostID)`\n\n## Syntax\n\n```sql\nCREATE TABLE Users (\n    ID SERIAL PRIMARY KEY,\n    Email TEXT UNIQUE,\n    Username TEXT,\n    CONSTRAINT unique_user_name UNIQUE (Username)\n);\n```\n\n---\n\n## üéØ Your Task\n\nAdd a constraint to ensure no two products have the same barcode.",
    "starter_code": "-- Find duplicate emails\n\n",
    "solution_code": "SELECT email, COUNT(*)\nFROM users\nGROUP BY email\nHAVING COUNT(*) > 1;",
    "expected_output": "email | count\n(no rows)",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1121": {
    "id": 1121,
    "title": "When to Denormalize",
    "content": "# üí• When to Denormalize: Breaking the Rules\n\n## The Performance Trade-off\n\nNormalization requires JOINs. JOINs take time.\nSometimes, we duplicate data ON PURPOSE to read it faster.\n\n## Examples\n\n1. **Cached Totals**: Storing `TotalSpent` on the `User` table instead of summing `Orders` every time.\n2. **History**: Freezing the `ProductPrice` in the `OrderItems` table so it doesn't change if the main product price changes.\n3. **Star Schema**: Dimensions are often denormalized for simple reporting.\n\n## The Cost\n\nYou must manage updates carefully! If you update the source, you must update the copy.\n\n---\n\n## üéØ Your Task\n\nDetermine if a scenario justifies denormalization for performance.",
    "starter_code": "-- Get daily order counts\n\n",
    "solution_code": "SELECT DATE(order_date) as date, COUNT(*) as order_count\nFROM orders\nGROUP BY DATE(order_date);",
    "expected_output": "date       | order_count\n2024-03-01 | 15\n2024-03-02 | 23",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1122": {
    "id": 1122,
    "title": "Analytics Schema Design",
    "content": "# üèóÔ∏è Analytics Schema: Designing for Questions\n\n## Work Backwards\n\nDon't start with data. Start with questions.\n\"CEO wants to know Revenue by Region by Month\".\n\n**Grain**: The level of detail. \"One row per Sale\".\n\n**Dimensions**:\n- Date (Hierarchy: Year > Qtr > Month > Day)\n- Location (Region > Country > City)\n\n**Facts**:\n- Revenue (Summable)\n- Discount (Summable)\n- Profit (Summable)\n\n## OBT (One Big Table)\n\nModern warehouses (BigQuery/Snowflake) often prefer one massive, wide table over Star Schema because storage is cheap and JOINs are expensive.\n\n---\n\n## üéØ Your Task\n\nDesign a schema to track website interactions and marketing attribution.",
    "starter_code": "-- Monthly revenue aggregation\n\n",
    "solution_code": "SELECT DATE_TRUNC('month', order_date) as month, SUM(amount) as revenue\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;",
    "expected_output": "month      | revenue\n2024-01-01 | 45000\n2024-02-01 | 52000",
    "chapter_id": 210,
    "chapter_title": "Database Design Essentials"
  },
  "1123": {
    "id": 1123,
    "title": "Why Performance Matters",
    "content": "# üöÄ Why Performance Matters\n\n## The \"It Works\" Trap\n\nA query that takes 1ms on your test machine (10 rows) might take 1 hour in production (10 million rows).\n\n## Impacts of Slow Queries\n\n- **User Experience**: Page loads lag.\n- **Cost**: Cloud databases bill by compute/IO.\n- **Availability**: One slow query can lock tables and crash the site.\n\n## Big O for SQL\n\n- **Index Scan**: O(log N) - Fast\n- **Full Table Scan**: O(N) - Slow\n- **Nested Loop Join (bad)**: O(N*M) - Very Slow\n\n---\n\n## üéØ Your Task\n\nCompare the execution time of two queries achieving the same result.",
    "starter_code": "-- Select only id and name from users\n\n",
    "solution_code": "SELECT id, name FROM users;",
    "expected_output": "id | name\n1  | Alice\n2  | Bob",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1124": {
    "id": 1124,
    "title": "Introduction to Indexes",
    "content": "# üîñ Intro to Indexes: The Book Index\n\n## An Analogy\n\nImagine finding \"Harry Potter\" in a library.\n- **No Index**: Walk every shelf, scan every spine. (Full Table Scan)\n- **Index**: Go to computer, type \"Harry Potter\", get Shelf #4. (Index Scan)\n\n## Syntax\n\n```sql\nCREATE INDEX idx_lastname ON Users (LastName);\n```\n\n## What it does\n\nIt creates a separate data structure (B-Tree) that keeps `LastName` sorted and points to the actual row.\n\n## When to Index?\n\n- Columns in **WHERE** clauses.\n- Columns in **JOIN** conditions.\n- Columns in **ORDER BY**.\n\n---\n\n## üéØ Your Task\n\nCreate an index on the 'Email' column of the Users table.",
    "starter_code": "-- Find user by email\n\n",
    "solution_code": "SELECT * FROM users WHERE email = 'alice@mail.com';",
    "expected_output": "id | name  | email\n1  | Alice | alice@mail.com",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1125": {
    "id": 1125,
    "title": "How Indexes Work",
    "content": "# üå≥ How Indexes Work: The B-Tree\n\n## Balanced Tree\n\nMost SQL indexes use a B-Tree structure.\n- Root Node -> Branch Nodes -> Leaf Nodes.\n- Finding any value takes very few steps (Logarithmic time).\n\n## Searching\n\nQuery: `WHERE ID = 505`\n1. Root: \"Go right for > 500\"\n2. Branch: \"Go left for < 600\"\n3. Leaf: Found 505! Pointer to data.\n\n## Writing Cost\n\nIndexes speed up READS but slow down WRITES.\nEvery `INSERT` or `UPDATE` must also update the index tree.\n\n---\n\n## üéØ Your Task\n\nIdentify which query operation (SELECT vs INSERT) would be slower after adding an index.",
    "starter_code": "-- Find orders in March 2024\n\n",
    "solution_code": "SELECT * FROM orders\nWHERE order_date BETWEEN '2024-03-01' AND '2024-03-31';",
    "expected_output": "order_id | order_date | amount\n10       | 2024-03-15 | 100",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1126": {
    "id": 1126,
    "title": "Index Trade-offs",
    "content": "# ‚öñÔ∏è Index Trade-offs: Not Free Lunch\n\n## The Costs\n\n1. **Storage**: Indexes take up disk space. Sometimes more than the table itself!\n2. **Write Penalty**: Updating the index on every change.\n3. **Maintenance**: Indexes can get fragmented.\n\n## Over-Indexing\n\nDon't index everything.\n- Low Cardinality (Gender: M/F) -> Index useless (use Bitmap if available).\n- High Cardinality (Email, UUID) -> Index great.\n\n## Rule of Thumb\n\nIndex columns used frequently for filtering/joining. Monitor usage and drop unused indexes.\n\n---\n\n## üéØ Your Task\n\nDecide whether to index the 'Status' column (3 values) or 'TransactionID' column (unique).",
    "starter_code": "-- Find orders for customer 5\n\n",
    "solution_code": "SELECT * FROM orders WHERE customer_id = 5;",
    "expected_output": "order_id | customer_id | amount\n12       | 5           | 200",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1127": {
    "id": 1127,
    "title": "Reading EXPLAIN Output",
    "content": "# üìú Reading EXPLAIN: The Plan\n\n## Asking the Database\n\n`EXPLAIN SELECT ...` tells you *how* the DB will execute the query.\n\n## Key Terms\n\n- **Seq Scan**: Reading every row (Slow for large, fine for small).\n- **Index Scan**: Using the B-Tree.\n- **Cost**: Arbitrary units of effort (e.g., 0.00..45.2). Look for high numbers.\n- **Rows**: Estimated result count.\n\n## Analyzing\n\n1. Run EXPLAIN.\n2. Look for \"Seq Scan\" on large tables.\n3. Check if indexes are being ignored.\n\n---\n\n## üéØ Your Task\n\nRun EXPLAIN on a query and identify if it uses an index.",
    "starter_code": "-- Query that benefits from customer_id index\n\n",
    "solution_code": "SELECT * FROM orders WHERE customer_id = 1;",
    "expected_output": "order_id | customer_id | amount\n1        | 1           | 50",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1128": {
    "id": 1128,
    "title": "Sequential vs Index Scan",
    "content": "# üèéÔ∏è Seq Scan vs Index Scan\n\n## The Tipping Point\n\nThe optimizer is smart.\nIf you select `WHERE ID > 0` (fetching 100% of rows), using an index is SLOWER (jumping around disk). It will choose Seq Scan.\nIf you select `WHERE ID = 5` (fetching 1 row), Index Scan wins.\n\n## Selectivity\n\n- **High Selectivity**: Returns few rows (Index good).\n- **Low Selectivity**: Returns many rows (Seq Scan often better).\n\n## Covering Index\n\nIf the index has ALL columns needed:\n`SELECT Name FROM Users WHERE Name = 'Ali'` (Index on Name).\nThis is an **Index Only Scan** - incredibly fast!\n\n---\n\n## üéØ Your Task\n\nForce a Sequential Scan by writing a query that returns nearly all rows.",
    "starter_code": "-- Find the single order with id = 1\n\n",
    "solution_code": "SELECT * FROM orders WHERE id = 1;",
    "expected_output": "id | amount\n1  | 50",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1129": {
    "id": 1129,
    "title": "Filter Early Principle",
    "content": "# ÔøΩÔøΩÔ∏è Filter Early: Reduce Material\n\n## The Golden Rule of Optimization\n\nReduce the number of rows as early as possible in the execution pipeline.\n\n## Subqueries Example\n\n**Bad**:\n1. Join huge Tables A and B.\n2. Filter the result by Date.\n\n**Good**:\n1. Filter Table A by Date.\n2. Join the small result to B.\n\n## In WHERE clause\n\n`WHERE function(Column) = 'Value'` often kills index usage (Non-Sargable).\n`WHERE Column = reverse_function('Value')` allows index usage.\n\n---\n\n## üéØ Your Task\n\nRewrite a query to filter data *before* performing an expensive calculation.",
    "starter_code": "-- Filter orders before joining\n\n",
    "solution_code": "SELECT c.name, o.amount\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.amount > 100;",
    "expected_output": "name  | amount\nAlice | 150",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1130": {
    "id": 1130,
    "title": "Reducing Rows Before Joins",
    "content": "# ‚úÇÔ∏è Trim Before You Join\n\n## CTE Strategy\n\nUse a CTE to pre-filter rows before joining to a massive fact table.\n\n```sql\nWITH ActiveUsers AS (\n    SELECT ID FROM Users WHERE Status = 'Active'\n)\nSELECT * \nFROM Sales s\nJOIN ActiveUsers u ON s.UserID = u.ID;\n```\nThis is often faster than joining everything and then filtering `WHERE u.Status = 'Active'`, though modern optimizers are getting better at equating them.\n\n## Join Conditions\n\nAdd filters directly in the JOIN ON clause:\n`JOIN Orders o ON u.ID = o.UserID AND o.Date > '2024-01-01'`\n\n---\n\n## üéØ Your Task\n\nOptimize a query by applying filters inside a CTE.",
    "starter_code": "-- Aggregate first, join second\n\n",
    "solution_code": "WITH order_totals AS (\n  SELECT customer_id, SUM(amount) as total\n  FROM orders\n  GROUP BY customer_id\n)\nSELECT c.name, ot.total\nFROM customers c\nJOIN order_totals ot ON c.id = ot.customer_id;",
    "expected_output": "name  | total\nAlice | 500\nBob   | 300",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1131": {
    "id": 1131,
    "title": "Avoiding SELECT *",
    "content": "# üö´ Avoid SELECT *: Be Specific\n\n## Why NOT *?\n\n1. **Network**: Transferring 100 columns when you need 2 is wasteful.\n2. **IO**: The DB has to read columns from disk that you ignore.\n3. **Application Breakage**: If a column schema changes, your code might break.\n4. **Covering Indexes**: `SELECT *` guarantees you can't satisfy the query from index alone.\n\n## The Fix\n\nAlways list your columns.\n`SELECT ID, Name, CreatedAt FROM Users;`\n\n---\n\n## üéØ Your Task\n\nReplace a SELECT * query with specific column selection to improve performance.",
    "starter_code": "-- Select specific columns only\n\n",
    "solution_code": "SELECT name, email FROM customers;",
    "expected_output": "name  | email\nAlice | alice@mail.com",
    "chapter_id": 211,
    "chapter_title": "Performance & Query Plans"
  },
  "1132": {
    "id": 1132,
    "title": "INSERT Basics",
    "content": "# üì• INSERT: Adding Data\n\n## Basic Syntax\n\n```sql\nINSERT INTO key_value_store (key, value)\nVALUES ('setting', 'dark_mode');\n```\n\n## Specifying Columns\n\nALWAYS specify columns.\n**Bad**: `INSERT INTO Users VALUES (1, 'Ali');`\nIf the table adds a column, this breaks.\n\n**Good**: `INSERT INTO Users (ID, Name) VALUES (1, 'Ali');`\n\n## Returning Data (PostgreSQL)\n\n```sql\nINSERT INTO Users (Name) VALUES ('Bob') \nRETURNING ID;\n```\nGets the auto-generated ID back immediately!\n\n---\n\n## üéØ Your Task\n\nInsert a new record into the 'Products' table.",
    "starter_code": "-- Insert a product\n\n",
    "solution_code": "INSERT INTO products (name, price)\nVALUES ('Widget', 25.00);",
    "expected_output": "INSERT 0 1",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1133": {
    "id": 1133,
    "title": "INSERT Multiple Rows",
    "content": "# üì¶ Bulk INSERT: Batching\n\n## Single Query, Many Rows\n\nMuch faster than running 1000 separate INSERT queries.\n\n```sql\nINSERT INTO Users (Name, Email)\nVALUES \n    ('Ali', 'ali@test.com'),\n    ('Bob', 'bob@test.com'),\n    ('Cat', 'cat@test.com');\n```\n\nMake sure not to exceed the packet size limit of the database driver!\n\n---\n\n## üéØ Your Task\n\nInsert three new employees in a single statement.",
    "starter_code": "-- Insert multiple users\n\n",
    "solution_code": "INSERT INTO users (name, email) VALUES\n  ('Alice', 'alice@mail.com'),\n  ('Bob', 'bob@mail.com'),\n  ('Charlie', 'charlie@mail.com');",
    "expected_output": "INSERT 0 3",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1134": {
    "id": 1134,
    "title": "INSERT from SELECT",
    "content": "# üìã INSERT INTO ... SELECT\n\n## Copying Data\n\nYou can populate a table using results from a query.\n\n**Scenario**: Archive old orders.\n\n```sql\nINSERT INTO ArchivedOrders (ID, Date, Total)\nSELECT ID, Date, Total\nFROM Orders\nWHERE Date < '2020-01-01';\n```\n\n## Creating Tables on the Fly\n\n`CREATE TABLE NewTable AS SELECT ...` (CTAS)\nThis creates the table *and* fills it.\n\n---\n\n## üéØ Your Task\n\nCopy 'Out of Stock' products into a 'RestockList' table.",
    "starter_code": "-- Copy high-value orders\n\n",
    "solution_code": "INSERT INTO vip_orders\nSELECT * FROM orders WHERE amount > 1000;",
    "expected_output": "INSERT 0 5",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1135": {
    "id": 1135,
    "title": "UPDATE Basics",
    "content": "# üîÑ UPDATE: Modifying Data\n\n## Syntax\n\n```sql\nUPDATE Users\nSET Status = 'Active'\nWHERE ID = 1;\n```\n\n## The WHERE Clause is Critical!\n\n‚ö†Ô∏è **DANGER**: If you forget ANY WHERE clause:\n`UPDATE Users SET Status = 'Active';`\n**Every user is now Active. You just broke production.**\n\n## Updating Multiple Columns\n\n```sql\nUPDATE Products\nSET Price = 19.99, Stock = 50\nWHERE ID = 101;\n```\n\n---\n\n## üéØ Your Task\n\nUpdate the phone number for a specific customer.",
    "starter_code": "-- Update product price\n\n",
    "solution_code": "UPDATE products\nSET price = 30\nWHERE name = 'Widget';",
    "expected_output": "UPDATE 1",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1136": {
    "id": 1136,
    "title": "UPDATE with Conditions",
    "content": "# üîÄ Conditional UPDATE\n\n## Math in Updates\n\nIncrement a counter:\n```sql\nUPDATE Posts \nSET Views = Views + 1 \nWHERE ID = 5;\n```\n\n## Using Other Columns\n\n```sql\nUPDATE Employees\nSET Salary = Salary * 1.05  -- 5% raise\nWHERE PerformanceRating > 4;\n```\n\n## Update from Another Table (PostgreSQL)\n\n```sql\nUPDATE Products p\nSET Cost = s.NewCost\nFROM SupplierUpdates s\nWHERE p.SupplierID = s.ID;\n```\n\n---\n\n## üéØ Your Task\n\nGive a 10% discount to all products in the 'OldInventory' category.",
    "starter_code": "-- Update pending order status\n\n",
    "solution_code": "UPDATE orders\nSET status = 'processing'\nWHERE status = 'pending';",
    "expected_output": "UPDATE 10",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1137": {
    "id": 1137,
    "title": "DELETE Basics",
    "content": "# ‚ùå DELETE: Removing Data\n\n## Syntax\n\n```sql\nDELETE FROM Users \nWHERE ID = 5;\n```\n\n## The WHERE clause is Critical! (Again)\n\n`DELETE FROM Users;` -> **Empty table.** (Use `TRUNCATE` if you really meant that, it's faster).\n\n## Returning Deleted Data\n\n```sql\nDELETE FROM Queue\nWHERE Status = 'Done'\nRETURNING ID;\n```\n\n---\n\n## üéØ Your Task\n\nDelete a specific order that was cancelled.",
    "starter_code": "-- Delete cancelled orders\n\n",
    "solution_code": "DELETE FROM orders WHERE status = 'cancelled';",
    "expected_output": "DELETE 5",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1138": {
    "id": 1138,
    "title": "DELETE with Conditions",
    "content": "# üßπ Conditional Cleanup\n\n## Deleting Ranges\n\n```sql\nDELETE FROM Logs\nWHERE Date < NOW() - INTERVAL '1 year';\n```\n\n## Delete with Subquery\n\n```sql\nDELETE FROM Users\nWHERE ID IN (\n    SELECT UserID FROM BannedList\n);\n```\n\n## Soft Delete\n\nUsually, we don't actually DELETE. We use an `IsDeleted` flag or `DeletedAt` timestamp.\n`UPDATE Users SET DeletedAt = NOW() WHERE ID = 1;`\nThis allows recovery!\n\n---\n\n## üéØ Your Task\n\nRemove all session logs older than 30 days.",
    "starter_code": "-- Delete orders before 2023\n\n",
    "solution_code": "DELETE FROM orders WHERE order_date < '2023-01-01';",
    "expected_output": "DELETE 100",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1139": {
    "id": 1139,
    "title": "Transaction Basics",
    "content": "# üíº Transactions: All or Nothing\n\n## ACID Properties\n\n- **Atomicity**: All steps happen, or none.\n- **Consistency**: Data stays valid.\n- **Isolation**: Nobody sees half-finished work.\n- **Durability**: Saved data survives crashess.\n\n## Bank Transfer Example\n\n1. Subtract $100 from Alice.\n2. Add $100 to Bob.\n\nIf step 2 fails (server crash), step 1 MUST vanish. Alice shouldn't lose money.\n\n---\n\n## üéØ Your Task\n\nStart a transaction to explore its syntax.",
    "starter_code": "-- Preview data before transaction\n\n",
    "solution_code": "SELECT * FROM orders WHERE status = 'pending';",
    "expected_output": "order_id | status\n1        | pending",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1140": {
    "id": 1140,
    "title": "BEGIN, COMMIT, ROLLBACK",
    "content": "# üö¶ Transaction Commands\n\n## The Flow\n\n1. `BEGIN;` (Start the transaction)\n2. Run SQL commands... (Changes are invisible to others).\n3. `COMMIT;` (Save changes permanently).\n4. `ROLLBACK;` (Undo changes if something went wrong).\n\n## Interactive Example\n\n```sql\nBEGIN;\nUPDATE Account SET Balance = 0 WHERE ID = 1;\n-- Oops! I didn't mean to do that!\nROLLBACK;\n-- Balance is back to normal. Phew.\n```\n\n---\n\n## üéØ Your Task\n\nSimulate a failed update by using ROLLBACK.",
    "starter_code": "-- Check account balances\n\n",
    "solution_code": "SELECT id, balance FROM accounts;",
    "expected_output": "id | balance\n1  | 1000\n2  | 500",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1141": {
    "id": 1141,
    "title": "Savepoints",
    "content": "# üìç Savepoints: Checkpoints\n\n## Partial Rollback\n\nWithin a long transaction, you can set markers.\n\n```sql\nBEGIN;\nINSERT INTO A ...;\nSAVEPOINT my_save;\nINSERT INTO B ...; -- Error!\nROLLBACK TO my_save;\n-- Insert A is kept, Insert B is undone.\nCOMMIT;\n```\n\nUseful for complex scripts where one minor error shouldn't kill the whole job.\n\n---\n\n## üéØ Your Task\n\nCreate a savepoint before a risky operation.",
    "starter_code": "-- Count existing orders\n\n",
    "solution_code": "SELECT COUNT(*) as order_count FROM orders;",
    "expected_output": "order_count\n150",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1142": {
    "id": 1142,
    "title": "Isolation Levels Intro",
    "content": "# üîí Isolation Levels: Who Sees What?\n\n## Dirty Read (Read Uncommitted)\nSeeing uncommitted data (Drafts). Dangerous!\n\n## Read Committed (Default)\nOnly seeing saved data. But if I query twice, data might change in between.\n\n## Repeatable Read\nSnapshot. If I query twice, I see the Exact Same Thing (even if others updated it).\n\n## Serializable\nStrict Queue. Guarantees safety, but slow.\n\n---\n\n## üéØ Your Task\n\nSet the transaction isolation level to SERIALIZABLE.",
    "starter_code": "-- Simple query to test\n\n",
    "solution_code": "SELECT 1 as test;",
    "expected_output": "test\n1",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1143": {
    "id": 1143,
    "title": "Safe Update Patterns",
    "content": "# üõ°Ô∏è Safe Updates: Locking\n\n## Race Conditions\n\nTwo people buy the last ticket at the exact same millisecond. Who gets it?\n\n## FOR UPDATE\n\n```sql\nSELECT * FROM Tickets \nWHERE ID = 1 \nFOR UPDATE;\n```\nThis **locks** the row. No one else can update it until I commit.\n\n## Optimistic Locking\n\nUse a version column.\n`UPDATE ... SET Ver = 2 WHERE ID = 1 AND Ver = 1;`\nIf rows affected = 0, someone else touched it!\n\n---\n\n## üéØ Your Task\n\nUse SELECT ... FOR UPDATE to lock a row before modifying it.",
    "starter_code": "-- Preview products with price < 10\n\n",
    "solution_code": "SELECT * FROM products WHERE price < 10;",
    "expected_output": "id | name   | price\n1  | Widget | 5",
    "chapter_id": 212,
    "chapter_title": "Mutations & Transactions"
  },
  "1144": {
    "id": 1144,
    "title": "View Basics",
    "content": "# üñºÔ∏è Views: The Virtual Table\n\n## What is a View?\n\nA saved query that looks like a table.\nIt doesn't store data (usually), it stores the **definition**.\n\n## Creating a View\n\n```sql\nCREATE VIEW ActiveUsers AS\nSELECT * FROM Users WHERE Status = 'Active';\n```\n\n## Using a View\n\n```sql\nSELECT * FROM ActiveUsers;\n```\n\n## Why use them?\n\n1. **Simplicity**: Hide complex Joins from users.\n2. **Security**: Give users access to the View, but not the underlying Table (e.g., hide Salary column).\n3. **Consistency**: Ensure everyone defines \"Active User\" the same way.\n\n---\n\n## üéØ Your Task\n\nCreate a view that simplifies a complex 3-table join.",
    "starter_code": "-- Query active orders\n\n",
    "solution_code": "SELECT * FROM orders WHERE status = 'active';",
    "expected_output": "order_id | status\n1        | active",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1145": {
    "id": 1145,
    "title": "Updatable Views",
    "content": "# ‚úèÔ∏è Updatable Views\n\n## Can you write to a View?\n\nSometimes! If the view is simple (1 table, no aggregates), SQL maps the write back to the base table.\n\n```sql\nUPDATE ActiveUsers\nSET LastLogin = NOW()\nWHERE ID = 1;\n```\n\n## Restrictions\n\nYou generally CANNOT update views with:\n- `GROUP BY`\n- `DISTINCT`\n- Calculated Fields\n- Multiple Tables (Joins) - usually restricted.\n\n## Utility\n\nUseful for legacy apps. Rename the table, create a view with the old name, and the app still works!\n\n---\n\n## üéØ Your Task\n\nAttempt to update a record through a simple view.",
    "starter_code": "-- Query for daily orders view\n\n",
    "solution_code": "SELECT DATE(order_date) as date, COUNT(*) as orders\nFROM orders\nGROUP BY DATE(order_date);",
    "expected_output": "date       | orders\n2024-03-01 | 15",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1146": {
    "id": 1146,
    "title": "Materialized Views",
    "content": "# üíæ Materialized Views: The Cache\n\n## View vs Materialized View\n\n- **Standard View**: Runs the query Every Time you select from it. (Real-time, potentially slow).\n- **Materialized View**: Runs the query Once and **stores the result** on disk. (Fast, but needs refreshing).\n\n## Syntax\n\n```sql\nCREATE MATERIALIZED VIEW MonthlySales AS\nSELECT ...\n```\n\n## Refreshing\n\nData gets stale! You must refresh it.\n```sql\nREFRESH MATERIALIZED VIEW MonthlySales;\n```\n\n## Use Case\n\nHeavy analytical dashboards where 5-minute old data is acceptable.\n\n---\n\n## üéØ Your Task\n\nCreate a materialized view for an expensive aggregation query.",
    "starter_code": "-- Customer summary suitable for a view\n\n",
    "solution_code": "SELECT c.name, COUNT(o.id) as order_count, SUM(o.amount) as total\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.id, c.name;",
    "expected_output": "name  | order_count | total\nAlice | 5           | 500",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1147": {
    "id": 1147,
    "title": "Materialized Views",
    "content": "# üíæ Materialized Views: Pre-Computed Results\n\n## What is a Materialized View?\n\nA **materialized view** stores the result of a query physically. Unlike regular views, the data is pre-computed and saved, making queries much faster!\n\n## Regular View vs Materialized View\n\n| Feature | Regular View | Materialized View |\n|---------|-------------|-------------------|\n| Storage | No data stored | Data is stored |\n| Query speed | Recomputes each time | Instant (reads saved data) |\n| Freshness | Always current | May be stale |\n| Use case | Simple abstraction | Performance optimization |\n\n## Real-World Analogy\n\n- **Regular View**: Calculating your bank balance every time (always accurate, but slow)\n- **Materialized View**: Your cached balance on the app (fast, but might be slightly old)\n\n## Creating a Materialized View\n\n```sql\nCREATE MATERIALIZED VIEW daily_summary AS\nSELECT \n    date,\n    SUM(sales) as total_sales,\n    COUNT(*) as order_count\nFROM orders\nGROUP BY date;\n```\n\n## Refreshing Data\n\n```sql\n-- Manual refresh\nREFRESH MATERIALIZED VIEW daily_summary;\n\n-- In some databases, can set auto-refresh\n```\n\n## When to Use\n\n‚úÖ Expensive aggregations run frequently\n‚úÖ Data doesn't need to be real-time\n‚úÖ Dashboard queries\n‚ùå Rapidly changing data needing real-time accuracy\n\n---\n\n## üéØ Your Task\n\nCreate a materialized view for monthly sales summary.",
    "starter_code": "-- Create a materialized view for monthly sales\nCREATE MATERIALIZED VIEW monthly_sales_summary AS\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(amount) as total_revenue,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    AVG(amount) as avg_order_value\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date);\n\n-- Query it instantly!\nSELECT * FROM monthly_sales_summary;",
    "solution_code": "CREATE MATERIALIZED VIEW monthly_sales_summary AS\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(amount) as total_revenue,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    AVG(amount) as avg_order_value\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date);\n\nSELECT * FROM monthly_sales_summary;",
    "expected_output": "2024-01|125000|450|277.78\n2024-02|142000|520|273.08",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1148": {
    "id": 1148,
    "title": "Naming Conventions",
    "content": "# üìù SQL Naming Conventions: Consistency Matters\n\n## Why Naming Conventions?\n\nGood naming makes code **readable**, **maintainable**, and **self-documenting**. When you return to code months later, clear names save hours!\n\n## Table Naming Best Practices\n\n| Convention | Example | Why |\n|------------|---------|-----|\n| snake_case | order_items | Consistent, readable |\n| Plural nouns | customers | Tables contain many rows |\n| Prefix by layer | stg_orders | Know the data stage |\n| No abbreviations | customer_id | Clarity over brevity |\n\n## Column Naming\n\n```sql\n-- ‚úÖ Good\ncustomer_id, order_date, total_amount, is_active\n\n-- ‚ùå Bad\nCustID, ord_dt, amt, Active\n```\n\n## Common Prefixes\n\n- **is_** for booleans: `is_active`, `is_deleted`\n- **has_** for booleans: `has_subscription`\n- **_at** for timestamps: `created_at`, `updated_at`\n- **_date** for dates: `order_date`, `birth_date`\n- **_id** for foreign keys: `customer_id`, `product_id`\n\n## Layer Prefixes\n\n| Prefix | Layer | Example |\n|--------|-------|---------|\n| raw_ | Raw source | raw_stripe_payments |\n| stg_ | Staging | stg_payments |\n| int_ | Intermediate | int_payment_joined |\n| dim_ | Dimension | dim_customers |\n| fct_ | Fact | fct_orders |\n\n## Real-World Impact\n\n```sql\n-- Which query would you rather debug at 2am?\nSELECT c.cid, o.amt FROM tbl_c c JOIN o_data o ON c.cid = o.c_id\n\n-- vs\n\nSELECT c.customer_id, o.order_amount \nFROM dim_customers c \nJOIN fct_orders o ON c.customer_id = o.customer_id\n```\n\n---\n\n## üéØ Your Task\n\nRename these poorly-named columns following conventions.",
    "starter_code": "-- Before: Bad naming\nCREATE TABLE tbl_cust (\n    ID int,\n    CustName varchar,\n    email varchar,\n    Active boolean,\n    DtCreated timestamp\n);\n\n-- After: Good naming  \nCREATE TABLE customers (\n    customer_id int,\n    customer_name varchar,\n    email varchar,\n    is_active boolean,\n    created_at timestamp\n);",
    "solution_code": "CREATE TABLE customers (\n    customer_id int,\n    customer_name varchar,\n    email varchar,\n    is_active boolean,\n    created_at timestamp\n);",
    "expected_output": "Table created with proper naming",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1149": {
    "id": 1149,
    "title": "SQL Style Guide",
    "content": "# üìê SQL Style Guide: Writing Beautiful Queries\n\n## Why Style Matters\n\nConsistent style makes SQL **readable**, **reviewable**, and **maintainable**. In a team, everyone should write SQL the same way.\n\n## Capitalization\n\n```sql\n-- ‚úÖ Keywords in UPPERCASE, identifiers in lowercase\nSELECT customer_name, order_amount\nFROM orders\nWHERE status = 'completed'\n\n-- ‚ùå Inconsistent capitalization\nselect Customer_Name, ORDER_AMOUNT\nfrom ORDERS\nWhere Status = 'completed'\n```\n\n## Indentation and Line Breaks\n\n```sql\n-- ‚úÖ Good: Each clause on new line, consistent indentation\nSELECT \n    c.customer_name,\n    o.order_date,\n    o.order_amount\nFROM customers c\nJOIN orders o\n    ON c.customer_id = o.customer_id\nWHERE o.order_date >= '2024-01-01'\n  AND o.status = 'completed'\nORDER BY o.order_date DESC;\n\n-- ‚ùå Bad: Everything on one line\nSELECT c.customer_name, o.order_date, o.order_amount FROM customers c JOIN orders o ON c.customer_id = o.customer_id WHERE o.order_date >= '2024-01-01' AND o.status = 'completed' ORDER BY o.order_date DESC;\n```\n\n## Comma Style\n\n```sql\n-- Leading commas (easier to comment out lines)\nSELECT \n    customer_id\n    ,customer_name\n    ,email\n    ,created_at\n\n-- Trailing commas (more common)\nSELECT \n    customer_id,\n    customer_name,\n    email,\n    created_at\n```\n\n## CTEs and Subqueries\n\n```sql\n-- ‚úÖ Descriptive CTE names\nWITH monthly_revenue AS (\n    SELECT DATE_TRUNC('month', order_date) as month,\n           SUM(amount) as revenue\n    FROM orders\n    GROUP BY 1\n)\nSELECT * FROM monthly_revenue;\n```\n\n---\n\n## üéØ Your Task\n\nReformat this messy query following style guidelines.",
    "starter_code": "-- Before: Messy\nselect c.name,count(*) as cnt,sum(o.amt) as total from customers c join orders o on c.id=o.cust_id where o.status='completed' group by c.name having count(*)>5 order by total desc;\n\n-- After: Clean and styled\nSELECT \n    c.customer_name,\n    COUNT(*) as order_count,\n    SUM(o.amount) as total_amount\nFROM customers c\nJOIN orders o \n    ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\nGROUP BY c.customer_name\nHAVING COUNT(*) > 5\nORDER BY total_amount DESC;",
    "solution_code": "SELECT \n    c.customer_name,\n    COUNT(*) as order_count,\n    SUM(o.amount) as total_amount\nFROM customers c\nJOIN orders o \n    ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\nGROUP BY c.customer_name\nHAVING COUNT(*) > 5\nORDER BY total_amount DESC;",
    "expected_output": "Query formatted successfully",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1150": {
    "id": 1150,
    "title": "Staging Models",
    "content": "# üèóÔ∏è Staging Models: The First Layer of Transformation\n\n## What are Staging Models?\n\n**Staging models** are the first layer in a dbt/analytics engineering project. They sit directly on top of raw source tables and handle initial cleaning.\n\n## The Role of Staging\n\n1. **Rename** confusing column names to clear ones\n2. **Type cast** strings to proper types\n3. **Filter** out test/junk data\n4. **Standardize** naming conventions\n5. **NO business logic** - just cleaning!\n\n## Real-World Analogy\n\nStaging is like the mail room: sort, label, and route packages (data) to the right departments‚Äîbut don't open them or make decisions about their contents.\n\n## Example Staging Model\n\n```sql\n-- stg_customers.sql\nSELECT\n    -- Rename columns\n    id as customer_id,\n    cust_nm as customer_name,\n    \n    -- Type casting\n    CAST(signup_dt as DATE) as signup_date,\n    \n    -- Standardize\n    LOWER(email) as email,\n    UPPER(state_code) as state\n    \nFROM raw.customers\nWHERE is_test = false  -- Filter test data\n```\n\n## Naming Convention\n\n- Prefix with `stg_`\n- Use source table name\n- Example: `stg_stripe_payments`, `stg_salesforce_accounts`\n\n## One Staging Model Per Source\n\nEach raw table gets exactly one staging model. Never join multiple sources in staging!\n\n---\n\n## üéØ Your Task\n\nCreate a staging model for the raw orders table.",
    "starter_code": "-- stg_orders.sql\n-- Clean up the raw orders table\n\nSELECT\n    -- Rename and clean columns\n    order_id,\n    customer_id,\n    \n    -- Cast to proper types\n    CAST(order_dt as DATE) as order_date,\n    CAST(total_amt as DECIMAL(10,2)) as order_amount,\n    \n    -- Standardize status values\n    UPPER(status) as order_status,\n    \n    -- Add metadata\n    CURRENT_TIMESTAMP as _loaded_at\n\nFROM raw.orders\nWHERE is_deleted = false;",
    "solution_code": "SELECT\n    order_id,\n    customer_id,\n    CAST(order_dt as DATE) as order_date,\n    CAST(total_amt as DECIMAL(10,2)) as order_amount,\n    UPPER(status) as order_status,\n    CURRENT_TIMESTAMP as _loaded_at\nFROM raw.orders\nWHERE is_deleted = false;",
    "expected_output": "Staging model created successfully",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1151": {
    "id": 1151,
    "title": "Intermediate Models",
    "content": "# üîß Intermediate Models: Building Blocks of Analysis\n\n## What are Intermediate Models?\n\n**Intermediate models** (or \"int\" models) are the middle layer where you:\n- Join data from multiple staging models\n- Apply business logic\n- Create reusable building blocks\n\n## The Transformation Pipeline\n\n```\nRaw Sources ‚Üí Staging ‚Üí Intermediate ‚Üí Marts\n     ‚Üì           ‚Üì           ‚Üì           ‚Üì\n  \"raw\"       \"stg_\"      \"int_\"      \"dim_/fct_\"\n```\n\n## What Happens in Intermediate?\n\n1. **Join** related staging models\n2. **Apply business logic** (calculations, flags)\n3. **Create reusable objects** used by multiple marts\n4. **Complex transformations** that are too big for marts\n\n## Example Intermediate Model\n\n```sql\n-- int_orders_with_customers.sql\nSELECT\n    o.order_id,\n    o.order_date,\n    o.order_amount,\n    c.customer_name,\n    c.customer_segment,\n    -- Business logic: calculate customer lifetime value\n    SUM(o.order_amount) OVER (PARTITION BY o.customer_id) as customer_ltv\nFROM stg_orders o\nJOIN stg_customers c ON o.customer_id = c.customer_id\n```\n\n## Naming Convention\n\n- Prefix with `int_`\n- Descriptive name of what it contains\n- Example: `int_orders_with_products`, `int_customer_order_history`\n\n---\n\n## üéØ Your Task\n\nCreate an intermediate model joining orders with customer data.",
    "starter_code": "-- int_orders_enriched.sql\n-- Join orders with customer information\n\nSELECT\n    o.order_id,\n    o.order_date,\n    o.order_amount,\n    o.order_status,\n    \n    -- Customer info\n    c.customer_name,\n    c.email,\n    c.signup_date,\n    \n    -- Business logic: days since signup\n    o.order_date - c.signup_date as days_as_customer,\n    \n    -- Customer order rank\n    ROW_NUMBER() OVER (PARTITION BY o.customer_id ORDER BY o.order_date) as customer_order_number\n\nFROM stg_orders o\nLEFT JOIN stg_customers c ON o.customer_id = c.customer_id;",
    "solution_code": "SELECT\n    o.order_id,\n    o.order_date,\n    o.order_amount,\n    o.order_status,\n    c.customer_name,\n    c.email,\n    c.signup_date,\n    o.order_date - c.signup_date as days_as_customer,\n    ROW_NUMBER() OVER (PARTITION BY o.customer_id ORDER BY o.order_date) as customer_order_number\nFROM stg_orders o\nLEFT JOIN stg_customers c ON o.customer_id = c.customer_id;",
    "expected_output": "Intermediate model created",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1152": {
    "id": 1152,
    "title": "Marts Layer",
    "content": "# üè™ Marts Layer: Business-Ready Data\n\n## What are Marts?\n\n**Marts** (or \"data marts\") are the final output tables that business users query. They're organized by business domain and ready for dashboards!\n\n## Types of Mart Tables\n\n### Dimension Tables (dim_)\nDescriptive data about entities:\n- `dim_customers` - customer attributes\n- `dim_products` - product catalog\n- `dim_dates` - date dimension\n\n### Fact Tables (fct_)\nMeasurable events/transactions:\n- `fct_orders` - order transactions\n- `fct_page_views` - website events\n- `fct_payments` - payment records\n\n## Mart Organization\n\nOrganize marts by business domain:\n```\nmarts/\n‚îú‚îÄ‚îÄ core/          # Shared across domains\n‚îÇ   ‚îú‚îÄ‚îÄ dim_customers.sql\n‚îÇ   ‚îî‚îÄ‚îÄ dim_dates.sql\n‚îú‚îÄ‚îÄ sales/         # Sales team\n‚îÇ   ‚îî‚îÄ‚îÄ fct_orders.sql\n‚îî‚îÄ‚îÄ marketing/     # Marketing team\n    ‚îî‚îÄ‚îÄ fct_campaigns.sql\n```\n\n## What Makes a Good Mart?\n\n‚úÖ **Self-contained**: Can be queried without joins\n‚úÖ **Well-documented**: Column descriptions\n‚úÖ **Performant**: Optimized for common queries\n‚úÖ **Business-friendly**: Uses clear naming\n\n## Example Mart\n\n```sql\n-- fct_orders.sql\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    order_amount,\n    order_status,\n    -- Pre-calculated metrics\n    SUM(order_amount) OVER (PARTITION BY customer_id) as customer_total_revenue\nFROM int_orders_enriched\n```\n\n---\n\n## üéØ Your Task\n\nCreate a fact table for orders with pre-calculated metrics.",
    "starter_code": "-- fct_orders.sql\n-- Final orders mart ready for business users\n\nSELECT\n    -- Keys\n    order_id,\n    customer_id,\n    product_id,\n    \n    -- Dates\n    order_date,\n    DATE_TRUNC('month', order_date) as order_month,\n    \n    -- Measures\n    quantity,\n    unit_price,\n    quantity * unit_price as line_total,\n    \n    -- Status\n    order_status,\n    CASE WHEN order_status = 'DELIVERED' THEN true ELSE false END as is_completed\n\nFROM int_orders_enriched;",
    "solution_code": "SELECT\n    order_id,\n    customer_id,\n    product_id,\n    order_date,\n    DATE_TRUNC('month', order_date) as order_month,\n    quantity,\n    unit_price,\n    quantity * unit_price as line_total,\n    order_status,\n    CASE WHEN order_status = 'DELIVERED' THEN true ELSE false END as is_completed\nFROM int_orders_enriched;",
    "expected_output": "Fact table created",
    "chapter_id": 213,
    "chapter_title": "Analytics Engineering"
  },
  "1153": {
    "id": 1153,
    "title": "Cloud vs Traditional DB",
    "content": "# ‚òÅÔ∏è Cloud vs Traditional Databases\n\n## The Fundamental Difference\n\n| Feature | Traditional DB | Cloud Warehouse |\n|---------|---------------|-----------------|\n| Scaling | Buy bigger server | Add more compute |\n| Cost | Fixed hardware | Pay per use |\n| Storage | Limited | Virtually unlimited |\n| Concurrency | Limited users | Many users |\n\n## Separation of Storage and Compute\n\nCloud warehouses separate storage and compute:\n- **Storage**: Your data sits in cheap cloud storage (S3, GCS)\n- **Compute**: Processing power scales up/down as needed\n\n## Real-World Analogy\n\n- **Traditional DB**: Owning a restaurant kitchen (fixed capacity)\n- **Cloud Warehouse**: Renting commercial kitchens by the hour (scale on demand)\n\n## Key Cloud Warehouse Features\n\n### 1. Elastic Scaling\n```\nNeed to run a big query? Scale up temporarily.\nQuery done? Scale back down.\nPay only for what you use!\n```\n\n### 2. Warehouse/Compute Clusters\n```sql\n-- Snowflake: Multiple warehouses for different teams\n-- BigQuery: Slots for compute allocation\n-- Redshift: Serverless or provisioned clusters\n```\n\n### 3. Results Caching\nMany cloud warehouses cache query results:\n```sql\n-- First run: 30 seconds, $0.50\n-- Second run: 0.5 seconds, $0 (cached!)\n```\n\n## Popular Cloud Warehouses\n\n| Name | Cloud Provider | Strength |\n|------|---------------|----------|\n| BigQuery | Google Cloud | Serverless, ML built-in |\n| Snowflake | Multi-cloud | Sharing, governance |\n| Redshift | AWS | AWS integration |\n| Databricks | Multi-cloud | Lakehouse, notebooks |\n\n---\n\n## üéØ Your Task\n\nCompare query performance concepts in cloud vs traditional.",
    "starter_code": "-- Cloud warehouse advantage: parallel processing\n-- Each node processes part of the data simultaneously\n\n-- Example: 10 billion row table\n-- Traditional: 1 server processes all rows sequentially\n-- Cloud: 100 nodes each process 100M rows in parallel\n\n-- Pseudo-code showing the concept\n/*\nTraditional (sequential):\n  for row in all_rows:\n    process(row)\n  // Time: 10 hours\n\nCloud (parallel):\n  parallel_for row_chunk in distribute(all_rows, 100):\n    process(row_chunk)\n  // Time: 6 minutes\n*/",
    "solution_code": "-- Cloud parallel processing example\nSELECT COUNT(*), SUM(amount)\nFROM billion_row_table\nWHERE date = '2024-01-15';\n-- Processed in seconds via parallel compute",
    "expected_output": "Cloud processing demonstrated",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1154": {
    "id": 1154,
    "title": "QUALIFY Clause",
    "content": "# üéØ QUALIFY: Filter Window Function Results\n\n## What is QUALIFY?\n\n**QUALIFY** is a powerful clause (in Snowflake, Databricks, and others) that filters the results of window functions‚Äîlike WHERE, but for window calculations!\n\n## The Problem\n\nWithout QUALIFY, filtering on window functions is tedious:\n```sql\n-- Subquery required\nSELECT * FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rn\n    FROM employees\n) \nWHERE rn = 1;\n```\n\n## The QUALIFY Solution\n\n```sql\n-- Clean and simple!\nSELECT *\nFROM employees\nQUALIFY ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) = 1;\n```\n\n## Execution Order\n\n```\nFROM ‚Üí WHERE ‚Üí GROUP BY ‚Üí HAVING ‚Üí SELECT ‚Üí QUALIFY ‚Üí ORDER BY\n```\n\nQUALIFY runs AFTER the window function is calculated.\n\n## Common Use Cases\n\n### Top N Per Group\n```sql\nSELECT *\nFROM orders\nQUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) <= 3;\n```\n\n### Deduplication\n```sql\nSELECT *\nFROM raw_events\nQUALIFY ROW_NUMBER() OVER (PARTITION BY event_id ORDER BY loaded_at DESC) = 1;\n```\n\n### Running Total Filter\n```sql\nSELECT *\nFROM transactions\nQUALIFY SUM(amount) OVER (ORDER BY date) >= 1000;\n```\n\n## Database Support\n\n| Database | QUALIFY Support |\n|----------|----------------|\n| Snowflake | ‚úÖ |\n| Databricks | ‚úÖ |\n| BigQuery | ‚úÖ (recent) |\n| PostgreSQL | ‚ùå (use subquery) |\n| MySQL | ‚ùå (use subquery) |\n\n---\n\n## üéØ Your Task\n\nUse QUALIFY to find the highest salary per department.",
    "starter_code": "-- Find top earner in each department using QUALIFY\nSELECT \n    employee_name,\n    department,\n    salary\nFROM employees\nQUALIFY ROW_NUMBER() OVER (\n    PARTITION BY department \n    ORDER BY salary DESC\n) = 1;",
    "solution_code": "SELECT \n    employee_name,\n    department,\n    salary\nFROM employees\nQUALIFY ROW_NUMBER() OVER (\n    PARTITION BY department \n    ORDER BY salary DESC\n) = 1;",
    "expected_output": "Alice|Engineering|120000\nBob|Sales|95000",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1155": {
    "id": 1155,
    "title": "QUALIFY with ROW_NUMBER",
    "content": "# üî¢ QUALIFY with ROW_NUMBER: Powerful Deduplication\n\n## The Deduplication Problem\n\nRaw data often has duplicates. You need to keep just one version of each record‚Äîusually the most recent or highest quality.\n\n## Traditional Approach (Messy)\n\n```sql\n-- Requires subquery\nSELECT * FROM (\n    SELECT *,\n           ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY updated_at DESC) as rn\n    FROM raw_users\n)\nWHERE rn = 1;\n```\n\n## QUALIFY Approach (Clean)\n\n```sql\nSELECT *\nFROM raw_users\nQUALIFY ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY updated_at DESC) = 1;\n```\n\n## Common Patterns\n\n### Keep Latest Version\n```sql\nSELECT *\nFROM events\nQUALIFY ROW_NUMBER() OVER (PARTITION BY event_id ORDER BY loaded_at DESC) = 1;\n```\n\n### Keep First Occurrence\n```sql\nSELECT *\nFROM purchases  \nQUALIFY ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY purchase_date ASC) = 1;\n```\n\n### Keep Top N per Group\n```sql\nSELECT *\nFROM orders\nQUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_amount DESC) <= 3;\n```\n\n### Keep Highest Priority\n```sql\nSELECT *\nFROM raw_data\nQUALIFY ROW_NUMBER() OVER (\n    PARTITION BY id \n    ORDER BY \n        CASE source WHEN 'primary' THEN 1 WHEN 'secondary' THEN 2 ELSE 3 END\n) = 1;\n```\n\n## Performance Benefits\n\nQUALIFY is not only cleaner but often performs better because the database can optimize the window function and filter together.\n\n---\n\n## üéØ Your Task\n\nDeduplicate raw orders keeping the latest update per order.",
    "starter_code": "-- Deduplicate raw_orders keeping latest version per order_id\nSELECT\n    order_id,\n    customer_id,\n    order_amount,\n    order_status,\n    updated_at\nFROM raw_orders\nQUALIFY ROW_NUMBER() OVER (\n    PARTITION BY order_id \n    ORDER BY updated_at DESC\n) = 1;",
    "solution_code": "SELECT\n    order_id,\n    customer_id,\n    order_amount,\n    order_status,\n    updated_at\nFROM raw_orders\nQUALIFY ROW_NUMBER() OVER (\n    PARTITION BY order_id \n    ORDER BY updated_at DESC\n) = 1;",
    "expected_output": "Deduplicated results",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1156": {
    "id": 1156,
    "title": "Table Partitioning",
    "content": "# üìä Table Partitioning: Divide and Conquer\n\n## What is Partitioning?\n\n**Partitioning** divides a large table into smaller, more manageable pieces based on column values. Queries only scan relevant partitions!\n\n## Real-World Analogy\n\nInstead of searching through one giant filing cabinet, partition your files by year‚Äîthen when you need 2024 data, you only search the 2024 drawer!\n\n## How It Works\n\n```sql\n-- Query without partitioning: scans ALL data\nSELECT * FROM orders WHERE order_date = '2024-01-15';\n-- Scans: 10 billion rows üò±\n\n-- Query with date partitioning: scans ONE partition\nSELECT * FROM orders WHERE order_date = '2024-01-15';\n-- Scans: 27 million rows (just January 2024) ‚úÖ\n```\n\n## Creating Partitioned Tables\n\n### BigQuery\n```sql\nCREATE TABLE orders\nPARTITION BY DATE(order_date)\nAS SELECT * FROM raw_orders;\n```\n\n### Snowflake\n```sql\nCREATE TABLE orders\nCLUSTER BY (order_date)  -- Similar concept\nAS SELECT * FROM raw_orders;\n```\n\n## Partition Strategies\n\n| Strategy | Use When |\n|----------|----------|\n| Date/Time | Time-series data, most common |\n| Integer Range | ID ranges |\n| List | Known categories (country, status) |\n\n## Best Practices\n\n‚úÖ Partition on columns used in WHERE clauses\n‚úÖ Avoid too many small partitions\n‚úÖ Consider partition pruning in query design\n‚ùå Don't partition if table is small (<1GB)\n\n---\n\n## üéØ Your Task\n\nCreate a partitioned table for web events.",
    "starter_code": "-- Create date-partitioned events table (BigQuery syntax)\nCREATE TABLE analytics.events\nPARTITION BY DATE(event_timestamp)\nOPTIONS (\n    partition_expiration_days = 365\n)\nAS\nSELECT\n    event_id,\n    user_id,\n    event_type,\n    event_timestamp,\n    page_url\nFROM raw.events;\n\n-- Query only scans relevant date partitions\nSELECT event_type, COUNT(*)\nFROM analytics.events\nWHERE DATE(event_timestamp) = '2024-01-15'\nGROUP BY event_type;",
    "solution_code": "CREATE TABLE analytics.events\nPARTITION BY DATE(event_timestamp)\nAS SELECT * FROM raw.events;\n\nSELECT event_type, COUNT(*)\nFROM analytics.events\nWHERE DATE(event_timestamp) = '2024-01-15'\nGROUP BY event_type;",
    "expected_output": "Partitioned table created",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1157": {
    "id": 1157,
    "title": "Clustering Keys",
    "content": "# üóÇÔ∏è Clustering Keys: Organizing Data Within Partitions\n\n## What is Clustering?\n\n**Clustering** organizes data within partitions by sorting and grouping related rows together. This optimizes queries filtering on clustered columns.\n\n## Partitioning vs Clustering\n\n| Feature | Partitioning | Clustering |\n|---------|-------------|------------|\n| Divides | Into separate partitions | Sorts within partitions |\n| Columns | Usually 1 (often date) | Up to 4 columns |\n| When to use | Date filters | Other frequent filters |\n\n## How It Works\n\nWithout clustering:\n```\nData stored randomly: [user_5, user_3, user_1, user_5, user_2, user_3...]\n```\n\nWith clustering on user_id:\n```\nData sorted together: [user_1, user_1, user_2, user_2, user_3, user_3...]\n```\n\n## Creating Clustered Tables\n\n### BigQuery\n```sql\nCREATE TABLE orders\nPARTITION BY DATE(order_date)\nCLUSTER BY customer_id, product_id\nAS SELECT * FROM raw_orders;\n```\n\n### Snowflake\n```sql\nCREATE TABLE orders\nCLUSTER BY (region, customer_id);\n```\n\n## When to Use Clustering\n\n‚úÖ Columns frequently used in WHERE clauses\n‚úÖ Columns used in JOINs\n‚úÖ High-cardinality columns (many unique values)\n‚úÖ Columns used in aggregations (GROUP BY)\n\n## Best Practices\n\n- Order matters! Put most important filter column first\n- Use with partitioning for best results\n- Monitor cluster efficiency over time\n\n---\n\n## üéØ Your Task\n\nCreate a clustered table for order analytics.",
    "starter_code": "-- Create partitioned AND clustered table\nCREATE TABLE analytics.orders\nPARTITION BY DATE(order_date)\nCLUSTER BY customer_id, region, product_category\nAS\nSELECT\n    order_id,\n    customer_id,\n    region,\n    product_category,\n    order_date,\n    order_amount\nFROM raw.orders;\n\n-- This query benefits from both partition pruning AND clustering\nSELECT customer_id, SUM(order_amount) as total\nFROM analytics.orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'\n  AND region = 'US-WEST'\nGROUP BY customer_id;",
    "solution_code": "CREATE TABLE analytics.orders\nPARTITION BY DATE(order_date)\nCLUSTER BY customer_id, region, product_category\nAS SELECT * FROM raw.orders;\n\nSELECT customer_id, SUM(order_amount)\nFROM analytics.orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'\n  AND region = 'US-WEST'\nGROUP BY customer_id;",
    "expected_output": "Clustered table created",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1158": {
    "id": 1158,
    "title": "Cost-Based Thinking",
    "content": "# üí∞ Cost-Based Thinking: Money and Performance\n\n## Cloud Warehouse Pricing\n\nIn cloud data warehouses (BigQuery, Snowflake, Redshift), you typically pay for:\n- **Compute**: Time spent processing queries\n- **Storage**: Data stored in tables\n- **Data scanned**: Amount of data read\n\n## The Cost Equation\n\n```\nCost = Data Scanned √ó Price per TB\n```\n\nFor example, in BigQuery:\n- On-demand: ~$5 per TB scanned\n- 1 TB table fully scanned = $5 per query!\n\n## Reducing Costs\n\n### 1. Only SELECT Columns You Need\n```sql\n-- ‚ùå Expensive: scans all columns\nSELECT * FROM large_table;\n\n-- ‚úÖ Cheap: scans only needed columns\nSELECT customer_id, order_amount FROM large_table;\n```\n\n### 2. Use Partitions\n```sql\n-- ‚ùå Scans entire table\nSELECT * FROM events WHERE event_type = 'purchase';\n\n-- ‚úÖ Scans only 1 day of partitions\nSELECT * FROM events \nWHERE event_date = '2024-01-15' \n  AND event_type = 'purchase';\n```\n\n### 3. Preview Before Running\n```sql\n-- BigQuery: Dry run shows bytes scanned\n-- Snowflake: Query history shows costs\n```\n\n## Monitoring Costs\n\n- Set up query budgets\n- Review expensive queries weekly\n- Train team on cost-conscious queries\n\n---\n\n## üéØ Your Task\n\nOptimize this expensive query for lower costs.",
    "starter_code": "-- ‚ùå Expensive query (scans everything)\nSELECT * \nFROM events \nWHERE user_id = 12345;\n\n-- ‚úÖ Optimized query\nSELECT \n    event_id,\n    event_type,\n    event_timestamp\nFROM events \nWHERE event_date = CURRENT_DATE  -- Partition pruning\n  AND user_id = 12345;  -- Then filter",
    "solution_code": "SELECT \n    event_id,\n    event_type,\n    event_timestamp\nFROM events \nWHERE event_date = CURRENT_DATE\n  AND user_id = 12345;",
    "expected_output": "Query optimized for cost",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1159": {
    "id": 1159,
    "title": "Scanning Fewer Columns",
    "content": "# üìâ Scanning Fewer Columns: Column Pruning\n\n## The Problem\n\nIn columnar databases (BigQuery, Snowflake, Redshift), data is stored by column. `SELECT *` forces the engine to read EVERY column!\n\n## Cost Impact\n\n```sql\n-- Wide table: 100 columns, 10 billion rows\n\nSELECT *  -- Reads all 100 columns = $$$$\nFROM huge_table;\n\nSELECT customer_id, order_amount  -- Reads 2 columns = $\nFROM huge_table;\n```\n\n## Real-World Example\n\n| Query | Columns Read | Data Scanned | Cost |\n|-------|-------------|--------------|------|\n| SELECT * | 100 | 500 GB | $2.50 |\n| SELECT (5 cols) | 5 | 25 GB | $0.13 |\n\nThat's **95% savings** just by being specific!\n\n## Best Practices\n\n### ‚úÖ Always name your columns\n```sql\nSELECT customer_id, order_date, order_amount\nFROM orders;\n```\n\n### ‚ùå Never use SELECT * in production\n```sql\nSELECT * FROM orders;  -- Don't do this!\n```\n\n### ‚úÖ Especially in subqueries and CTEs\n```sql\nWITH order_summary AS (\n    SELECT customer_id, SUM(amount) as total  -- Only what's needed\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT * FROM order_summary;  -- OK here - CTE is already narrow\n```\n\n## Exceptions\n\n- **Exploring data**: SELECT * is fine in development\n- **Very narrow tables**: If table has 3-5 columns\n- **Exporting full rows**: When you truly need everything\n\n---\n\n## üéØ Your Task\n\nOptimize this query by selecting only needed columns.",
    "starter_code": "-- ‚ùå Before: Scans all columns\nSELECT *\nFROM customer_transactions\nWHERE transaction_date = '2024-01-15';\n\n-- ‚úÖ After: Only needed columns\nSELECT \n    transaction_id,\n    customer_id,\n    amount,\n    transaction_type\nFROM customer_transactions\nWHERE transaction_date = '2024-01-15';",
    "solution_code": "SELECT \n    transaction_id,\n    customer_id,\n    amount,\n    transaction_type\nFROM customer_transactions\nWHERE transaction_date = '2024-01-15';",
    "expected_output": "Query optimized",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1160": {
    "id": 1160,
    "title": "Scanning Fewer Rows",
    "content": "# üìä Scanning Fewer Rows: Partition Pruning\n\n## The Core Concept\n\n**Partition pruning** is when the query engine skips entire partitions of data that don't match your filters. This can reduce scans from billions to millions of rows!\n\n## How Partitioning Helps\n\n```\nTable: events (partitioned by date)\n‚îú‚îÄ‚îÄ 2024-01-01/  (100M rows)\n‚îú‚îÄ‚îÄ 2024-01-02/  (100M rows)\n‚îú‚îÄ‚îÄ 2024-01-03/  (100M rows)\n‚îî‚îÄ‚îÄ ... (365 days = 36.5B rows)\n\nQuery: WHERE event_date = '2024-01-15'\nResult: Only scans 100M rows (0.3% of table!)\n```\n\n## Writing Partition-Pruning Queries\n\n### ‚úÖ Good: Filter on partition column\n```sql\nSELECT *\nFROM events\nWHERE event_date = '2024-01-15';  -- Scans 1 partition\n```\n\n### ‚ùå Bad: Function on partition column\n```sql\nSELECT *\nFROM events\nWHERE YEAR(event_date) = 2024;  -- Scans ALL partitions!\n```\n\n### ‚úÖ Fixed: Use range instead\n```sql\nSELECT *\nFROM events\nWHERE event_date >= '2024-01-01' \n  AND event_date < '2025-01-01';  -- Prunes correctly\n```\n\n## Common Mistakes\n\n| Query Pattern | Pruning? | Why |\n|--------------|----------|-----|\n| WHERE date = '2024-01-15' | ‚úÖ Yes | Direct comparison |\n| WHERE date BETWEEN dates | ‚úÖ Yes | Range comparison |\n| WHERE EXTRACT(YEAR FROM date) = 2024 | ‚ùå No | Function on column |\n| WHERE date_string LIKE '2024%' | ‚ùå No | String pattern |\n\n## Verify Pruning\n\n```sql\n-- BigQuery\nSELECT total_bytes_processed FROM query_plan;\n\n-- Snowflake  \nSELECT partitions_scanned FROM query_history;\n```\n\n---\n\n## üéØ Your Task\n\nRewrite this query to enable partition pruning.",
    "starter_code": "-- ‚ùå Before: No partition pruning (function on partition column)\nSELECT customer_id, SUM(amount)\nFROM transactions\nWHERE MONTH(transaction_date) = 1 AND YEAR(transaction_date) = 2024\nGROUP BY customer_id;\n\n-- ‚úÖ After: Partition pruning enabled\nSELECT customer_id, SUM(amount)\nFROM transactions\nWHERE transaction_date >= '2024-01-01' \n  AND transaction_date < '2024-02-01'\nGROUP BY customer_id;",
    "solution_code": "SELECT customer_id, SUM(amount)\nFROM transactions\nWHERE transaction_date >= '2024-01-01' \n  AND transaction_date < '2024-02-01'\nGROUP BY customer_id;",
    "expected_output": "Query optimized for partition pruning",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "1161": {
    "id": 1161,
    "title": "Query Optimization Tips",
    "content": "# ‚ö° Query Optimization: Best Practices\n\n## The Optimization Checklist\n\nBefore running any expensive query, run through this checklist:\n\n### 1. Column Selection\n```sql\n-- ‚ùå SELECT *\n-- ‚úÖ Select only needed columns\nSELECT customer_id, order_date, amount FROM orders;\n```\n\n### 2. Partition Filters\n```sql\n-- ‚ùå WHERE YEAR(date) = 2024\n-- ‚úÖ Use direct date ranges\nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01'\n```\n\n### 3. Predicate Pushdown\nPut filters as early as possible:\n```sql\n-- ‚ùå Filter after join\nSELECT * FROM a JOIN b ON a.id = b.id WHERE a.status = 'active';\n\n-- ‚úÖ Filter before join (if possible)\nSELECT * FROM (SELECT * FROM a WHERE status = 'active') a \nJOIN b ON a.id = b.id;\n```\n\n### 4. Avoid DISTINCT When Possible\n```sql\n-- ‚ùå Expensive\nSELECT DISTINCT customer_id FROM orders;\n\n-- ‚úÖ If grouping anyway\nSELECT customer_id FROM orders GROUP BY customer_id;\n```\n\n### 5. Use Approximate Functions\n```sql\n-- ‚ùå Exact count (slow)\nSELECT COUNT(DISTINCT user_id) FROM events;\n\n-- ‚úÖ Approximate count (fast, ~2% error)\nSELECT APPROX_COUNT_DISTINCT(user_id) FROM events;\n```\n\n### 6. Limit Early in Development\n```sql\n-- Always limit when exploring\nSELECT * FROM huge_table LIMIT 100;\n```\n\n## Performance Analysis\n\n```sql\n-- View query plan\nEXPLAIN SELECT ...\n\n-- View execution stats\nEXPLAIN ANALYZE SELECT ...\n```\n\n---\n\n## üéØ Your Task\n\nOptimize this inefficient query using multiple techniques.",
    "starter_code": "-- ‚ùå Before: Multiple inefficiencies\nSELECT DISTINCT *\nFROM customer_orders\nWHERE EXTRACT(YEAR FROM order_date) = 2024\n  AND EXTRACT(MONTH FROM order_date) = 1;\n\n-- ‚úÖ After: Optimized\nSELECT \n    order_id,\n    customer_id,\n    order_amount,\n    order_date\nFROM customer_orders\nWHERE order_date >= '2024-01-01' \n  AND order_date < '2024-02-01'\nGROUP BY 1, 2, 3, 4;",
    "solution_code": "SELECT \n    order_id,\n    customer_id,\n    order_amount,\n    order_date\nFROM customer_orders\nWHERE order_date >= '2024-01-01' \n  AND order_date < '2024-02-01';",
    "expected_output": "Query fully optimized",
    "chapter_id": 214,
    "chapter_title": "Cloud Warehouse Features"
  },
  "145": {
    "title": "Variable Swapping Challenge",
    "chapter_title": "Variables",
    "content": "# üîÑ Variable Swapping: Exchange Values\n\n## The Problem\n\nYou have two variables and need to exchange their values:\n```python\na = 10\nb = 20\n# After swap: a = 20, b = 10\n```\n\n## The Classic Way (Temporary Variable)\n\n```python\na = 10\nb = 20\n\ntemp = a    # Save a in temp\na = b       # Overwrite a with b\nb = temp    # Put original a (from temp) into b\n\nprint(a, b)  # 20, 10\n```\n\n## The Pythonic Way (Tuple Unpacking)\n\nPython has a beautiful one-liner:\n\n```python\na, b = b, a\n```\n\nHow it works:\n1. Right side creates a tuple: `(b, a)` ‚Üí `(20, 10)`\n2. Left side unpacks it: `a = 20, b = 10`\n\n## Why Swapping Matters\n\nSwapping is used in many algorithms:\n- **Sorting**: Swap elements to reorder\n- **Reversing**: Swap from ends toward middle\n- **Shuffling**: Random swaps\n\n## Multiple Variable Swap\n\n```python\n# Rotate three variables\na, b, c = b, c, a\n\n# Swap multiple pairs\nx1, y1, x2, y2 = x2, y2, x1, y1\n```\n\n## Common Mistake\n\n```python\n# WRONG - this doesn't swap!\na = b\nb = a  # Both are now the same value!\n```\n\n---\n\n## üéØ Your Task\n\nPractice swapping variables using different techniques.",
    "starter_code": "# Starting values\na = 10\nb = 20\nprint(f\"Before swap: a = {a}, b = {b}\")\n\n# Method 1: Tuple unpacking (Pythonic way)\na, b = b, a\nprint(f\"After swap:  a = {a}, b = {b}\")\n\n# Reset and try the temp variable method\nx = 100\ny = 200\nprint(f\"\\nBefore swap: x = {x}, y = {y}\")\n\ntemp = x\nx = y\ny = temp\nprint(f\"After swap:  x = {x}, y = {y}\")\n\n# Bonus: Rotate three variables\np, q, r = 1, 2, 3\nprint(f\"\\nBefore rotate: p={p}, q={q}, r={r}\")\np, q, r = q, r, p\nprint(f\"After rotate:  p={p}, q={q}, r={r}\")",
    "solution_code": "a, b = 10, 20\nprint(f\"Before: a={a}, b={b}\")\na, b = b, a\nprint(f\"After:  a={a}, b={b}\")",
    "expected_output": "Before swap: a = 10, b = 20\nAfter swap:  a = 20, b = 10"
  },
  "146": {
    "title": "Data Cleaning with Strings",
    "chapter_title": "Strings",
    "content": "# üßπ String Cleaning: Prepare Data for Analysis\n\n## Why Clean Strings?\n\nReal-world text data is messy:\n- Extra spaces: \"  John Smith  \"\n- Inconsistent case: \"new YORK\", \"New york\"\n- Unwanted characters: \"Price: $99.99\"\n\n## Essential Cleaning Methods\n\n### Strip Whitespace\n```python\nname = \"  Alice  \"\nname.strip()   # \"Alice\"\nname.lstrip()  # \"Alice  \" (left only)\nname.rstrip()  # \"  Alice\" (right only)\n```\n\n### Case Normalization\n```python\ncity = \"nEW yOrK\"\ncity.lower()   # \"new york\"\ncity.upper()   # \"NEW YORK\"\ncity.title()   # \"New York\"\n```\n\n### Remove Characters with replace()\n```python\nprice = \"$99.99\"\nprice.replace(\"$\", \"\")  # \"99.99\"\n\nphone = \"(555) 123-4567\"\nphone.replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\" \", \"\")\n# \"5551234567\"\n```\n\n## Chaining Methods\n\nYou can chain multiple method calls together:\n\n```python\nmessy = \"  HELLO WORLD  \"\nclean = messy.strip().lower()\nprint(clean)  # \"hello world\"\n```\n\n---\n\n## üéØ Your Task\n\nClean up some messy customer data using the methods above:\n1. Strip whitespace from the name\n2. Convert the name to Title Case\n3. Convert the email to lowercase\n4. Remove the $ from the price",
    "starter_code": "# Messy customer data\nname = \"  ALICE SMITH  \"\nemail = \" Alice@Example.COM  \"\nprice = \"$29.99\"\n\n# Step 1: Clean the name (strip whitespace, then title case)\nclean_name = name.strip().title()\n\n# Step 2: Clean the email (strip whitespace, then lowercase)\nclean_email = email.strip().lower()\n\n# Step 3: Clean the price (remove the $ symbol)\nclean_price = price.replace(\"$\", \"\")\n\n# Print the cleaned data\nprint(f\"Name: {clean_name}\")\nprint(f\"Email: {clean_email}\")\nprint(f\"Price: {clean_price}\")",
    "solution_code": "name = \"  ALICE SMITH  \"\nemail = \" Alice@Example.COM  \"\nprice = \"$29.99\"\n\nclean_name = name.strip().title()\nclean_email = email.strip().lower()\nclean_price = price.replace(\"$\", \"\")\n\nprint(f\"Name: {clean_name}\")\nprint(f\"Email: {clean_email}\")\nprint(f\"Price: {clean_price}\")",
    "expected_output": "Name: Alice Smith\nEmail: alice@example.com\nPrice: 29.99"
  },
  "147": {
    "title": "Calculating Statistics",
    "chapter_title": "Variables, Types & Memory",
    "content": "# üìä Calculating Statistics: Your First Data Analysis!\n\n## Welcome to Real Data Science\n\nYou've learned about numbers and math‚Äînow let's use those skills to analyze real data! Statistics help us summarize and understand information.\n\n## Key Statistics We'll Calculate\n\n### 1. Total (Sum)\nAdd up all values to get the grand total.\n```python\nq1 = 100\nq2 = 200\ntotal = q1 + q2\n```\n\n### 2. Average (Mean)\nDivide total by the count.\n```python\naverage = total / 2\n```\n\n### 3. Maximum & Minimum\nPython has built-in helper functions that can take multiple inputs:\n```python\nbest = max(10, 20, 5)  # 20\nworst = min(10, 20, 5) # 5\n```\n\n### 4. Range\nThe difference between the best and worst.\n```python\nvariation = best - worst\n```\n\n## Example Walkthrough\n\n```python\n# Quarterly Sales\nq1 = 15000\nq2 = 18500\nq3 = 24500\nq4 = 22000\n\n# Total\ntotal = q1 + q2 + q3 + q4\n\n# Average (divide by 4 quarters)\naverage = total / 4\n\n# Best Quarter\nbest = max(q1, q2, q3, q4)\n```\n\n---\n\n## üéØ Your Task\n\nYour company had these quarterly sales: Q1=$15,000, Q2=$18,500, Q3=$24,500, Q4=$22,000\n\nCalculate the Total, Average, Best Quarter, and Range.",
    "starter_code": "# Sales Data\nq1 = 15000\nq2 = 18500\nq3 = 24500\nq4 = 22000\n\n# 1. Total (Add them up)\ntotal_sales = q1 + q2 + q3 + q4\n\n# 2. Average (Divide total by 4)\naverage_sales = total_sales / 4\n\n# 3. Best Quarter (Use max)\nbest_quarter = max(q1, q2, q3, q4)\n\n# 4. Range (max - min)\nsales_range = best_quarter - min(q1, q2, q3, q4)\n\n# Print Report\nprint(f\"Total: ${total_sales}\")\nprint(f\"Average: ${average_sales}\")\nprint(f\"Best: ${best_quarter}\")\nprint(f\"Range: ${sales_range}\")",
    "solution_code": "q1 = 15000\nq2 = 18500\nq3 = 24500\nq4 = 22000\n\ntotal_sales = q1 + q2 + q3 + q4\naverage_sales = total_sales / 4\nbest_quarter = max(q1, q2, q3, q4)\nsales_range = best_quarter - min(q1, q2, q3, q4)\n\nprint(f\"Total: ${total_sales}\")\nprint(f\"Average: ${average_sales}\")\nprint(f\"Best: ${best_quarter}\")\nprint(f\"Range: ${sales_range}\")",
    "expected_output": "Total: $80000\nAverage: $20000.0\nBest: $24500\nRange: $9500"
  },
  "148": {
    "title": "Financial Calculations",
    "chapter_title": "Variables, Types & Memory",
    "content": "# üí∞ Financial Calculations: E-Commerce Checkout\n\n## The Business Problem\n\nYou're building the checkout page for an online store. When a customer is ready to pay, you need to calculate:\n\n1. **Subtotal**: Sum of all items\n2. **Discount**: The amount saved\n3. **Tax**: Sales tax on the discounted price\n4. **Final Total**: What they actually pay\n\n## The Math Behind It\n\n```python\n# Step 1: Add up all items\nitem1 = 10\nitem2 = 20\nsubtotal = item1 + item2\n\n# Step 2: Calculate discount amount\ndiscount = subtotal * 0.15  # 15% off\n\n# Step 3: Apply discount\nafter_discount = subtotal - discount\n\n# Step 4: Calculate tax\ntax = after_discount * 0.08  # 8% tax\n\n# Step 5: Final total\nfinal_total = after_discount + tax\n```\n\n## Order of Operations Matters!\n\n‚ö†Ô∏è **Common mistake**: Applying tax BEFORE discount!\n\n**Wrong**: Tax on $100, then discount = $108 - $15 = $93\n**Right**: Discount first ($85), then tax = $85 + $7.23 = $92.23\n\nThe customer saves more when you discount first!\n\n---\n\n## üéØ Your Task\n\nBuild a shopping cart calculator for 3 items ($29.99, $15.99, $49.99).\nApply a 15% discount, then 8.5% tax. Print each step.",
    "starter_code": "# Shopping cart items\nitem1 = 29.99\nitem2 = 15.99\nitem3 = 49.99\n\n# Step 1: Calculate subtotal\nsubtotal = item1 + item2 + item3\n\n# Step 2: Apply 15% discount\ndiscount_rate = 0.15\ndiscount_amount = subtotal * discount_rate\nafter_discount = subtotal - discount_amount\n\n# Step 3: Apply 8.5% tax\ntax_rate = 0.085\ntax_amount = after_discount * tax_rate\n\n# Step 4: Calculate final total\nfinal_total = after_discount + tax_amount\n\n# Print the receipt\nprint(f\"Subtotal: ${subtotal:.2f}\")\nprint(f\"Discount: -${discount_amount:.2f}\")\nprint(f\"Tax: ${tax_amount:.2f}\")\nprint(f\"Total: ${final_total:.2f}\")",
    "solution_code": "item1 = 29.99\nitem2 = 15.99\nitem3 = 49.99\n\nsubtotal = item1 + item2 + item3\ndiscount_amount = subtotal * 0.15\nafter_discount = subtotal - discount_amount\ntax_amount = after_discount * 0.085\nfinal_total = after_discount + tax_amount\n\nprint(f\"Subtotal: ${subtotal:.2f}\")\nprint(f\"Discount: -${discount_amount:.2f}\")\nprint(f\"Tax: ${tax_amount:.2f}\")\nprint(f\"Total: ${final_total:.2f}\")",
    "expected_output": "Subtotal: $95.97\nDiscount: -$14.40\nTax: $6.93\nTotal: $88.51"
  },
  "149": {
    "title": "Converting Between Types",
    "chapter_title": "Type Conversion",
    "content": "# üîÑ Type Conversion: Transform Data Types\n\n## Why Convert Types?\n\nData often arrives as strings (from files, user input, APIs) but you need numbers for calculations:\n\n```python\nprice = \"29.99\"\nquantity = \"3\"\n\n# This fails! String + String = Concatenation\ntotal = price * quantity  # \"29.9929.9929.99\" üò±\n\n# Convert first!\ntotal = float(price) * int(quantity)  # 89.97 ‚úÖ\n```\n\n## The Big Three Converters\n\n```python\nint(\"42\")      # String ‚Üí Integer: 42\nfloat(\"3.14\")  # String ‚Üí Float: 3.14\nstr(100)       # Number ‚Üí String: \"100\"\n```\n\n## Common Conversions\n\n```python\n# String to Number\nint(\"100\")        # 100\nfloat(\"3.14\")     # 3.14\nint(\"100\", 2)     # 4 (binary to decimal!)\n\n# Number to String\nstr(42)           # \"42\"\nf\"{3.14159:.2f}\"  # \"3.14\"\n\n# Between Number Types\nint(3.7)          # 3 (truncates, doesn't round!)\nfloat(42)         # 42.0\nround(3.7)        # 4 (rounds)\n\n# To Boolean\nbool(0)           # False\nbool(1)           # True\nbool(\"\")          # False\nbool(\"text\")      # True\n```\n\n## Collection Conversions\n\n```python\nlist(\"hello\")     # ['h', 'e', 'l', 'l', 'o']\ntuple([1, 2, 3])  # (1, 2, 3)\nset([1, 2, 2])    # {1, 2}\nlist({1, 2, 3})   # [1, 2, 3]\n```\n\n---\n\n## üéØ Your Task\n\nPractice converting between different data types.",
    "starter_code": "# Various type conversions\nprint(\"=== String to Number ===\")\nage_string = \"25\"\nprice_string = \"19.99\"\n\nage = int(age_string)\nprice = float(price_string)\nprint(f\"Age: {age} (type: {type(age).__name__})\")\nprint(f\"Price: {price} (type: {type(price).__name__})\")\n\nprint(\"\\n=== Number to String ===\")\nquantity = 5\nformatted = f\"You ordered {quantity} items at ${price} each\"\nprint(formatted)\n\nprint(\"\\n=== Float to Int ===\")\nrating = 4.7\nstars = int(rating)  # Truncates!\nrounded = round(rating)\nprint(f\"Rating {rating} ‚Üí int: {stars}, rounded: {rounded}\")\n\nprint(\"\\n=== List/Set/Tuple ===\")\nnumbers = [1, 2, 2, 3, 3, 3]\nunique = set(numbers)\nback_to_list = list(unique)\nprint(f\"List: {numbers}\")\nprint(f\"Set: {unique}\")\nprint(f\"Back to list: {back_to_list}\")",
    "solution_code": "age = int(\"25\")\nprice = float(\"19.99\")\nprint(f\"Age: {age}, Price: ${price:.2f}\")\nprint(f\"Unique: {set([1,2,2,3,3])}\")",
    "expected_output": "=== String to Number ===\nAge: 25 (type: int)\nPrice: 19.99 (type: float)"
  },
  "150": {
    "title": "Truthy and Falsy Values",
    "chapter_title": "Logic",
    "content": "# ‚úÖ‚ùå Truthy and Falsy: What Python Considers True\n\n## The Key Insight\n\nPython doesn't just have True and False‚ÄîANY value can be used in a boolean context!\n\n## Falsy Values (Evaluate to False)\n\nThese specific values are considered \"False\" when converted to boolean:\n\n```python\n# 1. Zeroes\nbool(0)         # False\nbool(0.0)       # False\n\n# 2. Empty Sequences\nbool(\"\")        # False (Empty string)\nbool([])        # False (Empty list)\n\n# 3. None and False\nbool(None)      # False\nbool(False)     # False\n```\n\n## Truthy Values (Evaluate to True)\n\n**Everything else** is considered True!\n\n```python\nbool(1)         # True\nbool(-5)        # True\nbool(\"hello\")   # True\nbool(\" \")       # True (Space is a character!)\n```\n\n## Why This Matters\n\nYou can interpret data existence directly:\n\n```python\nusername = \"\"\nis_valid = bool(username)  # False, user didn't type anything!\n\nscore = 10\nhas_score = bool(score)    # True, they have points!\n```\n\n---\n\n## üéØ Your Task\n\nCheck the truthiness of different values using the `bool()` function.",
    "starter_code": "# Test various values\n\n# 1. Zero vs One\nprint(f\"0 is: {bool(0)}\")\nprint(f\"1 is: {bool(1)}\")\n\n# 2. Empty Strings vs Text\nprint(f\"Empty '' is: {bool('')}\")\nprint(f\"Text 'hi' is: {bool('hi')}\")\n\n# 3. None\nprint(f\"None is: {bool(None)}\")",
    "solution_code": "print(f\"0 is: {bool(0)}\")\nprint(f\"1 is: {bool(1)}\")\nprint(f\"Empty '' is: {bool('')}\")\nprint(f\"Text 'hi' is: {bool('hi')}\")\nprint(f\"None is: {bool(None)}\")",
    "expected_output": "0 is: False\n1 is: True\nEmpty '' is: False\nText 'hi' is: True\nNone is: False"
  },
  "151": {
    "title": "Parsing User Input",
    "chapter_title": "Variables, Types & Memory",
    "content": "# üì• Parsing Input: Strings to Numbers\n\n## The Problem\n\nWhen we receive data (like from a form or file), it usually comes as distinct **Strings**.\n\n```python\nraw_price = \"29.99\"\nraw_quantity = \"5\"\n```\n\nIf we try to do math, it fails:\n```python\ntotal = raw_price * raw_quantity\n# Error or crazy string repetition!\n```\n\n## The Parsing Process\n\nWe must convert each piece individually.\n\n```python\n# 1. Convert Price (Decimal -> float)\nprice = float(raw_price)  # 29.99\n\n# 2. Convert Quantity (Whole number -> int)\nqty = int(raw_quantity)   # 5\n\n# 3. Calculate\ntotal = price * qty       # 149.95\n```\n\n## Common Types to Parse\n\n- `float()`: Money, weights, temperature.\n- `int()`: Counts, IDs, ages.\n- `str()`: Anything visual.\n\n---\n\n## üéØ Your Task\n\nYou received three separate inputs for a product. Convert them and calculate the total value.\n\nInputs:\n- Price: \"49.50\"\n- Quantity: \"10\"\n- Tax Rate: \"0.08\"",
    "starter_code": "# Raw inputs (Strings)\ninput_price = \"49.50\"\ninput_qty = \"10\"\ninput_tax = \"0.08\"\n\n# 1. Parse inputs to correct types\nprice = float(input_price)\nqty = int(input_qty)\ntax_rate = float(input_tax)\n\n# 2. Calculate Total (Price * Qty)\nsubtotal = price * qty\n\n# 3. Add Tax (Subtotal * Tax Rate)\ntotal = subtotal + (subtotal * tax_rate)\n\nprint(f\"Subtotal: ${subtotal}\")\nprint(f\"Total with Tax: ${total}\")",
    "solution_code": "input_price = \"49.50\"\ninput_qty = \"10\"\ninput_tax = \"0.08\"\n\nprice = float(input_price)\nqty = int(input_qty)\ntax = float(input_tax)\n\nsubtotal = price * qty\ntotal = subtotal + (subtotal * tax)\n\nprint(f\"Subtotal: ${subtotal}\")\nprint(f\"Total with Tax: ${total}\")",
    "expected_output": "Subtotal: $495.0\nTotal with Tax: $534.6"
  },
  "152": {
    "title": "Complex Type Conversion",
    "chapter_title": "Type Conversion",
    "content": "# üîÄ Complex Type Conversion\n\n## Tricky Scenarios\n\nSometimes conversion isn't straightforward.\n\n## 1. Float to Int (Truncation)\nWhen converting decimal to integer, Python **cuts off** the decimal (it doesn't round!).\n\n```python\nint(3.99)  # 3 (Not 4!)\nint(-3.99) # -3\n```\nTo round, use `round()` first: `int(round(3.99))` -> 4.\n\n## 2. String Float to Int\nYou cannot convert a string with dots directly to an int!\n\n```python\nint(\"12.5\")        # ‚ùå Error!\nint(float(\"12.5\")) # ‚úÖ 12 (String -> Float -> Int)\n```\n\n## 3. Truthy Strings\n\n```python\nbool(\"False\")  # True (Because the string is not empty!)\n```\n\n---\n\n## üéØ Your Task\n\nPerform these tricky conversions:\n1. Convert \"12.99\" to an integer (truncate).\n2. Round 15.6 and convert to integer.\n3. Convert string \"0\" to boolean.",
    "starter_code": "# 1. Truncate \"12.99\" -> 12\nval1 = \"12.99\"\nres1 = int(float(val1))\n\n# 2. Round 15.6 -> 16\nval2 = 15.6\nres2 = int(round(val2))\n\n# 3. Boolean of \"0\"\nval3 = \"0\"\nres3 = bool(val3)\n\nprint(f\"Truncated: {res1}\")\nprint(f\"Rounded: {res2}\")\nprint(f\"Bool of '0': {res3}\")",
    "solution_code": "val1 = \"12.99\"\nres1 = int(float(val1))\nval2 = 15.6\nres2 = int(round(val2))\nval3 = \"0\"\nres3 = bool(val3)\n\nprint(f\"Truncated: {res1}\")\nprint(f\"Rounded: {res2}\")\nprint(f\"Bool of '0': {res3}\")",
    "expected_output": "Truncated: 12\nRounded: 16\nBool of '0': True"
  },
  "153": {
    "title": "Counting Characters",
    "chapter_title": "Loops",
    "content": "# üî¢ Counting Items: The Iterator Pattern\n\n## The Problem\n\nHow do you find out how many times a specific thing appears? You **count** it!\n\n## The Counting Pattern\n\n1. Initialize a counter to zero (`count = 0`)\n2. Loop through the sequence (string or range)\n3. If item matches, increment counter (`count += 1`)\n\n## Counting in Strings\n\n```python\ntext = \"hello world\"\ncount = 0\n\nfor char in text:\n    if char == 'l':\n        count += 1\n\nprint(f\"Found {count} L's\")\n```\n\n## Counting Numbers in Range\n\n```python\n# Count how many numbers 0-99 are divisible by 7\ncount = 0\nfor i in range(100):\n    if i % 7 == 0:\n        count += 1\n```\n\n## Why not just `count()`?\n\nStrings have a `.count()` method, but learning this logic is essential for complex conditions (like \"count usage of letters A, E, I, O, U combined\").\n\n---\n\n## üéØ Your Task\n\nCount how many **vowels** (a, e, i, o, u) are in the text provided.",
    "starter_code": "# Analyze this text\ntext = \"Programming in Python is fun and powerful!\"\n\n# Initialize counter\nvowel_count = 0\n\n# Loop through every character\nfor char in text:\n    # Check if character is a vowel (lowercase only for simplicity)\n    # HINT: You can check: if char in \"aeiou\":\n    if char.lower() in \"aeiou\":\n        vowel_count += 1\n\nprint(f\"Text: {text}\")\nprint(f\"Vowel Count: {vowel_count}\")",
    "solution_code": "text = \"Programming in Python is fun and powerful!\"\nvowel_count = 0\nfor char in text:\n    if char.lower() in \"aeiou\":\n        vowel_count += 1\nprint(vowel_count)",
    "expected_output": "Text: Programming in Python is fun and powerful!\nVowel Count: 11"
  },
  "154": {
    "title": "Finding Totals",
    "chapter_title": "Loops",
    "content": "# ‚ûï Finding Totals: The Accumulator Pattern\n\n## The Pattern\n\nThe **accumulator pattern** is fundamental to programming:\n1. Start with a container (usually 0 or \"\")\n2. Loop through data\n3. Add to the container\n4. Return the final result\n\n## Summing Numbers in Range\n\n```python\n# Sum numbers 0 to 4 (0+1+2+3+4 = 10)\ntotal = 0\nfor i in range(5):\n    total += i\n```\n\n## Building a String\n\n```python\nresult = \"\"\nfor char in \"hello\":\n    result += char + \"-\"\n# \"h-e-l-l-o-\"\n```\n\n## Conditional Accumulation\n\n```python\n# Sum only even numbers 0-9\ntotal = 0\nfor i in range(10):\n    if i % 2 == 0:\n        total += i\n```\n\n## When to Use Which\n\n| Task | Built-in | Accumulator |\n|------|----------|-------------|\n| Sum Range | `sum(range(n))` | Custom logic needed |\n| Join Strings | `\"\".join()` | Custom logic needed |\n| Complex Logic | N/A | Accumulator is Best |\n\n---\n\n## üéØ Your Task\n\nCalculate the **sum of squares** for numbers from 0 to 9.\n(0¬≤ + 1¬≤ + 2¬≤ + ... + 9¬≤)",
    "starter_code": "# Calculate sum of squares for 0-9\ntotal_squares = 0\n\n# Loop through 0-9\nfor i in range(10):\n    # Add square of i to total\n    square = i ** 2\n    total_squares += square\n\nprint(f\"Sum of squares 0-9: {total_squares}\")",
    "solution_code": "total = 0\nfor i in range(10):\n    total += i**2\nprint(total)",
    "expected_output": "Sum of squares 0-9: 285"
  },
  "155": {
    "title": "Number Sequences",
    "chapter_title": "Loops",
    "content": "# üî¢ Number Sequences: Generate Ranges with range()\n\n## Why range()?\n\nNeed to count from 1 to 100? Loop 50 times? Python's `range()` generates numbers on demand:\n\n```python\n# Instead of manually listing numbers\nfor i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n    print(i)\n\n# Use range!\nfor i in range(10):\n    print(i)\n```\n\n## Three Ways to Use range()\n\n### 1. range(stop) - Start at 0\n```python\nrange(5)  # 0, 1, 2, 3, 4\n```\n\n### 2. range(start, stop)\n```python\nrange(1, 6)  # 1, 2, 3, 4, 5\nrange(10, 15)  # 10, 11, 12, 13, 14\n```\n\n### 3. range(start, stop, step)\n```python\nrange(0, 10, 2)   # 0, 2, 4, 6, 8 (evens)\nrange(1, 10, 2)   # 1, 3, 5, 7, 9 (odds)\nrange(10, 0, -1)  # 10, 9, 8, 7... (countdown!)\n```\n\n## Common Patterns\n\n```python\n# Loop n times\nfor i in range(n):\n    do_something()\n\n# Access list by index\nfor i in range(len(my_list)):\n    print(f\"{i}: {my_list[i]}\")\n\n# Sum first 100 numbers\ntotal = sum(range(1, 101))  # 5050\n```\n\n## Converting to List\n\n```python\nlist(range(5))  # [0, 1, 2, 3, 4]\n```\n\n---\n\n## üéØ Your Task\n\nUse range() to generate various number sequences.",
    "starter_code": "# Basic range: 0 to 9\nprint(\"0 to 9:\")\nprint(list(range(10)))\n\n# Start and stop: 1 to 10\nprint(\"\\n1 to 10:\")\nprint(list(range(1, 11)))\n\n# Step: even numbers 0-20\nprint(\"\\nEven numbers 0-20:\")\nprint(list(range(0, 21, 2)))\n\n# Countdown\nprint(\"\\nCountdown from 5:\")\nfor i in range(5, 0, -1):\n    print(i)\nprint(\"Liftoff!\")\n\n# Sum of 1 to 100\ntotal = sum(range(1, 101))\nprint(f\"\\nSum of 1-100: {total}\")",
    "solution_code": "print(\"0-4:\", list(range(5)))\nprint(\"1-5:\", list(range(1, 6)))\nprint(\"Evens:\", list(range(0, 11, 2)))\nprint(\"Sum 1-100:\", sum(range(1, 101)))",
    "expected_output": "0 to 9:\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n1 to 10:\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
  },
  "156": {
    "title": "Countdown Timer",
    "chapter_title": "Loops",
    "content": "# ‚è∞ Countdown Timer: Loop Control in Action\n\n## Why While Loops?\n\nWhile loops run until a condition is false‚Äîperfect for countdowns!\n\n```python\ncount = 10\nwhile count > 0:\n    print(count)\n    count -= 1\nprint(\"Liftoff! üöÄ\")\n```\n\n## The Pattern\n\n```python\nwhile condition:   # Keep going while true\n    # Do something\n    # Update something (or you'll loop forever!)\n```\n\n## Common Countdown Patterns\n\n### Basic Countdown\n```python\nn = 5\nwhile n > 0:\n    print(n)\n    n -= 1\n```\n\n### Countdown with Formatting\n```python\nimport time\n\nfor i in range(10, 0, -1):\n    print(f\"\\r{i}...\", end=\"\")\n    time.sleep(1)\nprint(\"\\rBlastoff!\")\n```\n\n### Countdown with Break\n```python\nwhile count > 0:\n    if abort_requested:\n        print(\"Countdown aborted!\")\n        break\n    count -= 1\n```\n\n## ‚ö†Ô∏è Infinite Loop Prevention\n\nAlways make sure the condition will eventually become False!\n\n```python\n# BAD - loops forever!\nwhile True:\n    print(\"Help!\")\n\n# GOOD - has an exit condition\ncount = 5\nwhile count > 0:\n    print(count)\n    count -= 1\n```\n\n---\n\n## üéØ Your Task\n\nCreate a countdown timer that counts down from a number and prints a message at zero.",
    "starter_code": "# Countdown timer\ncount = 5\n\nprint(\"Countdown starting...\")\n\nwhile count > 0:\n    print(count)\n    count -= 1\n\nprint(\"üöÄ Liftoff!\")\n\n# Alternative using for loop\nprint(\"\\nAlternative countdown:\")\nfor i in range(5, 0, -1):\n    print(i)\nprint(\"üöÄ Liftoff!\")",
    "solution_code": "count = 5\nwhile count > 0:\n    print(count)\n    count -= 1\nprint(\"Liftoff!\")",
    "expected_output": "Countdown starting...\n5\n4\n3\n2\n1\nüöÄ Liftoff!"
  },
  "157": {
    "title": "Input Validation",
    "chapter_title": "Loops",
    "content": "# ‚úÖ Input Validation: Loop Until Valid\n\n## The Problem\n\nUser input can be wrong. Instead of crashing, keep asking until they get it right!\n\n## The Validation Loop Pattern\n\n```python\nwhile True:\n    user_input = input(\"Enter a number: \")\n    if user_input.isdigit():\n        number = int(user_input)\n        break  # Exit loop on valid input\n    else:\n        print(\"Invalid! Please enter a number.\")\n```\n\n## Common Validations\n\n### Numeric Validation\n```python\nwhile True:\n    try:\n        age = int(input(\"Age: \"))\n        if 0 < age < 120:\n            break\n        print(\"Enter a realistic age\")\n    except ValueError:\n        print(\"Must be a number\")\n```\n\n### Range Validation\n```python\nwhile True:\n    choice = int(input(\"Choose 1-3: \"))\n    if 1 <= choice <= 3:\n        break\n    print(\"Must be 1, 2, or 3\")\n```\n\n### String Validation\n```python\nwhile True:\n    email = input(\"Email: \")\n    if \"@\" in email and \".\" in email:\n        break\n    print(\"Invalid email format\")\n```\n\n## Limiting Attempts\n\n```python\nattempts = 3\nwhile attempts > 0:\n    password = input(\"Password: \")\n    if password == \"secret\":\n        print(\"Access granted!\")\n        break\n    attempts -= 1\n    print(f\"{attempts} attempts remaining\")\nelse:\n    print(\"Account locked!\")\n```\n\n---\n\n## üéØ Your Task\n\nCreate a validation loop that ensures valid user input.",
    "starter_code": "# Simulate input validation (normally uses input())\ntest_inputs = [\"abc\", \"-5\", \"150\", \"25\"]\ninput_index = 0\n\ndef get_test_input():\n    global input_index\n    val = test_inputs[input_index]\n    input_index += 1\n    return val\n\n# Validation loop\nvalid_age = None\nwhile valid_age is None:\n    user_input = get_test_input()\n    print(f\"Trying: '{user_input}'\")\n    \n    try:\n        age = int(user_input)\n        if 0 < age < 120:\n            valid_age = age\n            print(f\"Valid age: {valid_age}\")\n        else:\n            print(\"Age must be between 1 and 119\")\n    except ValueError:\n        print(\"Please enter a number\")\n\nprint(f\"\\nFinal validated age: {valid_age}\")",
    "solution_code": "# Validation loop pattern\nattempts = 0\nwhile attempts < 3:\n    user_input = ['abc', '-5', '25'][attempts]\n    try:\n        age = int(user_input)\n        if 0 < age < 120:\n            print(f\"Valid: {age}\")\n            break\n    except ValueError:\n        pass\n    attempts += 1",
    "expected_output": "Trying: 'abc'\nPlease enter a number\nTrying: '-5'\nAge must be between 1 and 119\nTrying: '150'\nAge must be between 1 and 119\nTrying: '25'\nValid age: 25"
  },
  "158": {
    "title": "Data Processing Loop",
    "chapter_title": "Loops",
    "content": "# üîÑ Data Processing: The Filter Pattern\n\n## The Pattern\n\nData processing often means taking raw input and making it clean. We can do this by building a new result string character by character.\n\n## Pattern: Build a Clean String\n\n1. Start with empty result (`clean = \"\"`)\n2. Loop through raw string\n3. If char is valid, add to result (`clean += char`)\n\n## Example: Remove Numbers\n\n```python\nraw = \"User123\"\nclean = \"\"\n\nfor char in raw:\n    # If it's NOT a digit, keep it\n    if not char.isdigit():\n        clean += char\n        \n# Result: \"User\"\n```\n\n## Example: Extract ID\n\n```python\nlog = \"Error #404: Not Found\"\n# Keep only digits\nid_str = \"\"\n\nfor char in log:\n    if char.isdigit():\n        id_str += char\n        \n# Result: \"404\"\n```\n\n## Why Loop?\n\nWhile `.replace()` is powerful, loops let you handle complex logic (like \"keep numbers only if they are after a # symbol\").\n\n---\n\n## üéØ Your Task\n\nYou received a corrupted message full of dots: `\"H.e.l.l.o. .W.o.r.l.d.\"`.\n\nLoop through it and build a clean message.",
    "starter_code": "# Corrupted message\nraw_message = \"H.e.l.l.o. .P.y.t.h.o.n.!.\"\n\n# Initialize accumulator\nclean_message = \"\"\n\n# Loop through each character\nfor char in raw_message:\n    # Only add character IF it is not a dot\n    if char != '.':\n        clean_message += char\n\nprint(f\"Raw:   {raw_message}\")\nprint(f\"Clean: {clean_message}\")",
    "solution_code": "raw = \"H.e.l.l.o.\"\nclean = \"\"\nfor char in raw:\n    if char != '.':\n        clean += char\nprint(clean)",
    "expected_output": "Raw:   H.e.l.l.o. .P.y.t.h.o.n.!.\nClean: Hello Python!"
  },
  "159": {
    "title": "Grade Calculator",
    "chapter_title": "Logic",
    "content": "# üìù Grade Calculator: Conditional Classification\n\n## The Grading Challenge\n\nConvert numeric scores to letter grades:\n- 90-100: A\n- 80-89: B\n- 70-79: C\n- 60-69: D\n- Below 60: F\n\n## Using if-elif-else\n\n```python\ndef get_grade(score):\n    if score >= 90:\n        return 'A'\n    elif score >= 80:\n        return 'B'\n    elif score >= 70:\n        return 'C'\n    elif score >= 60:\n        return 'D'\n    else:\n        return 'F'\n```\n\n## Order Matters!\n\nConditions are checked top to bottom. First true match wins:\n\n```python\n# WRONG - everyone gets F\nif score >= 60:\n    return 'D'  # Catches all 60+!\nelif score >= 70:\n    return 'C'  # Never reached!\n\n# CORRECT - check highest first\nif score >= 90:\n    return 'A'\nelif score >= 80:\n    return 'B'\n# ...etc\n```\n\n## Adding +/- Modifiers\n\n```python\ndef get_detailed_grade(score):\n    base_grades = [(90, 'A'), (80, 'B'), (70, 'C'), (60, 'D')]\n    \n    for cutoff, letter in base_grades:\n        if score >= cutoff:\n            mod = score - cutoff\n            if mod >= 7:\n                return f\"{letter}+\"\n            elif mod <= 2:\n                return f\"{letter}-\"\n            else:\n                return letter\n    \n    return 'F'\n```\n\n---\n\n## üéØ Your Task\n\nCreate a grade calculator that converts scores to letter grades.",
    "starter_code": "def get_grade(score):\n    '''Convert numeric score to letter grade'''\n    if score >= 90:\n        return 'A'\n    elif score >= 80:\n        return 'B'\n    elif score >= 70:\n        return 'C'\n    elif score >= 60:\n        return 'D'\n    else:\n        return 'F'\n\n# Test with multiple scores\ntest_scores = [95, 88, 75, 62, 45, 100, 70, 80]\n\nprint(\"Grade Report:\")\nfor score in test_scores:\n    grade = get_grade(score)\n    print(f\"  Score {score:3} ‚Üí Grade {grade}\")\n\n# Calculate class average\navg = sum(test_scores) / len(test_scores)\nprint(f\"\\nClass Average: {avg:.1f} ‚Üí {get_grade(avg)}\")",
    "solution_code": "def get_grade(score):\n    if score >= 90: return 'A'\n    elif score >= 80: return 'B'\n    elif score >= 70: return 'C'\n    elif score >= 60: return 'D'\n    return 'F'\n\nfor score in [95, 85, 75, 55]:\n    print(f\"{score} ‚Üí {get_grade(score)}\")",
    "expected_output": "Grade Report:\n  Score  95 ‚Üí Grade A\n  Score  88 ‚Üí Grade B\n  Score  75 ‚Üí Grade C\n  Score  62 ‚Üí Grade D\n  Score  45 ‚Üí Grade F"
  },
  "160": {
    "title": "Age Categories",
    "chapter_title": "Logic",
    "content": "# üéÇ Age Categories: Multi-Branch Decisions\n\n## The Problem\n\nClassify ages into life stages:\n- Child: 0-12\n- Teenager: 13-19\n- Adult: 20-64\n- Senior: 65+\n\n## The if-elif-else Chain\n\n```python\ndef get_age_category(age):\n    if age < 13:\n        return \"Child\"\n    elif age < 20:\n        return \"Teenager\"\n    elif age < 65:\n        return \"Adult\"\n    else:\n        return \"Senior\"\n```\n\n## Why elif Order Matters\n\nConditions are checked **in order**. Once one is True, the rest are skipped!\n\n```python\n# WRONG ORDER - everyone is \"Child\"!\nif age < 100:\n    return \"Human\"  # Always true (unless >100)\nelif age < 65:\n    return \"Adult\"  # Never reached!\n\n# CORRECT - most restrictive first\nif age < 13:\n    return \"Child\"\nelif age < 20:\n    return \"Teenager\"\n```\n\n## Adding More Detail\n\n```python\ndef get_age_info(age):\n    if age < 0:\n        return \"Invalid age\"\n    elif age < 1:\n        return \"Infant\"\n    elif age < 4:\n        return \"Toddler\"\n    elif age < 13:\n        return \"Child\"\n    elif age < 20:\n        return \"Teenager\"\n    elif age < 30:\n        return \"Young Adult\"\n    elif age < 65:\n        return \"Adult\"\n    else:\n        return \"Senior\"\n```\n\n## Alternative: Match Statement (Python 3.10+)\n\n```python\nmatch age // 20:\n    case 0:\n        return \"Child/Teen\"\n    case 1 | 2 | 3:\n        return \"Adult\"\n    case _:\n        return \"Senior\"\n```\n\n---\n\n## üéØ Your Task\n\nCreate an age classification function with multiple categories.",
    "starter_code": "def categorize_age(age):\n    '''Categorize a person by their age'''\n    if age < 0:\n        return \"Invalid\"\n    elif age < 13:\n        return \"Child\"\n    elif age < 20:\n        return \"Teenager\"\n    elif age < 65:\n        return \"Adult\"\n    else:\n        return \"Senior\"\n\n# Test with various ages\ntest_ages = [5, 13, 25, 65, 80, -1]\n\nprint(\"Age Classification:\")\nfor age in test_ages:\n    category = categorize_age(age)\n    print(f\"  Age {age:3} ‚Üí {category}\")",
    "solution_code": "def categorize_age(age):\n    if age < 13:\n        return \"Child\"\n    elif age < 20:\n        return \"Teenager\"\n    elif age < 65:\n        return \"Adult\"\n    return \"Senior\"\n\nfor age in [5, 15, 35, 70]:\n    print(f\"{age} ‚Üí {categorize_age(age)}\")",
    "expected_output": "Age Classification:\n  Age   5 ‚Üí Child\n  Age  13 ‚Üí Teenager\n  Age  25 ‚Üí Adult\n  Age  65 ‚Üí Senior\n  Age  80 ‚Üí Senior"
  },
  "161": {
    "title": "Complex Conditions",
    "chapter_title": "Logic",
    "content": "# üîÄ Complex Conditions: AND, OR, NOT in Action\n\n## Combining Conditions\n\nReal-world decisions rarely depend on one factor. Combine conditions with:\n- `and` - ALL must be true\n- `or` - ANY can be true\n- `not` - Inverts true ‚Üî false\n\n## AND: The Strict Gatekeeper\n\nBoth conditions must be true:\n\n```python\nif age >= 18 and has_license:\n    print(\"Can drive\")\n\nif is_member and credits > 0:\n    print(\"Can download\")\n```\n\n## OR: The Flexible Filter\n\nEither condition can be true:\n\n```python\nif is_admin or is_owner:\n    print(\"Full access\")\n\nif error_code == 404 or error_code == 500:\n    print(\"Server issue\")\n```\n\n## NOT: The Inverter\n\n```python\nif not is_banned:\n    print(\"Welcome!\")\n\n# Same as:\nif is_banned == False:\n    print(\"Welcome!\")\n```\n\n## Combining Multiple\n\n```python\n# Access if: (admin OR owner) AND (not suspended)\nif (is_admin or is_owner) and not is_suspended:\n    grant_access()\n\n# Eligible if: prime member, OR (regular + high spend)\nif is_prime or (is_member and total_spent > 1000):\n    free_shipping()\n```\n\n## Parentheses Matter!\n\n```python\n# Different outcomes!\nA and B or C    # (A and B) or C\nA and (B or C)  # A and (B or C)\n```\n\n---\n\n## üéØ Your Task\n\nBuild access control logic using complex conditions.",
    "starter_code": "def check_access(user):\n    '''Determine if user has access based on multiple conditions'''\n    is_admin = user.get('role') == 'admin'\n    is_subscriber = user.get('subscription', False)\n    is_verified = user.get('verified', False)\n    is_banned = user.get('banned', False)\n    \n    # No access if banned\n    if is_banned:\n        return \"Denied: User is banned\"\n    \n    # Full access for admins\n    if is_admin:\n        return \"Full access (admin)\"\n    \n    # Subscribers with verification get access\n    if is_subscriber and is_verified:\n        return \"Full access (verified subscriber)\"\n    \n    # Subscribers without verification get limited access\n    if is_subscriber:\n        return \"Limited access (please verify email)\"\n    \n    return \"No access\"\n\n# Test different user types\nusers = [\n    {\"name\": \"Admin\", \"role\": \"admin\"},\n    {\"name\": \"VIP\", \"subscription\": True, \"verified\": True},\n    {\"name\": \"New User\", \"subscription\": True, \"verified\": False},\n    {\"name\": \"Banned User\", \"subscription\": True, \"banned\": True},\n    {\"name\": \"Guest\"},\n]\n\nprint(\"Access Control Results:\")\nfor user in users:\n    access = check_access(user)\n    print(f\"  {user['name']:12} ‚Üí {access}\")",
    "solution_code": "def check_access(is_admin, is_subscriber, is_verified, is_banned):\n    if is_banned:\n        return \"Denied\"\n    if is_admin or (is_subscriber and is_verified):\n        return \"Full access\"\n    return \"Limited\"\n\nprint(check_access(True, False, False, False))\nprint(check_access(False, True, True, False))",
    "expected_output": "Access Control Results:\n  Admin        ‚Üí Full access (admin)\n  VIP          ‚Üí Full access (verified subscriber)\n  New User     ‚Üí Limited access (please verify email)\n  Banned User  ‚Üí Denied: User is banned\n  Guest        ‚Üí No access"
  },
  "162": {
    "title": "Nested Decision Tree",
    "chapter_title": "Logic",
    "content": "# ÔøΩÔøΩ Nested Decisions: If Inside If\n\n## When to Nest\n\nSometimes one decision depends on another:\n- \"If logged in, then check if admin\"\n- \"If order exists, then check if paid\"\n\n## Basic Nested Structure\n\n```python\nif condition1:\n    if condition2:\n        # Both conditions true\n        do_something()\n    else:\n        # Only first true\n        do_alternative()\nelse:\n    # First condition false\n    do_fallback()\n```\n\n## Real Example: Login Flow\n\n```python\ndef handle_login(user, password):\n    if user_exists(user):\n        if check_password(user, password):\n            if is_2fa_enabled(user):\n                return \"Enter 2FA code\"\n            else:\n                return \"Login successful\"\n        else:\n            return \"Invalid password\"\n    else:\n        return \"User not found\"\n```\n\n## Flattening with Early Returns\n\nNested code can be hard to read. Use early returns instead:\n\n```python\ndef handle_login(user, password):\n    if not user_exists(user):\n        return \"User not found\"\n    \n    if not check_password(user, password):\n        return \"Invalid password\"\n    \n    if is_2fa_enabled(user):\n        return \"Enter 2FA code\"\n    \n    return \"Login successful\"\n```\n\n## Decision Tree Pattern\n\n```python\ndef categorize_animal(has_fur, lays_eggs, can_fly):\n    if has_fur:\n        if lays_eggs:\n            return \"Platypus!\"\n        else:\n            return \"Mammal\"\n    else:\n        if can_fly:\n            return \"Bird\"\n        else:\n            return \"Reptile or Fish\"\n```\n\n---\n\n## üéØ Your Task\n\nBuild a decision tree to categorize customer support tickets.",
    "starter_code": "def categorize_ticket(ticket):\n    '''Categorize support ticket based on nested conditions'''\n    is_urgent = ticket.get('priority') == 'urgent'\n    is_vip = ticket.get('is_vip', False)\n    category = ticket.get('category', 'general')\n    \n    if is_urgent:\n        if is_vip:\n            return \"Priority 1 - VIP Urgent\"\n        else:\n            if category == 'billing':\n                return \"Priority 2 - Urgent Billing\"\n            else:\n                return \"Priority 2 - Urgent General\"\n    else:\n        if is_vip:\n            return \"Priority 3 - VIP Normal\"\n        else:\n            return \"Priority 4 - Standard\"\n\n# Test tickets\ntickets = [\n    {\"id\": 1, \"priority\": \"urgent\", \"is_vip\": True, \"category\": \"technical\"},\n    {\"id\": 2, \"priority\": \"urgent\", \"is_vip\": False, \"category\": \"billing\"},\n    {\"id\": 3, \"priority\": \"normal\", \"is_vip\": True, \"category\": \"general\"},\n    {\"id\": 4, \"priority\": \"normal\", \"is_vip\": False, \"category\": \"general\"},\n]\n\nprint(\"Ticket Categorization:\")\nfor ticket in tickets:\n    result = categorize_ticket(ticket)\n    print(f\"  Ticket {ticket['id']}: {result}\")",
    "solution_code": "def categorize_ticket(urgent, vip):\n    if urgent and vip:\n        return \"P1 - VIP Urgent\"\n    elif urgent:\n        return \"P2 - Urgent\"\n    elif vip:\n        return \"P3 - VIP\"\n    return \"P4 - Standard\"\n\nprint(categorize_ticket(True, True))",
    "expected_output": "Ticket Categorization:\n  Ticket 1: Priority 1 - VIP Urgent\n  Ticket 2: Priority 2 - Urgent Billing\n  Ticket 3: Priority 3 - VIP Normal\n  Ticket 4: Priority 4 - Standard"
  },
  "163": {
    "title": "Smart Filter",
    "chapter_title": "Logic",
    "content": "# üîç Smart Filtering: Multiple Criteria Logic\n\n## The Problem\n\nReal filtering often requires multiple conditions:\n- \"In stock AND under $50\"\n- \"Premium members OR orders over $100\"\n- \"NOT discontinued AND rating > 4\"\n\n## Combining Conditions\n\n### AND: All conditions must be true\n```python\nif in_stock and price < 50:\n    show_product()\n\nif age >= 18 and has_id and not banned:\n    allow_entry()\n```\n\n### OR: Any condition can be true\n```python\nif is_vip or total > 100:\n    offer_discount()\n\nif is_weekend or is_holiday:\n    apply_surge_pricing()\n```\n\n### NOT: Invert the condition\n```python\nif not is_sold_out:\n    show_availability()\n```\n\n## Function-Based Filtering\n\n```python\ndef should_contact(customer):\n    '''Determine if we should contact this customer'''\n    # Don't contact if opted out\n    if customer.get('opted_out', False):\n        return False\n    \n    # Contact if VIP or high value\n    if customer.get('is_vip') or customer.get('lifetime_value', 0) > 1000:\n        return True\n    \n    # Contact if recent and active\n    if customer.get('days_since_last', 999) < 30 and customer.get('is_active'):\n        return True\n    \n    return False\n```\n\n## With list comprehension\n\n```python\n# Filter products matching criteria\ndeals = [p for p in products \n         if p['price'] < 50 and p['stock'] > 0 and p['rating'] >= 4]\n```\n\n---\n\n## üéØ Your Task\n\nCreate a smart filter function with multiple conditions.",
    "starter_code": "def filter_products(products, max_price=None, min_rating=None, in_stock_only=False):\n    '''Filter products based on multiple criteria'''\n    results = []\n    \n    for product in products:\n        # Check price\n        if max_price and product['price'] > max_price:\n            continue\n        \n        # Check rating\n        if min_rating and product['rating'] < min_rating:\n            continue\n        \n        # Check stock\n        if in_stock_only and product['stock'] <= 0:\n            continue\n        \n        results.append(product)\n    \n    return results\n\n# Sample data\nproducts = [\n    {\"name\": \"Widget A\", \"price\": 25, \"rating\": 4.5, \"stock\": 10},\n    {\"name\": \"Widget B\", \"price\": 75, \"rating\": 4.8, \"stock\": 0},\n    {\"name\": \"Widget C\", \"price\": 30, \"rating\": 3.2, \"stock\": 5},\n    {\"name\": \"Widget D\", \"price\": 45, \"rating\": 4.6, \"stock\": 15},\n]\n\n# Apply filters\nfiltered = filter_products(products, max_price=50, min_rating=4.0, in_stock_only=True)\n\nprint(\"Filtered products (under $50, rating 4+, in stock):\")\nfor p in filtered:\n    print(f\"  {p['name']}: ${p['price']}, {p['rating']}‚òÖ\")",
    "solution_code": "products = [{\"name\": \"A\", \"price\": 25, \"rating\": 4.5, \"stock\": 10}]\nfiltered = [p for p in products if p['price'] < 50 and p['rating'] >= 4 and p['stock'] > 0]\nprint(filtered)",
    "expected_output": "Filtered products (under $50, rating 4+, in stock):\n  Widget A: $25, 4.5‚òÖ\n  Widget D: $45, 4.6‚òÖ"
  },
  "164": {
    "title": "Priority Sorter",
    "chapter_title": "Logic",
    "content": "# üéØ Priority Sorter: Ranking with Logic\n\n## The Problem\n\nAssign priority levels based on multiple factors:\n- Urgency (high/medium/low)\n- Customer tier (VIP/regular)\n- Value ($$$ amount)\n\n## Single Factor Priority\n\n```python\ndef get_priority(urgency):\n    if urgency == \"high\":\n        return 1\n    elif urgency == \"medium\":\n        return 2\n    else:\n        return 3\n```\n\n## Multi-Factor Priority\n\nCombine multiple conditions:\n\n```python\ndef calculate_priority(urgency, is_vip, value):\n    # VIP + High urgency = Top priority\n    if is_vip and urgency == \"high\":\n        return 1\n    # VIP OR high urgency\n    elif is_vip or urgency == \"high\":\n        return 2\n    # High value\n    elif value > 10000:\n        return 2\n    # Medium urgency\n    elif urgency == \"medium\":\n        return 3\n    # Low priority\n    else:\n        return 4\n```\n\n## Numeric Score Approach\n\n```python\ndef priority_score(urgency, is_vip, value):\n    score = 0\n    \n    # Urgency weight\n    urgency_scores = {\"high\": 30, \"medium\": 20, \"low\": 10}\n    score += urgency_scores.get(urgency, 0)\n    \n    # VIP bonus\n    if is_vip:\n        score += 25\n    \n    # Value weight\n    score += min(value // 1000, 20)  # Cap at 20 points\n    \n    return score\n```\n\n## Sorting by Priority\n\n```python\ntasks = [\n    {\"name\": \"Task A\", \"priority\": 2},\n    {\"name\": \"Task B\", \"priority\": 1},\n    {\"name\": \"Task C\", \"priority\": 3}\n]\n\nsorted_tasks = sorted(tasks, key=lambda t: t[\"priority\"])\n# Task B (1), Task A (2), Task C (3)\n```\n\n---\n\n## üéØ Your Task\n\nCreate a priority scoring system based on multiple factors.",
    "starter_code": "def calculate_priority(task):\n    '''Calculate task priority score (higher = more important)'''\n    score = 0\n    \n    # Urgency (30 points max)\n    urgency_scores = {\"critical\": 30, \"high\": 20, \"medium\": 10, \"low\": 0}\n    score += urgency_scores.get(task.get(\"urgency\", \"low\"), 0)\n    \n    # VIP customer (20 points)\n    if task.get(\"is_vip\", False):\n        score += 20\n    \n    # Value-based (up to 20 points)\n    value = task.get(\"value\", 0)\n    score += min(value // 100, 20)\n    \n    return score\n\n# Test with sample tasks\ntasks = [\n    {\"name\": \"Fix bug\", \"urgency\": \"critical\", \"is_vip\": True, \"value\": 5000},\n    {\"name\": \"New feature\", \"urgency\": \"medium\", \"is_vip\": False, \"value\": 1000},\n    {\"name\": \"Support call\", \"urgency\": \"high\", \"is_vip\": True, \"value\": 100},\n]\n\nprint(\"Task Priorities:\")\nsorted_tasks = sorted(tasks, key=calculate_priority, reverse=True)\nfor task in sorted_tasks:\n    score = calculate_priority(task)\n    print(f\"  {task['name']}: {score} points\")",
    "solution_code": "def calculate_priority(task):\n    score = {\"critical\": 30, \"high\": 20, \"medium\": 10}.get(task.get(\"urgency\", \"low\"), 0)\n    if task.get(\"is_vip\"):\n        score += 20\n    return score\n\ntasks = [{\"name\": \"A\", \"urgency\": \"critical\"}, {\"name\": \"B\", \"urgency\": \"low\"}]\nfor t in sorted(tasks, key=calculate_priority, reverse=True):\n    print(f\"{t['name']}: {calculate_priority(t)}\")",
    "expected_output": "Task Priorities:\n  Fix bug: 70 points\n  Support call: 41 points\n  New feature: 20 points"
  },
  "165": {
    "title": "Calculate Tax",
    "chapter_title": "Functions",
    "content": "# üí∞ Tax Calculator: Real-World Function Design\n\n## Why Functions for Calculations?\n\nTax calculations are:\n- Used in many places (checkout, invoices, reports)\n- Have specific rules that shouldn't be copied everywhere\n- Need to be easily updatable when rates change\n\n## Basic Tax Function\n\n```python\ndef calculate_tax(amount, tax_rate=0.0825):\n    '''Calculate tax for a given amount'''\n    return amount * tax_rate\n```\n\n## Complete Tax Calculation\n\n```python\ndef calculate_total(subtotal, tax_rate=0.0825, discount=0):\n    '''Calculate final total with discount and tax'''\n    after_discount = subtotal - discount\n    tax = after_discount * tax_rate\n    total = after_discount + tax\n    \n    return {\n        'subtotal': subtotal,\n        'discount': discount,\n        'tax': round(tax, 2),\n        'total': round(total, 2)\n    }\n```\n\n## Tax Rate Lookup\n\n```python\nstate_tax_rates = {\n    'CA': 0.0825,\n    'TX': 0.0625,\n    'NY': 0.08,\n    'OR': 0.0,  # No sales tax!\n}\n\ndef get_tax_for_state(amount, state):\n    rate = state_tax_rates.get(state, 0.0)\n    return amount * rate\n```\n\n## Why Return a Dict?\n\nReturning multiple related values as a dict is cleaner:\n```python\nresult = calculate_total(100, discount=10)\nprint(f\"Tax: {result['tax']}\")\nprint(f\"Total: {result['total']}\")\n```\n\n---\n\n## üéØ Your Task\n\nBuild a tax calculator function that handles subtotals, discounts, and tax.",
    "starter_code": "def calculate_total(subtotal, tax_rate=0.0825, discount=0):\n    '''Calculate total with discount and tax'''\n    # Apply discount first\n    after_discount = subtotal - discount\n    \n    # Calculate tax on discounted amount\n    tax = after_discount * tax_rate\n    \n    # Final total\n    total = after_discount + tax\n    \n    return {\n        'subtotal': subtotal,\n        'discount': discount,\n        'after_discount': round(after_discount, 2),\n        'tax': round(tax, 2),\n        'total': round(total, 2)\n    }\n\n# Test the function\nresult = calculate_total(100, discount=15)\n\nprint(\"Invoice Summary:\")\nprint(f\"  Subtotal: ${result['subtotal']:.2f}\")\nprint(f\"  Discount: -${result['discount']:.2f}\")\nprint(f\"  After Discount: ${result['after_discount']:.2f}\")\nprint(f\"  Tax (8.25%): ${result['tax']:.2f}\")\nprint(f\"  Total: ${result['total']:.2f}\")",
    "solution_code": "def calculate_total(subtotal, tax_rate=0.0825, discount=0):\n    after_discount = subtotal - discount\n    tax = after_discount * tax_rate\n    total = after_discount + tax\n    return {'tax': round(tax, 2), 'total': round(total, 2)}\n\nprint(calculate_total(100, discount=15))",
    "expected_output": "Invoice Summary:\n  Subtotal: $100.00\n  Discount: -$15.00\n  After Discount: $85.00\n  Tax (8.25%): $7.01\n  Total: $92.01"
  },
  "166": {
    "title": "Format Currency",
    "chapter_title": "Functions",
    "content": "# üíµ Currency Formatting: Make Numbers Human-Readable\n\n## Why Format Currency?\n\nRaw numbers are hard to read:\n- `1234567.89` vs `$1,234,567.89`\n\nProfessional applications format money with:\n- Currency symbols ($, ‚Ç¨, ¬£)\n- Thousand separators (commas)\n- Consistent decimal places\n\n## Python's String Formatting\n\n```python\n# Basic formatting\nprice = 1234.5\nf\"${price:.2f}\"  # \"$1234.50\"\n\n# With comma separators\nf\"${price:,.2f}\"  # \"$1,234.50\"\n```\n\n## The Format Specifier Breakdown\n\n```python\nf\"{value:,.2f}\"\n      ‚Üë  ‚Üë ‚Üë\n      |  | ‚îî‚îÄ‚îÄ f = float\n      |  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ 2 = two decimal places\n      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ , = thousand separator\n```\n\n## Reusable Function\n\n```python\ndef format_currency(amount, symbol=\"$\", decimals=2):\n    '''Format a number as currency'''\n    formatted = f\"{amount:,.{decimals}f}\"\n    return f\"{symbol}{formatted}\"\n\nformat_currency(1234567.89)        # \"$1,234,567.89\"\nformat_currency(1234567.89, \"‚Ç¨\")   # \"‚Ç¨1,234,567.89\"\nformat_currency(1234567.89, \"¬£\", 0)  # \"¬£1,234,568\"\n```\n\n## Handling Edge Cases\n\n```python\ndef format_currency(amount, symbol=\"$\"):\n    if amount < 0:\n        return f\"-{symbol}{abs(amount):,.2f}\"\n    return f\"{symbol}{amount:,.2f}\"\n\nformat_currency(-500)  # \"-$500.00\"\n```\n\n---\n\n## üéØ Your Task\n\nCreate a function that formats numbers as currency with proper symbols and separators.",
    "starter_code": "def format_currency(amount, symbol=\"$\", decimals=2):\n    '''Format a number as currency with symbol and commas'''\n    # Handle negative amounts\n    if amount < 0:\n        return f\"-{symbol}{abs(amount):,.{decimals}f}\"\n    return f\"{symbol}{amount:,.{decimals}f}\"\n\n# Test the function\nprices = [1234.5, 999999.99, 45.678, -250.50]\n\nprint(\"Formatted prices:\")\nfor price in prices:\n    print(f\"  {price} ‚Üí {format_currency(price)}\")\n\n# Test with different symbols\nprint(f\"\\nEuros: {format_currency(1500.99, '‚Ç¨')}\")\nprint(f\"Pounds: {format_currency(1500.99, '¬£')}\")",
    "solution_code": "def format_currency(amount, symbol=\"$\", decimals=2):\n    if amount < 0:\n        return f\"-{symbol}{abs(amount):,.{decimals}f}\"\n    return f\"{symbol}{amount:,.{decimals}f}\"\n\nprint(format_currency(1234567.89))\nprint(format_currency(-500.50))",
    "expected_output": "Formatted prices:\n  1234.5 ‚Üí $1,234.50\n  999999.99 ‚Üí $999,999.99\n  45.678 ‚Üí $45.68\n  -250.5 ‚Üí -$250.50"
  },
  "167": {
    "title": "Flexible Greeting",
    "chapter_title": "Functions",
    "content": "# üëã Default Parameters: Flexible Functions\n\n## Why Default Parameters?\n\nMake functions more flexible by providing sensible defaults:\n\n```python\n# Without defaults - always need all args\ngreet(\"Alice\", \"Hello\")\ngreet(\"Bob\", \"Hello\")  # Same greeting every time!\n\n# With defaults - optional customization\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\ngreet(\"Alice\")           # \"Hello, Alice!\"\ngreet(\"Bob\", \"Hi\")       # \"Hi, Bob!\"\n```\n\n## The Syntax\n\n```python\ndef function_name(required, optional=\"default\"):\n    # required must be provided\n    # optional uses \"default\" if not provided\n    pass\n```\n\n## Order Matters!\n\nRequired parameters must come before optional ones:\n\n```python\n# CORRECT\ndef greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    return f\"{greeting}, {name}{punctuation}\"\n\n# WRONG - SyntaxError!\ndef greet(greeting=\"Hello\", name):  # Default before required\n    pass\n```\n\n## Calling with Named Arguments\n\n```python\ndef format_name(first, last, title=\"\"):\n    return f\"{title} {first} {last}\".strip()\n\nformat_name(\"Alice\", \"Smith\")           # \"Alice Smith\"\nformat_name(\"Bob\", \"Jones\", \"Dr.\")      # \"Dr. Bob Jones\"\nformat_name(first=\"Eve\", last=\"Chen\", title=\"Prof.\")\n```\n\n## Common Use Cases\n\n```python\n# Logging with optional level\ndef log(message, level=\"INFO\"):\n    print(f\"[{level}] {message}\")\n\n# API calls with optional timeout\ndef fetch(url, timeout=30):\n    # make request with timeout\n    pass\n```\n\n---\n\n## üéØ Your Task\n\nCreate a greeting function with customizable greeting and punctuation.",
    "starter_code": "def greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    '''Create a customizable greeting message'''\n    return f\"{greeting}, {name}{punctuation}\"\n\n# Test with different combinations\nprint(greet(\"Alice\"))                           # Uses all defaults\nprint(greet(\"Bob\", \"Hi\"))                       # Custom greeting\nprint(greet(\"Charlie\", \"Welcome\", \"!!!\"))       # All custom\nprint(greet(\"Diana\", punctuation=\"?\"))          # Named argument",
    "solution_code": "def greet(name, greeting=\"Hello\", punctuation=\"!\"):\n    return f\"{greeting}, {name}{punctuation}\"\n\nprint(greet(\"Alice\"))\nprint(greet(\"Bob\", \"Hi\"))\nprint(greet(\"Charlie\", \"Welcome\", \"!!!\"))",
    "expected_output": "Hello, Alice!\nHi, Bob!\nWelcome, Charlie!!!\nHello, Diana?"
  },
  "168": {
    "title": "Stats Calculator",
    "chapter_title": "Functions",
    "content": "# üìä Stats Calculator: Return Multiple Values\n\n## Why Return Multiple Values?\n\nSome calculations naturally produce multiple results:\n- Statistics: mean, median, min, max, std\n- Position: x, y coordinates\n- Config: status, message, data\n\n## Three Ways to Return Multiple Values\n\n### 1. Return a Tuple\n```python\ndef calculate_stats(numbers):\n    return min(numbers), max(numbers), sum(numbers)/len(numbers)\n\nresult = calculate_stats([1, 2, 3, 4, 5])\n# (1, 5, 3.0)\n\n# Unpack directly\nmin_val, max_val, avg = calculate_stats([1, 2, 3, 4, 5])\n```\n\n### 2. Return a Dictionary\n```python\ndef calculate_stats(numbers):\n    return {\n        'min': min(numbers),\n        'max': max(numbers),\n        'mean': sum(numbers) / len(numbers)\n    }\n\nresult = calculate_stats([1, 2, 3, 4, 5])\nprint(result['mean'])  # 3.0\n```\n\n### 3. Return a Named Tuple\n```python\nfrom collections import namedtuple\n\nStats = namedtuple('Stats', ['min', 'max', 'mean'])\n\ndef calculate_stats(numbers):\n    return Stats(min(numbers), max(numbers), sum(numbers)/len(numbers))\n\nresult = calculate_stats([1, 2, 3, 4, 5])\nprint(result.mean)  # 3.0\n```\n\n## When to Use Which\n\n| Method | Use When |\n|--------|----------|\n| Tuple | Few values, order is obvious |\n| Dict | Many values, need named access |\n| Named Tuple | Need both position AND names |\n\n---\n\n## üéØ Your Task\n\nBuild a stats calculator that returns multiple statistics at once.",
    "starter_code": "def calculate_stats(numbers):\n    '''Calculate and return multiple statistics'''\n    n = len(numbers)\n    total = sum(numbers)\n    mean = total / n\n    \n    sorted_nums = sorted(numbers)\n    if n % 2 == 0:\n        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n    else:\n        median = sorted_nums[n//2]\n    \n    return {\n        'count': n,\n        'sum': total,\n        'mean': round(mean, 2),\n        'median': median,\n        'min': min(numbers),\n        'max': max(numbers),\n        'range': max(numbers) - min(numbers)\n    }\n\n# Test the function\ndata = [10, 20, 30, 40, 50, 60, 70]\nstats = calculate_stats(data)\n\nprint(\"Statistics Report:\")\nfor key, value in stats.items():\n    print(f\"  {key}: {value}\")",
    "solution_code": "def calculate_stats(numbers):\n    return {\n        'mean': sum(numbers) / len(numbers),\n        'min': min(numbers),\n        'max': max(numbers)\n    }\n\nstats = calculate_stats([10, 20, 30, 40, 50])\nprint(f\"Mean: {stats['mean']}\")",
    "expected_output": "Statistics Report:\n  count: 7\n  sum: 280\n  mean: 40.0\n  median: 40\n  min: 10\n  max: 70\n  range: 60"
  },
  "169": {
    "title": "Data Transformer",
    "chapter_title": "Functions",
    "content": "# üîÑ Lambda + Map: Transform Data in One Line\n\n## Why Lambda Functions?\n\nSometimes you need a tiny function just ONCE. Writing a full `def` function feels like overkill:\n\n```python\n# Too much code for something this simple!\ndef celsius_to_fahrenheit(c):\n    return c * 9/5 + 32\n\ntemps_f = list(map(celsius_to_fahrenheit, temps_c))\n```\n\nLambda lets you do this in ONE line!\n\n## What is a Lambda?\n\nA **lambda** is an anonymous (nameless) mini-function:\n\n```python\n# Regular function\ndef double(x):\n    return x * 2\n\n# Same thing as lambda\ndouble = lambda x: x * 2\n```\n\n## The Lambda Syntax\n\n```python\nlambda arguments: expression\n       ‚Üë              ‚Üë\n    What goes in   What comes out\n```\n\n## Lambda + Map = Power Combo\n\n`map()` applies a function to every item in a list:\n\n```python\nnumbers = [1, 2, 3, 4, 5]\n\n# Without lambda (need separate function)\ndef square(x):\n    return x ** 2\nsquared = list(map(square, numbers))\n\n# With lambda (all in one line!)\nsquared = list(map(lambda x: x ** 2, numbers))\n# [1, 4, 9, 16, 25]\n```\n\n## Real-World Uses\n\n```python\n# Convert prices from USD to EUR\nprices_eur = list(map(lambda p: p * 0.85, prices_usd))\n\n# Extract first names from full names\nfirst_names = list(map(lambda name: name.split()[0], full_names))\n\n# Clean up data\ncleaned = list(map(lambda s: s.strip().lower(), messy_strings))\n```\n\n## When to Use Lambda vs def\n\n| Use Lambda When | Use def When |\n|----------------|--------------|\n| One-liner logic | Multiple lines needed |\n| Used once | Reused in multiple places |\n| Simple transformation | Complex logic |\n\n---\n\n## üéØ Your Task\n\nConvert a list of temperatures from Celsius to Fahrenheit using `lambda` and `map()`.\n\n**Formula:** F = C √ó 9/5 + 32",
    "starter_code": "# Temperatures in Celsius\ncelsius = [0, 10, 20, 30, 40]\n\n# Convert to Fahrenheit using lambda with map\n# Formula: F = C * 9/5 + 32\nfahrenheit = list(map(lambda c: c * 9/5 + 32, celsius))\n\nprint(fahrenheit)",
    "solution_code": "celsius = [0, 10, 20, 30, 40]\nfahrenheit = list(map(lambda c: c * 9/5 + 32, celsius))\nprint(fahrenheit)",
    "expected_output": "[32.0, 50.0, 68.0, 86.0, 104.0]"
  },
  "170": {
    "title": "Recursive Factorial",
    "chapter_title": "Functions",
    "content": "# üîÅ Recursion: Functions That Call Themselves\n\n## What is Recursion?\n\n**Recursion** is when a function calls itself. It's like looking into a mirror that reflects another mirror‚Äîit goes on until something stops it.\n\n## Why Use Recursion?\n\nSome problems are naturally \"self-similar\"‚Äîthe solution involves solving smaller versions of the same problem:\n- **Factorial**: 5! = 5 √ó 4! (and 4! = 4 √ó 3!, etc.)\n- **File systems**: Folders contain folders\n- **Family trees**: People have parents who have parents\n\n## The Two Essential Parts\n\nEvery recursive function needs:\n\n### 1. Base Case (The Stop Sign)\n```python\nif n <= 1:\n    return 1  # STOP! Don't call yourself again.\n```\n\n### 2. Recursive Case (The Mirror)\n```python\nreturn n * factorial(n - 1)  # Call yourself with smaller input\n```\n\n## How Factorial Works\n\n```\nfactorial(5)\n  = 5 √ó factorial(4)\n    = 5 √ó 4 √ó factorial(3)\n      = 5 √ó 4 √ó 3 √ó factorial(2)\n        = 5 √ó 4 √ó 3 √ó 2 √ó factorial(1)\n          = 5 √ó 4 √ó 3 √ó 2 √ó 1  (BASE CASE!)\n        = 5 √ó 4 √ó 3 √ó 2\n      = 5 √ó 4 √ó 6\n    = 5 √ó 24\n  = 120 ‚úÖ\n```\n\n## Visual: The Call Stack\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ factorial(1) = 1 ‚îÇ ‚Üê Base case reached!\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ factorial(2) = 2 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ factorial(3) = 6 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ factorial(4) = 24‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ factorial(5) =120‚îÇ ‚Üê Final answer\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## ‚ö†Ô∏è Common Mistake: Infinite Recursion\n\n```python\ndef bad_factorial(n):\n    return n * bad_factorial(n - 1)  # NO BASE CASE!\n# This never stops ‚Üí Stack Overflow error!\n```\n\n---\n\n## üéØ Your Task\n\nImplement a recursive factorial function that calculates n! (n factorial).",
    "starter_code": "def factorial(n):\n    '''Calculate n! recursively'''\n    # Base case: 1! = 1 (or 0! = 1)\n    if n <= 1:\n        return 1\n    # Recursive case: n! = n √ó (n-1)!\n    return n * factorial(n - 1)\n\n# Test it\nresult = factorial(5)\nprint(f\"5! = {result}\")",
    "solution_code": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)\n\nresult = factorial(5)\nprint(f\"5! = {result}\")",
    "expected_output": "5! = 120"
  },
  "171": {
    "title": "Top N Items",
    "chapter_title": "Data Structures",
    "content": "# üèÜ Top N Items: Finding the Best (or Worst)\n\n## Why This Pattern is Everywhere\n\n\"Show me the top 10 products\", \"Who are our best 5 customers?\", \"What are the lowest 3 scores?\" These questions come up constantly in data analysis!\n\n## The Simple Solution: Sort + Slice\n\nPython makes this incredibly easy with two steps:\n1. **Sort** the data (ascending or descending)\n2. **Slice** to get just what you need\n\n```python\nscores = [78, 92, 65, 91, 88, 96, 73, 84]\n\n# Top 3 (highest first)\ntop_3 = sorted(scores, reverse=True)[:3]  # [96, 92, 91]\n\n# Bottom 3 (lowest first)\nbottom_3 = sorted(scores)[:3]  # [65, 73, 78]\n```\n\n## Breaking It Down\n\n```python\nsorted(scores, reverse=True)[:3]\n       ‚Üë            ‚Üë          ‚Üë\n   The list    Descending?   Take first 3\n```\n\n## Real-World Examples\n\n```python\n# Top 5 selling products\nproducts.sort(key=lambda p: p['sales'], reverse=True)\ntop_sellers = products[:5]\n\n# 10 most recent orders\norders.sort(key=lambda o: o['date'], reverse=True)\nrecent = orders[:10]\n\n# 3 cheapest hotels\nhotels.sort(key=lambda h: h['price'])\nbudget_options = hotels[:3]\n```\n\n## Using heapq for Large Data\n\nWhen you have MILLIONS of items but only need top 10, sorting everything is wasteful. Use `heapq`:\n\n```python\nimport heapq\n\n# Much faster for large lists!\ntop_3 = heapq.nlargest(3, scores)\nbottom_3 = heapq.nsmallest(3, scores)\n```\n\n## Performance Comparison\n\n| Method | 1 Million Items, Top 10 |\n|--------|------------------------|\n| Sort + Slice | ~500ms |\n| heapq.nlargest | ~10ms |\n\n---\n\n## üéØ Your Task\n\nFind the top 3 highest test scores from a class.",
    "starter_code": "# Class test scores\nscores = [78, 92, 65, 91, 88, 96, 73, 84]\n\n# Get the top 3 scores\n# Sort descending, then take first 3\ntop_3 = sorted(scores, reverse=True)[:3]\n\nprint(f\"Top 3: {top_3}\")",
    "solution_code": "scores = [78, 92, 65, 91, 88, 96, 73, 84]\ntop_3 = sorted(scores, reverse=True)[:3]\nprint(f\"Top 3: {top_3}\")",
    "expected_output": "Top 3: [96, 92, 91]"
  },
  "172": {
    "title": "List Deduplication",
    "chapter_title": "Data Structures",
    "content": "# üîÑ Remove Duplicates: Clean Data Fast\n\n## Why Duplicates Are a Problem\n\nDuplicate data causes all kinds of issues:\n- **Inflated counts**: \"We have 1000 customers!\" (Actually 700, with duplicates)\n- **Wasted processing**: Analyzing the same record twice\n- **Wrong statistics**: Average skewed by repeated values\n\n## The Quick Way: Convert to Set\n\nSets automatically remove duplicates (they can only contain unique values):\n\n```python\nitems = [1, 2, 2, 3, 3, 3, 4]\nunique = list(set(items))  # [1, 2, 3, 4]\n```\n\n**But there's a catch!** Sets don't preserve order.\n\n## Preserving Order: The dict.fromkeys Trick\n\n```python\nitems = ['apple', 'banana', 'apple', 'cherry', 'banana']\nunique = list(dict.fromkeys(items))\n# ['apple', 'banana', 'cherry'] - Original order preserved!\n```\n\n## How dict.fromkeys Works\n\n1. `dict.fromkeys(items)` creates a dict with items as keys\n2. Dictionary keys are unique AND ordered (Python 3.7+)\n3. `list()` extracts just the keys\n\n## Order Comparison\n\n```python\nitems = ['c', 'a', 'b', 'a', 'c', 'b']\n\n# Using set (order may change!)\nlist(set(items))  # Could be ['a', 'b', 'c'] or any order\n\n# Using dict.fromkeys (order preserved!)\nlist(dict.fromkeys(items))  # Always ['c', 'a', 'b']\n```\n\n## Real-World Applications\n\n```python\n# Remove duplicate customer emails\nemails = ['a@x.com', 'b@x.com', 'a@x.com', 'c@x.com']\nunique_emails = list(dict.fromkeys(emails))\n\n# Remove duplicate product views (keep first occurrence)\nviews = ['shoes', 'shirt', 'shoes', 'pants', 'shirt']\nunique_views = list(dict.fromkeys(views))\n```\n\n---\n\n## üéØ Your Task\n\nRemove duplicate fruits from a list while preserving the original order.",
    "starter_code": "# List with duplicates\nfruits = ['apple', 'banana', 'apple', 'cherry', 'banana', 'cherry', 'apple']\n\n# Remove duplicates while preserving order\nunique_fruits = list(dict.fromkeys(fruits))\n\nprint(unique_fruits)",
    "solution_code": "fruits = ['apple', 'banana', 'apple', 'cherry', 'banana', 'cherry', 'apple']\nunique_fruits = list(dict.fromkeys(fruits))\nprint(unique_fruits)",
    "expected_output": "['apple', 'banana', 'cherry']"
  },
  "173": {
    "title": "Word Counter",
    "chapter_title": "Data Structures",
    "content": "# üìù Word Counter: Text Analysis Basics\n\n## Why Count Words?\n\nWord counting is the foundation of text analysis:\n- **SEO**: Is this article long enough?\n- **Writing**: Track your daily word count\n- **Analytics**: What words appear most often?\n\n## The Simple Approach\n\n```python\ntext = \"Hello world hello\"\nwords = text.split()  # ['Hello', 'world', 'hello']\nword_count = len(words)  # 3\n```\n\n## Counting Unique Words\n\n```python\nwords = text.lower().split()  # Normalize case\nunique_words = set(words)\nunique_count = len(unique_words)\n```\n\n## Counting Word Frequency\n\n```python\nfrom collections import Counter\n\ntext = \"the quick brown fox jumps over the lazy dog\"\nword_counts = Counter(text.lower().split())\n# Counter({'the': 2, 'quick': 1, 'brown': 1, ...})\n\n# Most common words\nword_counts.most_common(3)  # [('the', 2), ('quick', 1), ('brown', 1)]\n```\n\n## Handling Punctuation\n\n```python\nimport re\n\n# Remove punctuation before counting\nclean_text = re.sub(r'[^\\w\\s]', '', text)\nwords = clean_text.lower().split()\n```\n\n## Real-World Use\n\n```python\ndef analyze_text(text):\n    words = text.lower().split()\n    return {\n        'total_words': len(words),\n        'unique_words': len(set(words)),\n        'avg_word_length': sum(len(w) for w in words) / len(words)\n    }\n```\n\n---\n\n## üéØ Your Task\n\nCount total and unique words in a text, and find the most common.",
    "starter_code": "from collections import Counter\n\ntext = '''Python is a great programming language.\nPython is easy to learn and Python is powerful.\nLearning Python opens many doors.'''\n\n# Count words\nwords = text.lower().split()\ntotal_words = len(words)\n\n# Unique words\nunique_words = len(set(words))\n\n# Most common\nword_freq = Counter(words)\nmost_common = word_freq.most_common(3)\n\nprint(f\"Total words: {total_words}\")\nprint(f\"Unique words: {unique_words}\")\nprint(f\"Most common: {most_common}\")",
    "solution_code": "from collections import Counter\n\ntext = '''Python is great. Python is powerful.'''\nwords = text.lower().split()\n\nprint(f\"Total: {len(words)}\")\nprint(f\"Unique: {len(set(words))}\")\nprint(f\"Top 3: {Counter(words).most_common(3)}\")",
    "expected_output": "Total words: 17\nUnique words: 11\nMost common: [('python', 4), ('is', 3), ('a', 1)]"
  },
  "174": {
    "title": "Merge Dictionaries",
    "chapter_title": "Data Structures",
    "content": "# üîÄ Merging Dictionaries: Combining Data Sources\n\n## Why Merge Dictionaries?\n\nIn real applications, data comes from multiple sources:\n- **User defaults** + **User preferences** ‚Üí Complete settings\n- **Base config** + **Environment overrides** ‚Üí Final config\n- **Product info** + **Inventory data** ‚Üí Complete product view\n\n## Python 3.9+: The Pipe Operator\n\nThe cleanest way to merge dictionaries:\n\n```python\ndefaults = {\"theme\": \"light\", \"lang\": \"en\"}\nuser_prefs = {\"theme\": \"dark\"}\n\nmerged = defaults | user_prefs\n# {\"theme\": \"dark\", \"lang\": \"en\"}\n```\n\n**Note:** Later dictionary values override earlier ones!\n\n## Python 3.5+: The Spread Operator\n\n```python\nmerged = {**defaults, **user_prefs}\n# {\"theme\": \"dark\", \"lang\": \"en\"}\n```\n\n## The update() Method\n\nModifies the original dictionary:\n\n```python\nsettings = {\"theme\": \"light\", \"lang\": \"en\"}\nsettings.update({\"theme\": \"dark\"})\n# settings is now {\"theme\": \"dark\", \"lang\": \"en\"}\n```\n\n## Order Matters!\n\nLater values override earlier ones:\n\n```python\ndict1 = {\"a\": 1, \"b\": 2}\ndict2 = {\"b\": 3, \"c\": 4}\n\nmerged = dict1 | dict2  # {\"a\": 1, \"b\": 3, \"c\": 4}\n#                               ‚Üë dict2's value wins!\n```\n\n## Real-World Example: Config Layers\n\n```python\n# Base configuration\nbase_config = {\n    \"debug\": False,\n    \"log_level\": \"INFO\",\n    \"max_retries\": 3\n}\n\n# Development overrides\ndev_config = {\n    \"debug\": True,\n    \"log_level\": \"DEBUG\"\n}\n\n# Final config for development\nconfig = base_config | dev_config\n# {\"debug\": True, \"log_level\": \"DEBUG\", \"max_retries\": 3}\n```\n\n---\n\n## üéØ Your Task\n\nMerge default settings with user preferences to create final settings.",
    "starter_code": "# Default application settings\ndefaults = {\n    \"theme\": \"light\",\n    \"font_size\": 14,\n    \"notifications\": True,\n    \"language\": \"en\"\n}\n\n# User preferences (overrides some defaults)\nuser_prefs = {\n    \"theme\": \"dark\",\n    \"font_size\": 16\n}\n\n# Merge: user preferences override defaults\nfinal_settings = defaults | user_prefs\n\nprint(\"Final settings:\")\nfor key, value in final_settings.items():\n    print(f\"  {key}: {value}\")",
    "solution_code": "defaults = {\n    \"theme\": \"light\",\n    \"font_size\": 14,\n    \"notifications\": True,\n    \"language\": \"en\"\n}\n\nuser_prefs = {\n    \"theme\": \"dark\",\n    \"font_size\": 16\n}\n\nfinal_settings = defaults | user_prefs\n\nprint(\"Final settings:\")\nfor key, value in final_settings.items():\n    print(f\"  {key}: {value}\")",
    "expected_output": "Final settings:\n  theme: dark\n  font_size: 16\n  notifications: True\n  language: en"
  },
  "175": {
    "title": "Nested Lookup",
    "chapter_title": "Data Structures",
    "content": "# üîç Nested Data Structures: Deep Lookups\n\n## Why Nested Structures?\n\nReal data is rarely flat. APIs, configs, and databases return nested structures:\n\n```python\nuser = {\n    \"profile\": {\n        \"name\": \"Alice\",\n        \"settings\": {\n            \"theme\": \"dark\",\n            \"notifications\": True\n        }\n    }\n}\n```\n\n## Accessing Nested Data\n\n```python\n# Step by step\nprofile = user[\"profile\"]\nsettings = profile[\"settings\"]\ntheme = settings[\"theme\"]\n\n# All at once\ntheme = user[\"profile\"][\"settings\"][\"theme\"]\n```\n\n## Safe Access (Avoid KeyError)\n\n```python\n# Risky - crashes if key missing\ntheme = user[\"profile\"][\"settings\"][\"theme\"]\n\n# Safe with .get()\ntheme = user.get(\"profile\", {}).get(\"settings\", {}).get(\"theme\", \"light\")\n\n# Or with try/except\ntry:\n    theme = user[\"profile\"][\"settings\"][\"theme\"]\nexcept KeyError:\n    theme = \"light\"\n```\n\n## Nested Lists\n\n```python\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Access: matrix[row][column]\ncenter = matrix[1][1]  # 5\n```\n\n## Real-World Example\n\n```python\n# API response\nresponse = {\n    \"data\": {\n        \"users\": [\n            {\"id\": 1, \"name\": \"Alice\"},\n            {\"id\": 2, \"name\": \"Bob\"}\n        ]\n    }\n}\n\n# Get first user's name\nfirst_user_name = response[\"data\"][\"users\"][0][\"name\"]  # \"Alice\"\n```\n\n---\n\n## üéØ Your Task\n\nNavigate a nested structure to extract specific values.",
    "starter_code": "# Complex nested structure\ndata = {\n    \"company\": \"TechCorp\",\n    \"departments\": {\n        \"engineering\": {\n            \"head\": \"Alice\",\n            \"teams\": {\n                \"frontend\": [\"Bob\", \"Charlie\"],\n                \"backend\": [\"Diana\", \"Eve\"]\n            }\n        },\n        \"sales\": {\n            \"head\": \"Frank\",\n            \"teams\": {\n                \"enterprise\": [\"Grace\", \"Henry\"]\n            }\n        }\n    }\n}\n\n# Navigate to extract values\ncompany = data[\"company\"]\neng_head = data[\"departments\"][\"engineering\"][\"head\"]\nfrontend_team = data[\"departments\"][\"engineering\"][\"teams\"][\"frontend\"]\nfirst_backend = data[\"departments\"][\"engineering\"][\"teams\"][\"backend\"][0]\n\nprint(f\"Company: {company}\")\nprint(f\"Engineering Head: {eng_head}\")\nprint(f\"Frontend Team: {frontend_team}\")\nprint(f\"First Backend Dev: {first_backend}\")",
    "solution_code": "data = {\n    \"company\": \"TechCorp\",\n    \"departments\": {\n        \"engineering\": {\"head\": \"Alice\", \"teams\": {\"backend\": [\"Diana\", \"Eve\"]}}\n    }\n}\n\nprint(f\"Head: {data['departments']['engineering']['head']}\")\nprint(f\"Backend[0]: {data['departments']['engineering']['teams']['backend'][0]}\")",
    "expected_output": "Company: TechCorp\nEngineering Head: Alice\nFrontend Team: ['Bob', 'Charlie']\nFirst Backend Dev: Diana"
  },
  "176": {
    "title": "Unique Elements",
    "chapter_title": "Data Structures",
    "content": "# üî∑ Sets: Finding Unique Elements\n\n## What is a Set?\n\nA **set** is an unordered collection of UNIQUE elements. No duplicates allowed!\n\n```python\nmy_set = {1, 2, 3, 2, 1}  # Duplicates automatically removed\nprint(my_set)  # {1, 2, 3}\n```\n\n## Why Use Sets?\n\n1. **Automatic deduplication**: No duplicates, ever\n2. **Fast membership testing**: `x in my_set` is O(1), not O(n)!\n3. **Set operations**: Union, intersection, difference\n\n## Set Operations\n\n```python\na = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\n\n# Union: all unique elements from both\na | b  # {1, 2, 3, 4, 5, 6}\n\n# Intersection: elements in both\na & b  # {3, 4}\n\n# Difference: in a but not b\na - b  # {1, 2}\n\n# Symmetric difference: in either, but not both\na ^ b  # {1, 2, 5, 6}\n```\n\n## Converting Between Types\n\n```python\n# List to set (removes duplicates)\nunique = set([1, 2, 2, 3, 3, 3])  # {1, 2, 3}\n\n# Set to list\nback_to_list = list(unique)  # [1, 2, 3] (order may vary!)\n```\n\n## Real-World Uses\n\n```python\n# Find unique customers\nunique_customers = set(order['customer_id'] for order in orders)\n\n# Find common tags between articles\ncommon_tags = article1_tags & article2_tags\n\n# Find users who bought A but not B\nonly_a = buyers_a - buyers_b\n```\n\n---\n\n## üéØ Your Task\n\nUse sets to find common and unique elements between two groups.",
    "starter_code": "# Two groups of students\nmath_students = {'Alice', 'Bob', 'Charlie', 'Diana'}\nscience_students = {'Bob', 'Diana', 'Eve', 'Frank'}\n\n# Students in both classes\nboth = math_students & science_students\n\n# Students only in math\nmath_only = math_students - science_students\n\n# All unique students\nall_students = math_students | science_students\n\nprint(f\"In both classes: {both}\")\nprint(f\"Only in math: {math_only}\")\nprint(f\"All students: {all_students}\")",
    "solution_code": "math_students = {'Alice', 'Bob', 'Charlie', 'Diana'}\nscience_students = {'Bob', 'Diana', 'Eve', 'Frank'}\n\nboth = math_students & science_students\nmath_only = math_students - science_students\nall_students = math_students | science_students\n\nprint(f\"In both: {both}\")\nprint(f\"Math only: {math_only}\")\nprint(f\"Total: {len(all_students)}\")",
    "expected_output": "In both classes: {'Bob', 'Diana'}\nOnly in math: {'Alice', 'Charlie'}\nAll students: {'Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'}"
  },
  "177": {
    "title": "Line Counter",
    "chapter_title": "File Handling",
    "content": "# üìÑ Counting Lines: File Analysis Basics\n\n## Why Count Lines?\n\nLine counting sounds simple, but it's the foundation of file analysis:\n- **Data files**: \"How many records in this CSV?\"\n- **Code review**: \"How big is this codebase?\"\n- **Log analysis**: \"How many errors occurred?\"\n\n## The Simple Approach\n\n```python\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()\n    count = len(lines)\n```\n\n## Memory-Efficient Approach\n\nFor HUGE files (gigabytes), don't load everything into memory:\n\n```python\nwith open('huge_file.txt', 'r') as f:\n    count = sum(1 for line in f)\n```\n\nThis reads one line at a time, counting as it goes!\n\n## Common Variations\n\n```python\n# Count non-empty lines\nnon_empty = sum(1 for line in f if line.strip())\n\n# Count lines containing a word\nwith_error = sum(1 for line in f if 'ERROR' in line)\n\n# Count lines longer than 80 characters\nlong_lines = sum(1 for line in f if len(line) > 80)\n```\n\n## Real-World Use Case\n\n```python\n# Analyze a log file\nwith open('app.log', 'r') as f:\n    total = 0\n    errors = 0\n    warnings = 0\n    \n    for line in f:\n        total += 1\n        if 'ERROR' in line:\n            errors += 1\n        elif 'WARNING' in line:\n            warnings += 1\n    \n    print(f\"Total lines: {total}\")\n    print(f\"Errors: {errors}\")\n    print(f\"Warnings: {warnings}\")\n```\n\n---\n\n## üéØ Your Task\n\nCount the total lines and non-empty lines in a text file.",
    "starter_code": "# Create a sample file\ncontent = '''Hello World\nThis is line 2\n\nLine 4 after empty line\nLast line'''\n\nwith open('sample.txt', 'w') as f:\n    f.write(content)\n\n# Count lines\nwith open('sample.txt', 'r') as f:\n    lines = f.readlines()\n    total_lines = len(lines)\n    non_empty = sum(1 for line in lines if line.strip())\n\nprint(f\"Total lines: {total_lines}\")\nprint(f\"Non-empty lines: {non_empty}\")",
    "solution_code": "content = '''Hello World\nThis is line 2\n\nLine 4 after empty line\nLast line'''\n\nwith open('sample.txt', 'w') as f:\n    f.write(content)\n\nwith open('sample.txt', 'r') as f:\n    lines = f.readlines()\n    total_lines = len(lines)\n    non_empty = sum(1 for line in lines if line.strip())\n\nprint(f\"Total lines: {total_lines}\")\nprint(f\"Non-empty lines: {non_empty}\")",
    "expected_output": "Total lines: 5\nNon-empty lines: 4"
  },
  "178": {
    "title": "Header Extractor",
    "chapter_title": "File Handling",
    "content": "# üìÑ CSV Header Extraction: Quick Data Inspection\n\n## Why Extract Headers First?\n\nBefore analyzing a CSV, you need to know:\n- What columns does it have?\n- Are the column names clean?\n- Is this the right file?\n\nReading headers first is fast‚Äîno need to load the whole file!\n\n## Quick Header Peek\n\n```python\nimport csv\n\nwith open('data.csv', 'r') as f:\n    reader = csv.reader(f)\n    headers = next(reader)  # Just the first row\n    \nprint(headers)\n# ['customer_id', 'name', 'email', 'signup_date']\n```\n\n## Using pandas\n\n```python\nimport pandas as pd\n\n# Read only the columns (no data!)\nheaders = pd.read_csv('data.csv', nrows=0).columns.tolist()\n\n# Or peek at first few rows\npreview = pd.read_csv('data.csv', nrows=5)\n```\n\n## Header Cleaning\n\nReal CSV headers are often messy:\n\n```python\n# Original: [' Customer ID ', 'Purchase Date', 'Amount ($)']\n\n# Clean them\nheaders = [h.strip().lower().replace(' ', '_') for h in headers]\n# ['customer_id', 'purchase_date', 'amount_($)']\n```\n\n## Checking Expected Columns\n\n```python\nrequired_columns = ['customer_id', 'email', 'amount']\nmissing = set(required_columns) - set(headers)\n\nif missing:\n    print(f\"Missing columns: {missing}\")\n```\n\n---\n\n## üéØ Your Task\n\nExtract and clean CSV headers from a file.",
    "starter_code": "import csv\n\n# Create a sample CSV\ncsv_content = '''Customer ID, Full Name, Email Address, Purchase Amount\n1, Alice Smith, alice@example.com, 99.99\n2, Bob Jones, bob@example.com, 149.99'''\n\nwith open('sample.csv', 'w') as f:\n    f.write(csv_content)\n\n# Extract headers\nwith open('sample.csv', 'r') as f:\n    reader = csv.reader(f)\n    raw_headers = next(reader)\n\n# Clean headers (strip spaces, lowercase, replace spaces with underscores)\nclean_headers = [h.strip().lower().replace(' ', '_') for h in raw_headers]\n\nprint(\"Raw headers:\", raw_headers)\nprint(\"Clean headers:\", clean_headers)",
    "solution_code": "import csv\n\ncsv_content = '''Customer ID, Full Name, Email\n1, Alice, alice@x.com'''\n\nwith open('sample.csv', 'w') as f:\n    f.write(csv_content)\n\nwith open('sample.csv', 'r') as f:\n    headers = next(csv.reader(f))\n    clean = [h.strip().lower().replace(' ', '_') for h in headers]\n    print(clean)",
    "expected_output": "Raw headers: ['Customer ID', ' Full Name', ' Email Address', ' Purchase Amount']\nClean headers: ['customer_id', 'full_name', 'email_address', 'purchase_amount']"
  },
  "179": {
    "title": "JSON Navigator",
    "chapter_title": "File Handling",
    "content": "# ÔøΩÔøΩÔ∏è Navigating JSON Data\n\n## What is JSON?\n\n**JSON** (JavaScript Object Notation) is THE format for data exchange. APIs, config files, databases‚ÄîJSON is everywhere!\n\n```json\n{\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"skills\": [\"Python\", \"SQL\", \"ML\"]\n}\n```\n\n## Why JSON is Popular\n\n- **Human readable**: Unlike binary formats\n- **Language agnostic**: Works in Python, JavaScript, Java, etc.\n- **Nested structures**: Can represent complex data\n\n## Loading JSON in Python\n\n```python\nimport json\n\n# From a file\nwith open('data.json', 'r') as f:\n    data = json.load(f)\n\n# From a string\njson_string = '{\"name\": \"Alice\", \"age\": 30}'\ndata = json.loads(json_string)\n```\n\n## Navigating Nested JSON\n\nReal APIs return deeply nested data:\n\n```python\nresponse = {\n    \"user\": {\n        \"profile\": {\n            \"name\": \"Alice\",\n            \"settings\": {\n                \"theme\": \"dark\"\n            }\n        }\n    }\n}\n\n# Navigate step by step\ntheme = response[\"user\"][\"profile\"][\"settings\"][\"theme\"]\n# \"dark\"\n```\n\n## Safe Navigation (Avoid Errors)\n\n```python\n# Risky - crashes if key missing\ntheme = data[\"user\"][\"settings\"][\"theme\"]\n\n# Safe - returns default if missing\ntheme = data.get(\"user\", {}).get(\"settings\", {}).get(\"theme\", \"light\")\n```\n\n## Real-World Example: API Response\n\n```python\n# Weather API response\nweather = {\n    \"location\": {\"city\": \"NYC\", \"country\": \"US\"},\n    \"current\": {\n        \"temp_f\": 72.5,\n        \"condition\": {\"text\": \"Sunny\", \"icon\": \"sunny.png\"}\n    }\n}\n\ncity = weather[\"location\"][\"city\"]  # \"NYC\"\ntemp = weather[\"current\"][\"temp_f\"]  # 72.5\ncondition = weather[\"current\"][\"condition\"][\"text\"]  # \"Sunny\"\n```\n\n---\n\n## üéØ Your Task\n\nNavigate a nested JSON structure to extract specific values.",
    "starter_code": "import json\n\n# Simulated API response\napi_response = '''\n{\n    \"user\": {\n        \"id\": 12345,\n        \"profile\": {\n            \"name\": \"Alice Johnson\",\n            \"email\": \"alice@example.com\"\n        },\n        \"preferences\": {\n            \"theme\": \"dark\",\n            \"notifications\": true\n        }\n    }\n}\n'''\n\n# Parse JSON\ndata = json.loads(api_response)\n\n# Navigate to extract values\nuser_name = data[\"user\"][\"profile\"][\"name\"]\nuser_email = data[\"user\"][\"profile\"][\"email\"]\ntheme = data[\"user\"][\"preferences\"][\"theme\"]\n\nprint(f\"Name: {user_name}\")\nprint(f\"Email: {user_email}\")\nprint(f\"Theme: {theme}\")",
    "solution_code": "import json\n\napi_response = '''\n{\n    \"user\": {\n        \"id\": 12345,\n        \"profile\": {\n            \"name\": \"Alice Johnson\",\n            \"email\": \"alice@example.com\"\n        },\n        \"preferences\": {\n            \"theme\": \"dark\",\n            \"notifications\": true\n        }\n    }\n}\n'''\n\ndata = json.loads(api_response)\nuser_name = data[\"user\"][\"profile\"][\"name\"]\nuser_email = data[\"user\"][\"profile\"][\"email\"]\ntheme = data[\"user\"][\"preferences\"][\"theme\"]\n\nprint(f\"Name: {user_name}\")\nprint(f\"Email: {user_email}\")\nprint(f\"Theme: {theme}\")",
    "expected_output": "Name: Alice Johnson\nEmail: alice@example.com\nTheme: dark"
  },
  "180": {
    "title": "Config Parser",
    "chapter_title": "File Handling",
    "content": "# ‚öôÔ∏è Config Files: Managing Application Settings\n\n## Why Config Files?\n\nHardcoding values in your code is BAD:\n```python\n# Don't do this!\ndb_host = \"localhost\"\ndb_port = 5432\napi_key = \"abc123\"\n```\n\nIf you need to change something, you have to modify code!\n\n## The Solution: External Config\n\nStore settings in a separate file, then load them:\n\n```python\n# config.ini\n[database]\nhost = localhost\nport = 5432\n\n[api]\nkey = abc123\n```\n\n## Python's ConfigParser\n\n```python\nfrom configparser import ConfigParser\n\nconfig = ConfigParser()\nconfig.read('config.ini')\n\n# Access values\nhost = config.get('database', 'host')\nport = config.getint('database', 'port')\n```\n\n## Type-Specific Getters\n\n```python\n# String\nconfig.get('section', 'key')\n\n# Integer\nconfig.getint('section', 'key')\n\n# Float\nconfig.getfloat('section', 'key')\n\n# Boolean\nconfig.getboolean('section', 'key')  # true/false, yes/no, 1/0\n```\n\n## Modern Alternative: .env Files\n\nFor Docker/cloud deployments, `.env` files are popular:\n\n```python\n# Using python-dotenv\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\napi_key = os.getenv('API_KEY')\n```\n\n## Best Practices\n\n‚úÖ Never commit secrets to git\n‚úÖ Use different configs for dev/staging/prod\n‚úÖ Have sensible defaults\n‚úÖ Validate config values on startup\n\n---\n\n## üéØ Your Task\n\nCreate and parse a configuration file for an application.",
    "starter_code": "from configparser import ConfigParser\n\n# Create a config file\nconfig_content = '''[database]\nhost = localhost\nport = 5432\nname = myapp_db\n\n[app]\ndebug = true\nmax_connections = 100\n'''\n\n# Write to file\nwith open('config.ini', 'w') as f:\n    f.write(config_content)\n\n# Read and parse\nconfig = ConfigParser()\nconfig.read('config.ini')\n\n# Extract values with correct types\ndb_host = config.get('database', 'host')\ndb_port = config.getint('database', 'port')\ndebug = config.getboolean('app', 'debug')\nmax_conn = config.getint('app', 'max_connections')\n\nprint(f\"Database: {db_host}:{db_port}\")\nprint(f\"Debug mode: {debug}\")\nprint(f\"Max connections: {max_conn}\")",
    "solution_code": "from configparser import ConfigParser\n\nconfig_content = '''[database]\nhost = localhost\nport = 5432\n\n[app]\ndebug = true\nmax_connections = 100\n'''\n\nwith open('config.ini', 'w') as f:\n    f.write(config_content)\n\nconfig = ConfigParser()\nconfig.read('config.ini')\n\nprint(f\"Database: {config.get('database', 'host')}:{config.getint('database', 'port')}\")\nprint(f\"Debug: {config.getboolean('app', 'debug')}\")",
    "expected_output": "Database: localhost:5432\nDebug mode: True\nMax connections: 100"
  },
  "181": {
    "title": "Pattern Finder",
    "chapter_title": "Modules",
    "content": "# üîç Regular Expressions: Pattern Matching Power\n\n## What Are Regular Expressions?\n\n**Regex** (regular expressions) are patterns that match text. They're like super-powered find-and-replace!\n\n```python\nimport re\n\n# Find all email addresses\npattern = r'\\w+@\\w+\\.\\w+'\nemails = re.findall(pattern, text)\n```\n\n## Basic Patterns\n\n| Pattern | Matches | Example |\n|---------|---------|---------|\n| `\\d` | Any digit | 0-9 |\n| `\\w` | Word character | a-z, A-Z, 0-9, _ |\n| `\\s` | Whitespace | space, tab, newline |\n| `.` | Any character | literally anything |\n| `+` | 1 or more | `\\d+` matches \"123\" |\n| `*` | 0 or more | `\\d*` matches \"\" or \"123\" |\n| `?` | 0 or 1 | `\\d?` matches \"\" or \"5\" |\n\n## Essential Functions\n\n```python\nimport re\n\ntext = \"Contact: john@example.com or call 555-1234\"\n\n# Find first match\nmatch = re.search(r'\\d{3}-\\d{4}', text)  # 555-1234\n\n# Find all matches\nall_nums = re.findall(r'\\d+', text)  # ['555', '1234']\n\n# Replace matches\nclean = re.sub(r'\\d', 'X', text)  # \"Contact: john@example.com or call XXX-XXXX\"\n\n# Check if pattern matches\nif re.match(r'^Contact', text):\n    print(\"Starts with 'Contact'\")\n```\n\n## Real-World Uses\n\n```python\n# Validate phone number\nif re.match(r'^\\d{3}-\\d{3}-\\d{4}$', phone):\n    print(\"Valid phone!\")\n\n# Extract all hashtags\nhashtags = re.findall(r'#\\w+', tweet)\n\n# Clean whitespace\nclean = re.sub(r'\\s+', ' ', messy_text)\n```\n\n---\n\n## üéØ Your Task\n\nUse regex to find all phone numbers in a text.",
    "starter_code": "import re\n\ntext = '''\nContact us:\nOffice: 555-123-4567\nMobile: 555-987-6543\nFax: 555-111-2222\n'''\n\n# Pattern for phone numbers: 3 digits - 3 digits - 4 digits\npattern = r'\\d{3}-\\d{3}-\\d{4}'\n\n# Find all matches\nphone_numbers = re.findall(pattern, text)\n\nprint(f\"Found {len(phone_numbers)} phone numbers:\")\nfor num in phone_numbers:\n    print(f\"  {num}\")",
    "solution_code": "import re\n\ntext = '''\nOffice: 555-123-4567\nMobile: 555-987-6543\nFax: 555-111-2222\n'''\n\npattern = r'\\d{3}-\\d{3}-\\d{4}'\nphone_numbers = re.findall(pattern, text)\n\nprint(f\"Found {len(phone_numbers)} phone numbers:\")\nfor num in phone_numbers:\n    print(f\"  {num}\")",
    "expected_output": "Found 3 phone numbers:\n  555-123-4567\n  555-987-6543\n  555-111-2222"
  },
  "182": {
    "title": "Batch Replace",
    "chapter_title": "Modules",
    "content": "# üîÑ Batch Text Replacement: Clean Data Fast\n\n## The Problem\n\nReal-world text is messy:\n- Inconsistent spellings: \"USA\", \"U.S.A.\", \"United States\"\n- Typos: \"recieve\", \"reciept\"\n- Format differences: \"New York\", \"new york\", \"NY\"\n\n## Simple Replacement\n\n```python\ntext = \"I recieve emails about USA and U.S.A.\"\n\n# One replacement\ntext = text.replace(\"recieve\", \"receive\")\n```\n\n## Batch Replacements with a Dictionary\n\n```python\nreplacements = {\n    \"USA\": \"United States\",\n    \"U.S.A.\": \"United States\",\n    \"U.S.\": \"United States\"\n}\n\nfor old, new in replacements.items():\n    text = text.replace(old, new)\n```\n\n## Using Regular Expressions for Complex Patterns\n\n```python\nimport re\n\n# Replace all variations at once\ntext = re.sub(r'U\\.?S\\.?A?\\.?', 'United States', text)\n```\n\n## Pandas Integration\n\n```python\n# Replace in DataFrame column\ndf['country'] = df['country'].replace({\n    'USA': 'United States',\n    'UK': 'United Kingdom',\n    'U.S.': 'United States'\n})\n```\n\n## Real-World Example: Data Standardization\n\n```python\n# Standardize company names\ncompany_mapping = {\n    'MSFT': 'Microsoft',\n    'Microsoft Corp': 'Microsoft',\n    'Microsoft Corporation': 'Microsoft',\n    'GOOG': 'Google',\n    'Alphabet': 'Google'\n}\n\ndf['company'] = df['company'].replace(company_mapping)\n```\n\n---\n\n## üéØ Your Task\n\nStandardize country names in a dataset using batch replacement.",
    "starter_code": "import pandas as pd\n\n# Messy country data\ndf = pd.DataFrame({\n    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'country': ['USA', 'U.S.A.', 'United States', 'UK', 'U.K.']\n})\n\nprint(\"Before standardization:\")\nprint(df)\n\n# Replacement mapping\ncountry_map = {\n    'USA': 'United States',\n    'U.S.A.': 'United States',\n    'UK': 'United Kingdom',\n    'U.K.': 'United Kingdom'\n}\n\n# Apply batch replacement\ndf['country'] = df['country'].replace(country_map)\n\nprint(\"\\nAfter standardization:\")\nprint(df)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'customer': ['Alice', 'Bob', 'Charlie'],\n    'country': ['USA', 'U.S.A.', 'UK']\n})\n\ncountry_map = {'USA': 'United States', 'U.S.A.': 'United States', 'UK': 'United Kingdom'}\ndf['country'] = df['country'].replace(country_map)\nprint(df)",
    "expected_output": "After standardization:\n  customer         country\n0    Alice   United States\n1      Bob   United States\n2  Charlie   United States\n3    Diana  United Kingdom\n4      Eve  United Kingdom"
  },
  "183": {
    "title": "Math Utilities",
    "chapter_title": "Modules",
    "content": "# üî¢ The Math Module: Beyond Basic Arithmetic\n\n## Why Use the Math Module?\n\nPython's basic operators (+, -, *, /) are great, but real data science needs more:\n- Square roots, logarithms, powers\n- Trigonometry\n- Rounding, floor, ceiling\n- Constants like œÄ and e\n\n## Essential Functions\n\n```python\nimport math\n\n# Roots and powers\nmath.sqrt(16)      # 4.0\nmath.pow(2, 10)    # 1024.0\n\n# Logarithms\nmath.log(100)      # 4.605 (natural log)\nmath.log10(100)    # 2.0 (log base 10)\nmath.log2(8)       # 3.0 (log base 2)\n\n# Rounding\nmath.floor(3.7)    # 3 (round down)\nmath.ceil(3.2)     # 4 (round up)\n\n# Constants\nmath.pi            # 3.14159...\nmath.e             # 2.71828...\n```\n\n## Real Data Science Uses\n\n### Compound Interest\n```python\n# Future value of investment\nprincipal = 1000\nrate = 0.05\nyears = 10\nfuture_value = principal * math.pow(1 + rate, years)\n# $1,628.89\n```\n\n### Logarithmic Scaling\n```python\n# For data that spans huge ranges (1 to 1,000,000)\nlog_scaled = [math.log10(x) for x in data]\n```\n\n### Distance Calculations\n```python\n# Euclidean distance between two points\ndistance = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n```\n\n## math vs numpy\n\n| Use math for | Use numpy for |\n|-------------|---------------|\n| Single values | Arrays of values |\n| Simple calculations | Vectorized operations |\n| Built-in (no install) | Needs installation |\n\n---\n\n## üéØ Your Task\n\nUse the math module to calculate compound interest growth.",
    "starter_code": "import math\n\n# Investment parameters\nprincipal = 10000  # Initial investment\nrate = 0.07        # 7% annual return\nyears = 20         # Investment period\n\n# Calculate future value using compound interest formula\n# FV = P √ó (1 + r)^n\nfuture_value = principal * math.pow(1 + rate, years)\n\n# Calculate total growth\ngrowth = future_value - principal\ngrowth_percent = (growth / principal) * 100\n\nprint(f\"Initial: ${principal:,.2f}\")\nprint(f\"After {years} years: ${future_value:,.2f}\")\nprint(f\"Total growth: ${growth:,.2f} ({growth_percent:.1f}%)\")",
    "solution_code": "import math\n\nprincipal = 10000\nrate = 0.07\nyears = 20\n\nfuture_value = principal * math.pow(1 + rate, years)\ngrowth = future_value - principal\ngrowth_percent = (growth / principal) * 100\n\nprint(f\"Initial: ${principal:,.2f}\")\nprint(f\"After {years} years: ${future_value:,.2f}\")\nprint(f\"Total growth: ${growth:,.2f} ({growth_percent:.1f}%)\")",
    "expected_output": "Initial: $10,000.00\nAfter 20 years: $38,696.84\nTotal growth: $28,696.84 (287.0%)"
  },
  "184": {
    "title": "Random Sampler",
    "chapter_title": "Modules",
    "content": "# üé≤ Random Sampling: The Foundation of Data Science\n\n## Why Random Sampling Matters\n\nData scientists constantly sample data:\n- **A/B testing**: Randomly assign users to groups\n- **Training ML**: Shuffle data before splitting\n- **Statistics**: Sample from population\n- **Simulations**: Generate random scenarios\n\n## The random Module\n\n```python\nimport random\n\n# Random float between 0 and 1\nrandom.random()  # 0.7234...\n\n# Random integer in range\nrandom.randint(1, 100)  # 42\n\n# Random choice from list\nrandom.choice(['a', 'b', 'c'])  # 'b'\n\n# Multiple random choices (with replacement)\nrandom.choices(['a', 'b', 'c'], k=5)  # ['a', 'c', 'c', 'b', 'a']\n\n# Random sample (without replacement)\nrandom.sample(range(100), 10)  # [23, 67, 12, ...]\n\n# Shuffle a list in place\nrandom.shuffle(my_list)\n```\n\n## Reproducibility with Seeds\n\nFor reproducible results (same \"random\" every time):\n\n```python\nrandom.seed(42)  # Set the seed\nrandom.random()  # Always 0.6394...\n```\n\n## Real-World Examples\n\n### A/B Test Assignment\n```python\ndef assign_ab_group(user_id):\n    random.seed(user_id)  # Consistent per user\n    return 'A' if random.random() < 0.5 else 'B'\n```\n\n### Train/Test Split\n```python\ndata = list(range(100))\nrandom.shuffle(data)\ntrain = data[:80]\ntest = data[80:]\n```\n\n### Monte Carlo Simulation\n```python\nhits = 0\nfor _ in range(10000):\n    x, y = random.random(), random.random()\n    if x*x + y*y <= 1:\n        hits += 1\npi_estimate = 4 * hits / 10000\n```\n\n---\n\n## üéØ Your Task\n\nCreate a random sample of lottery numbers (6 unique numbers from 1-49).",
    "starter_code": "import random\n\n# Set seed for reproducibility\nrandom.seed(42)\n\n# Generate 6 unique lottery numbers from 1-49\n# Use random.sample() for unique values\nlottery_numbers = random.sample(range(1, 50), 6)\n\n# Sort them for display\nlottery_numbers.sort()\n\nprint(f\"Your lottery numbers: {lottery_numbers}\")",
    "solution_code": "import random\n\nrandom.seed(42)\nlottery_numbers = random.sample(range(1, 50), 6)\nlottery_numbers.sort()\nprint(f\"Your lottery numbers: {lottery_numbers}\")",
    "expected_output": "Your lottery numbers: [4, 10, 18, 20, 25, 39]"
  },
  "185": {
    "title": "Time Calculator",
    "chapter_title": "Modules",
    "content": "# ‚è∞ datetime: Calculate Time Like a Pro\n\n## Why datetime Matters\n\nTime calculations are everywhere:\n- \"How old is this user?\"\n- \"When does the subscription expire?\"\n- \"What's the average response time?\"\n\n## Creating Dates and Times\n\n```python\nfrom datetime import datetime, date, timedelta\n\n# Current time\nnow = datetime.now()  # 2024-01-15 14:30:00\n\n# Specific date\nbirthday = date(1990, 5, 15)\n\n# Specific datetime\nmeeting = datetime(2024, 3, 20, 14, 30)\n```\n\n## Time Arithmetic\n\n```python\nfrom datetime import timedelta\n\n# Add/subtract time\ntomorrow = now + timedelta(days=1)\nlast_week = now - timedelta(weeks=1)\nin_90_minutes = now + timedelta(hours=1, minutes=30)\n```\n\n## Calculate Differences\n\n```python\nstart = datetime(2024, 1, 1)\nend = datetime(2024, 3, 15)\n\ndiff = end - start\nprint(diff.days)  # 74 days\nprint(diff.total_seconds())  # 6393600.0\n```\n\n## Formatting Dates\n\n```python\nnow = datetime.now()\n\n# To string\nnow.strftime(\"%Y-%m-%d\")      # \"2024-01-15\"\nnow.strftime(\"%B %d, %Y\")     # \"January 15, 2024\"\n\n# From string\ndatetime.strptime(\"2024-01-15\", \"%Y-%m-%d\")\n```\n\n## Real-World Example\n\n```python\n# Calculate age\nbirth = date(1990, 5, 15)\ntoday = date.today()\nage = (today - birth).days // 365\n```\n\n---\n\n## üéØ Your Task\n\nCalculate the number of days between two dates.",
    "starter_code": "from datetime import datetime, timedelta\n\n# Define two events\nproject_start = datetime(2024, 1, 15)\ndeadline = datetime(2024, 4, 30)\n\n# Calculate difference\ndiff = deadline - project_start\ndays_remaining = diff.days\n\n# Add buffer time\nnew_deadline = deadline + timedelta(weeks=2)\n\nprint(f\"Project Start: {project_start.strftime('%B %d, %Y')}\")\nprint(f\"Original Deadline: {deadline.strftime('%B %d, %Y')}\")\nprint(f\"Days for project: {days_remaining}\")\nprint(f\"Extended Deadline: {new_deadline.strftime('%B %d, %Y')}\")",
    "solution_code": "from datetime import datetime, timedelta\n\nstart = datetime(2024, 1, 15)\nend = datetime(2024, 4, 30)\n\ndiff = end - start\nprint(f\"Days: {diff.days}\")\nprint(f\"Extended: {(end + timedelta(weeks=2)).strftime('%Y-%m-%d')}\")",
    "expected_output": "Project Start: January 15, 2024\nOriginal Deadline: April 30, 2024\nDays for project: 106\nExtended Deadline: May 14, 2024"
  },
  "186": {
    "title": "Counter Analysis",
    "chapter_title": "Modules",
    "content": "# üìä Counter: Count Anything Instantly\n\n## The Counting Problem\n\nYou have a list and need to count occurrences:\n\n```python\n# The tedious way\ncounts = {}\nfor item in items:\n    if item in counts:\n        counts[item] += 1\n    else:\n        counts[item] = 1\n```\n\n## The Counter Way\n\n```python\nfrom collections import Counter\n\nitems = ['apple', 'banana', 'apple', 'cherry', 'banana', 'apple']\ncounts = Counter(items)\n# Counter({'apple': 3, 'banana': 2, 'cherry': 1})\n```\n\nOne line does it all!\n\n## Counter Superpowers\n\n```python\nfrom collections import Counter\n\nwords = ['the', 'quick', 'brown', 'the', 'fox', 'the']\ncounter = Counter(words)\n\n# Most common elements\ncounter.most_common(2)  # [('the', 3), ('quick', 1)]\n\n# Access counts\ncounter['the']  # 3\ncounter['dog']  # 0 (missing = 0, not error!)\n\n# Add more items\ncounter.update(['the', 'lazy', 'dog'])\n\n# Combined counts\nc1 = Counter(['a', 'b', 'a'])\nc2 = Counter(['b', 'c', 'b'])\nc1 + c2  # Counter({'b': 3, 'a': 2, 'c': 1})\n```\n\n## Real-World Uses\n\n```python\n# Word frequency in a document\nword_freq = Counter(document.lower().split())\n\n# Most purchased products\ntop_products = Counter(order['product'] for order in orders).most_common(10)\n\n# Character frequency (for encryption analysis)\nchar_freq = Counter(message)\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze word frequency in a sentence using Counter.",
    "starter_code": "from collections import Counter\n\nsentence = \"the quick brown fox jumps over the lazy dog the fox was quick\"\n\n# Split into words and count\nwords = sentence.lower().split()\nword_counts = Counter(words)\n\n# Most common words\nprint(\"Word frequency analysis:\")\nfor word, count in word_counts.most_common(5):\n    print(f\"  '{word}': {count} times\")\n\nprint(f\"\\nTotal unique words: {len(word_counts)}\")",
    "solution_code": "from collections import Counter\n\nsentence = \"the quick brown fox jumps over the lazy dog the fox was quick\"\nwords = sentence.lower().split()\nword_counts = Counter(words)\n\nprint(\"Top 5 words:\")\nfor word, count in word_counts.most_common(5):\n    print(f\"  '{word}': {count}\")",
    "expected_output": "Word frequency analysis:\n  'the': 3 times\n  'quick': 2 times\n  'fox': 2 times\n  'brown': 1 times\n  'jumps': 1 times"
  },
  "187": {
    "title": "Path Operations",
    "chapter_title": "Modules",
    "content": "# üìÇ Path Operations: Navigate Files Safely\n\n## Why pathlib?\n\nDealing with file paths is tricky:\n- Windows uses `\\`, Mac/Linux use `/`\n- Joining paths manually is error-prone\n- String manipulation is clunky\n\n`pathlib` solves all this!\n\n## Basic Path Operations\n\n```python\nfrom pathlib import Path\n\n# Current directory\ncurrent = Path.cwd()\n\n# Home directory\nhome = Path.home()\n\n# Create a path\ndata_path = Path(\"/Users/me/data/file.csv\")\n```\n\n## Path Components\n\n```python\npath = Path(\"/Users/me/data/report.csv\")\n\npath.name       # \"report.csv\"\npath.stem       # \"report\"\npath.suffix     # \".csv\"\npath.parent     # Path(\"/Users/me/data\")\npath.parts      # ('/', 'Users', 'me', 'data', 'report.csv')\n```\n\n## Joining Paths\n\n```python\n# Use / operator (cross-platform!)\nbase = Path(\"/Users/me/data\")\nfile_path = base / \"reports\" / \"q1.csv\"\n# Path(\"/Users/me/data/reports/q1.csv\")\n```\n\n## Checking Paths\n\n```python\npath = Path(\"data/file.csv\")\n\npath.exists()       # Does it exist?\npath.is_file()      # Is it a file?\npath.is_dir()       # Is it a directory?\n```\n\n## Listing Directory Contents\n\n```python\n# All files in directory\nfor file in Path(\"data\").iterdir():\n    print(file)\n\n# Only .csv files\nfor csv_file in Path(\"data\").glob(\"*.csv\"):\n    print(csv_file)\n```\n\n---\n\n## üéØ Your Task\n\nUse pathlib to explore file paths and their components.",
    "starter_code": "from pathlib import Path\n\n# Create a path\nfile_path = Path(\"/Users/data/reports/sales_2024.csv\")\n\n# Extract components\nprint(f\"Full path: {file_path}\")\nprint(f\"File name: {file_path.name}\")\nprint(f\"Stem (no extension): {file_path.stem}\")\nprint(f\"Extension: {file_path.suffix}\")\nprint(f\"Parent directory: {file_path.parent}\")\n\n# Join paths\nbase_dir = Path(\"/Users/data\")\nnew_file = base_dir / \"exports\" / \"output.xlsx\"\nprint(f\"\\nNew joined path: {new_file}\")",
    "solution_code": "from pathlib import Path\n\npath = Path(\"/Users/data/reports/sales.csv\")\nprint(f\"Name: {path.name}\")\nprint(f\"Stem: {path.stem}\")\nprint(f\"Parent: {path.parent}\")",
    "expected_output": "Full path: /Users/data/reports/sales_2024.csv\nFile name: sales_2024.csv\nStem (no extension): sales_2024\nExtension: .csv\nParent directory: /Users/data/reports"
  },
  "188": {
    "title": "Combinations Generator",
    "chapter_title": "Modules",
    "content": "# üîÑ Itertools: Combinations and Permutations\n\n## When Do You Need Combinations?\n\n- \"What are all possible pairs from this team?\"\n- \"List every 3-topping pizza combination\"\n- \"Generate all possible test scenarios\"\n\n## Combinations vs Permutations\n\n| Type | Order Matters? | Example |\n|------|---------------|---------|\n| Combination | No | Pizza toppings |\n| Permutation | Yes | Password characters |\n\n```python\nfrom [A, B, C], pick 2:\nCombinations: AB, AC, BC       (3 total)\nPermutations: AB, BA, AC, CA, BC, CB (6 total)\n```\n\n## Using itertools\n\n```python\nfrom itertools import combinations, permutations\n\nitems = ['A', 'B', 'C']\n\n# All 2-item combinations (order doesn't matter)\nlist(combinations(items, 2))\n# [('A', 'B'), ('A', 'C'), ('B', 'C')]\n\n# All 2-item permutations (order matters)\nlist(permutations(items, 2))\n# [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]\n```\n\n## Counting Without Generating\n\n```python\nfrom math import comb, perm\n\ncomb(10, 3)  # 120 ways to choose 3 from 10\nperm(10, 3)  # 720 arrangements of 3 from 10\n```\n\n## Real-World Uses\n\n```python\n# All possible team pairs\nfor team1, team2 in combinations(teams, 2):\n    schedule_match(team1, team2)\n\n# Test all feature combinations\nfor features in combinations(['A', 'B', 'C', 'D'], 2):\n    run_test(features)\n```\n\n---\n\n## üéØ Your Task\n\nGenerate all possible 2-person teams from a group.",
    "starter_code": "from itertools import combinations\n\n# Members available\nmembers = ['Alice', 'Bob', 'Charlie', 'Diana']\n\n# Generate all possible pairs\npairs = list(combinations(members, 2))\n\nprint(f\"All possible 2-person teams ({len(pairs)} total):\")\nfor i, pair in enumerate(pairs, 1):\n    print(f\"  Team {i}: {pair[0]} & {pair[1]}\")",
    "solution_code": "from itertools import combinations\n\nmembers = ['Alice', 'Bob', 'Charlie', 'Diana']\npairs = list(combinations(members, 2))\n\nprint(f\"All pairs ({len(pairs)} total):\")\nfor pair in pairs:\n    print(f\"  {pair[0]} & {pair[1]}\")",
    "expected_output": "All possible 2-person teams (6 total):\n  Team 1: Alice & Bob\n  Team 2: Alice & Charlie\n  Team 3: Alice & Diana\n  Team 4: Bob & Charlie\n  Team 5: Bob & Diana\n  Team 6: Charlie & Diana"
  },
  "189": {
    "title": "Custom Arrays",
    "chapter_title": "NumPy",
    "content": "# üé® Creating Custom Arrays: Beyond Basics\n\n## Why Custom Arrays?\n\nNumPy offers many ways to create arrays for specific needs:\n- Initialize with zeros for accumulators\n- Fill with specific values for testing\n- Create ranges for sequences\n- Build identity matrices for linear algebra\n\n## Array Creators\n\n### Zeros and Ones\n```python\nimport numpy as np\n\nzeros = np.zeros(5)           # [0, 0, 0, 0, 0]\nones = np.ones((2, 3))        # 2x3 matrix of ones\nfull = np.full((3, 3), 7)     # 3x3 matrix filled with 7\n```\n\n### Ranges\n```python\n# Integer range\nnp.arange(0, 10, 2)          # [0, 2, 4, 6, 8]\n\n# Evenly spaced floats\nnp.linspace(0, 1, 5)         # [0, 0.25, 0.5, 0.75, 1]\n```\n\n### Special Matrices\n```python\nnp.eye(3)                    # 3x3 identity matrix\nnp.diag([1, 2, 3])           # Diagonal matrix\n```\n\n### Random Arrays\n```python\nnp.random.rand(3, 3)         # 3x3 uniform random [0,1)\nnp.random.randn(3, 3)        # 3x3 standard normal\nnp.random.randint(1, 10, (3, 3))  # 3x3 random integers\n```\n\n## Copying vs Referencing\n\n```python\na = np.array([1, 2, 3])\nb = a                 # Reference (same data!)\nc = a.copy()          # True copy (independent)\n\na[0] = 999\nprint(b)  # [999, 2, 3] - changed!\nprint(c)  # [1, 2, 3] - unchanged\n```\n\n---\n\n## üéØ Your Task\n\nCreate various custom arrays for data initialization.",
    "starter_code": "import numpy as np\n\n# Create different types of arrays\nzeros = np.zeros((3, 4))       # 3x4 matrix of zeros\nones = np.ones(5)              # 1D array of ones\nfilled = np.full((2, 2), 42)   # 2x2 filled with 42\n\n# Create sequences\nrange_array = np.arange(0, 10, 2)\nlinspace_array = np.linspace(0, 1, 5)\n\n# Create identity matrix\nidentity = np.eye(3)\n\nprint(\"Zeros (3x4):\")\nprint(zeros)\nprint(\"\\nSequence (0 to 10, step 2):\", range_array)\nprint(\"\\nLinspace (0 to 1, 5 points):\", linspace_array)\nprint(\"\\nIdentity (3x3):\")\nprint(identity)",
    "solution_code": "import numpy as np\n\nprint(\"Zeros:\", np.zeros(5))\nprint(\"Range:\", np.arange(0, 10, 2))\nprint(\"Linspace:\", np.linspace(0, 1, 5))",
    "expected_output": "Zeros (3x4):\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]"
  },
  "190": {
    "title": "Array Reshaping",
    "chapter_title": "NumPy",
    "content": "# üîÑ Array Reshaping: Change the View, Not the Data\n\n## Why Reshape?\n\nData comes in one shape, but you need another:\n- Flatten a 2D image for ML input\n- Convert 1D data into a matrix\n- Prepare data for neural network layers\n\n## Basic Reshaping\n\n```python\nimport numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6])\n\n# Reshape to 2x3\nmatrix = arr.reshape(2, 3)\n# [[1, 2, 3],\n#  [4, 5, 6]]\n\n# Reshape to 3x2\nmatrix2 = arr.reshape(3, 2)\n# [[1, 2],\n#  [3, 4],\n#  [5, 6]]\n```\n\n## Using -1 for Auto-Calculation\n\n```python\narr = np.arange(12)  # 0-11\n\n# \"I want 3 rows, figure out columns\"\narr.reshape(3, -1)  # 3x4\n\n# \"I want 4 columns, figure out rows\"\narr.reshape(-1, 4)  # 3x4\n```\n\n## Flatten and Ravel\n\n```python\nmatrix = np.array([[1, 2], [3, 4]])\n\nmatrix.flatten()  # [1, 2, 3, 4] - copy\nmatrix.ravel()    # [1, 2, 3, 4] - view (faster)\n```\n\n## Transpose\n\n```python\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6]])\n\nmatrix.T  # or np.transpose(matrix)\n# [[1, 4],\n#  [2, 5],\n#  [3, 6]]\n```\n\n## Important Rule\n\nTotal elements must stay the same!\n```python\narr = np.arange(12)  # 12 elements\narr.reshape(5, 3)    # ERROR! 5√ó3=15 ‚â† 12\n```\n\n---\n\n## üéØ Your Task\n\nReshape a 1D array into different 2D configurations.",
    "starter_code": "import numpy as np\n\n# Create 1D array with 12 elements\narr = np.arange(1, 13)\nprint(\"Original (1D):\", arr)\n\n# Reshape to 3x4\nmatrix_3x4 = arr.reshape(3, 4)\nprint(\"\\n3x4 matrix:\")\nprint(matrix_3x4)\n\n# Reshape to 4x3\nmatrix_4x3 = arr.reshape(4, 3)\nprint(\"\\n4x3 matrix:\")\nprint(matrix_4x3)\n\n# Reshape to 2x6 using -1\nmatrix_2x = arr.reshape(2, -1)\nprint(\"\\n2x? (auto-calculated):\")\nprint(matrix_2x)\n\n# Flatten back\nflat = matrix_3x4.flatten()\nprint(\"\\nFlattened back:\", flat)",
    "solution_code": "import numpy as np\n\narr = np.arange(1, 13)\nprint(\"3x4:\", arr.reshape(3, 4))\nprint(\"4x3:\", arr.reshape(4, 3))\nprint(\"Flat:\", arr.reshape(3, 4).flatten())",
    "expected_output": "Original (1D): [ 1  2  3  4  5  6  7  8  9 10 11 12]"
  },
  "191": {
    "title": "Element-wise Math",
    "chapter_title": "NumPy",
    "content": "# ‚ûï Element-wise Operations: Math on Entire Arrays\n\n## Why Element-wise Matters\n\nIn regular Python:\n```python\n# To double each element...\nresult = []\nfor x in numbers:\n    result.append(x * 2)\n```\n\nIn NumPy:\n```python\nresult = numbers * 2  # One line, 100x faster!\n```\n\n## Basic Element-wise Operations\n\n```python\nimport numpy as np\n\na = np.array([1, 2, 3, 4, 5])\n\na + 10      # [11, 12, 13, 14, 15]\na * 2       # [2, 4, 6, 8, 10]\na ** 2      # [1, 4, 9, 16, 25]\na / 2       # [0.5, 1.0, 1.5, 2.0, 2.5]\n```\n\n## Array + Array\n\n```python\na = np.array([1, 2, 3])\nb = np.array([10, 20, 30])\n\na + b       # [11, 22, 33]\na * b       # [10, 40, 90]\n```\n\n## Universal Functions (ufuncs)\n\n```python\nnp.sqrt(a)      # Square root of each\nnp.exp(a)       # e^x for each\nnp.log(a)       # Natural log of each\nnp.abs(a)       # Absolute value of each\nnp.sin(a)       # Sine of each\n```\n\n## Broadcasting\n\nNumPy automatically \"broadcasts\" operations:\n\n```python\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6]])\n\n# Add 10 to every element\nmatrix + 10\n\n# Add different values to each row\nmatrix + np.array([100, 200, 300])\n# [[101, 202, 303],\n#  [104, 205, 306]]\n```\n\n---\n\n## üéØ Your Task\n\nPerform element-wise calculations on arrays.",
    "starter_code": "import numpy as np\n\n# Temperature data in Celsius\ncelsius = np.array([0, 10, 20, 30, 40])\n\n# Element-wise conversion to Fahrenheit\n# Formula: F = C * 9/5 + 32\nfahrenheit = celsius * 9/5 + 32\n\nprint(\"Celsius:\", celsius)\nprint(\"Fahrenheit:\", fahrenheit)\n\n# Square each value\nsquared = celsius ** 2\nprint(\"\\nSquared:\", squared)\n\n# Apply numpy universal function\nsqrt = np.sqrt(celsius.astype(float))\nprint(\"Square root:\", sqrt)",
    "solution_code": "import numpy as np\n\ncelsius = np.array([0, 10, 20, 30, 40])\nfahrenheit = celsius * 9/5 + 32\n\nprint(\"Celsius:\", celsius)\nprint(\"Fahrenheit:\", fahrenheit)",
    "expected_output": "Celsius: [ 0 10 20 30 40]\nFahrenheit: [ 32.  50.  68.  86. 104.]"
  },
  "192": {
    "title": "Array Comparison",
    "chapter_title": "NumPy",
    "content": "# ‚öñÔ∏è Array Comparisons: Boolean Masks for Filtering\n\n## Why Comparison Operations?\n\nNeed to filter data? NumPy comparisons create **boolean masks**:\n\n```python\ndata = np.array([10, 25, 30, 15, 40])\nmask = data > 20  # [False, True, True, False, True]\nfiltered = data[mask]  # [25, 30, 40]\n```\n\n## Comparison Operators\n\n```python\nimport numpy as np\n\narr = np.array([1, 5, 10, 15, 20])\n\narr > 10    # [False, False, False, True, True]\narr == 5    # [False, True, False, False, False]\narr != 10   # [True, True, False, True, True]\narr <= 10   # [True, True, True, False, False]\n```\n\n## Combining Conditions\n\n```python\n# AND: both must be true\n(arr > 5) & (arr < 15)  # [False, False, True, False, False]\n\n# OR: either can be true\n(arr < 5) | (arr > 15)  # [True, False, False, False, True]\n\n# NOT: invert\n~(arr > 10)  # [True, True, True, False, False]\n```\n\n## Counting and Finding\n\n```python\n# How many match?\n(arr > 10).sum()  # 2\n\n# Any match?\n(arr > 10).any()  # True\n\n# All match?\n(arr > 0).all()   # True\n\n# Where do matches occur?\nnp.where(arr > 10)  # (array([3, 4]),)\n```\n\n## Complete Example\n\n```python\nprices = np.array([25, 150, 80, 200, 45])\n\n# Find items between $50-$150\naffordable = (prices >= 50) & (prices <= 150)\n# [False, True, True, False, False]\n\n# Get those items\nresult = prices[affordable]  # [150, 80]\n```\n\n---\n\n## üéØ Your Task\n\nUse comparisons to filter and analyze array data.",
    "starter_code": "import numpy as np\n\n# Sales data\nsales = np.array([45, 120, 85, 200, 150, 30, 175, 90])\n\n# Find high sales (> 100)\nhigh_sales_mask = sales > 100\nhigh_sales = sales[high_sales_mask]\n\n# Count how many\ncount = high_sales_mask.sum()\n\n# Find indices of high sales\nindices = np.where(sales > 100)[0]\n\nprint(f\"All sales: {sales}\")\nprint(f\"High sales mask: {high_sales_mask}\")\nprint(f\"High sales values: {high_sales}\")\nprint(f\"Count of high sales: {count}\")\nprint(f\"Indices of high sales: {indices}\")",
    "solution_code": "import numpy as np\n\nsales = np.array([45, 120, 85, 200, 150])\nhigh = sales > 100\nprint(f\"High sales: {sales[high]}\")\nprint(f\"Count: {high.sum()}\")",
    "expected_output": "High sales mask: [False  True False  True  True False  True False]\nHigh sales values: [120 200 150 175]\nCount of high sales: 4"
  },
  "193": {
    "title": "Filter Outliers",
    "chapter_title": "NumPy",
    "content": "# üéØ Filtering Outliers with NumPy\n\n## What Are Outliers?\n\nOutliers are values that are unusually far from the typical range. They can skew your analysis:\n\n```\nNormal data: [10, 12, 11, 13, 10, 12, 11]\nWith outlier: [10, 12, 11, 13, 10, 12, 1000]  ‚Üê 1000 is trouble!\n```\n\n## Two Common Methods\n\n### 1. IQR Method\nFlag values below Q1 - 1.5√óIQR or above Q3 + 1.5√óIQR\n\n```python\nq1 = np.percentile(data, 25)\nq3 = np.percentile(data, 75)\niqr = q3 - q1\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n```\n\n### 2. Z-Score Method\nFlag values more than 2-3 standard deviations from mean\n\n```python\nz_scores = (data - data.mean()) / data.std()\noutliers = np.abs(z_scores) > 2\n```\n\n## NumPy Boolean Filtering\n\n```python\nimport numpy as np\n\ndata = np.array([10, 12, 11, 1000, 13, 10])\n\n# Create boolean mask\nmask = data < 100  # [True, True, True, False, True, True]\n\n# Apply mask to filter\nclean_data = data[mask]  # [10, 12, 11, 13, 10]\n```\n\n## Complete Example\n\n```python\n# Remove outliers using IQR\nq1, q3 = np.percentile(data, [25, 75])\niqr = q3 - q1\nmask = (data >= q1 - 1.5*iqr) & (data <= q3 + 1.5*iqr)\nclean = data[mask]\n```\n\n---\n\n## üéØ Your Task\n\nRemove outliers from a dataset using the IQR method.",
    "starter_code": "import numpy as np\n\n# Sales data with some outliers\nsales = np.array([100, 150, 120, 1500, 130, 110, 140, 125, 2000, 135])\n\n# Calculate IQR\nq1 = np.percentile(sales, 25)\nq3 = np.percentile(sales, 75)\niqr = q3 - q1\n\n# Define bounds\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# Filter outliers\nmask = (sales >= lower_bound) & (sales <= upper_bound)\nclean_sales = sales[mask]\noutliers = sales[~mask]\n\nprint(f\"Original: {sales}\")\nprint(f\"Bounds: {lower_bound:.0f} to {upper_bound:.0f}\")\nprint(f\"Clean data: {clean_sales}\")\nprint(f\"Outliers removed: {outliers}\")",
    "solution_code": "import numpy as np\n\nsales = np.array([100, 150, 120, 1500, 130, 110, 140, 125, 2000, 135])\n\nq1 = np.percentile(sales, 25)\nq3 = np.percentile(sales, 75)\niqr = q3 - q1\n\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\nmask = (sales >= lower_bound) & (sales <= upper_bound)\nclean_sales = sales[mask]\n\nprint(f\"Clean data: {clean_sales}\")\nprint(f\"Outliers: {sales[~mask]}\")",
    "expected_output": "Clean data: [100 150 120 130 110 140 125 135]\nOutliers removed: [1500 2000]"
  },
  "194": {
    "title": "Conditional Replace",
    "chapter_title": "NumPy",
    "content": "# üîÑ Conditional Replacement: Modify Data Based on Conditions\n\n## Why Conditional Replace?\n\nSometimes you need to change values that meet certain criteria:\n- Cap outliers at a maximum\n- Replace negative values with zero\n- Clean up invalid data\n\n## Using np.where\n\n```python\nimport numpy as np\n\narr = np.array([1, -2, 3, -4, 5])\n\n# Replace negative values with 0\nresult = np.where(arr < 0, 0, arr)\n# [1, 0, 3, 0, 5]\n```\n\n## np.where Syntax\n\n```python\nnp.where(condition, value_if_true, value_if_false)\n```\n\n## Examples\n\n```python\n# Cap values at 100\nscores = np.array([85, 110, 95, 130, 78])\ncapped = np.where(scores > 100, 100, scores)\n# [85, 100, 95, 100, 78]\n\n# Binary classification\nages = np.array([15, 22, 17, 30, 16])\nadult = np.where(ages >= 18, 'adult', 'minor')\n# ['minor', 'adult', 'minor', 'adult', 'minor']\n```\n\n## Using Boolean Indexing\n\nAlternative approach:\n```python\narr = np.array([1, -2, 3, -4, 5])\n\n# Modify in place\narr[arr < 0] = 0  # [1, 0, 3, 0, 5]\n```\n\n## Clipping (Common Pattern)\n\n```python\n# Keep values between min and max\narr = np.array([5, 15, 25, 35, 45])\nnp.clip(arr, 10, 30)  # [10, 15, 25, 30, 30]\n```\n\n---\n\n## üéØ Your Task\n\nReplace values in an array based on conditions.",
    "starter_code": "import numpy as np\n\n# Test scores (some invalid)\nscores = np.array([85, -5, 105, 92, 78, 110, -10, 88])\n\nprint(\"Original scores:\", scores)\n\n# Replace negative scores with 0\n# Replace scores over 100 with 100\ncleaned = np.where(scores < 0, 0, scores)\ncleaned = np.where(cleaned > 100, 100, cleaned)\n\nprint(\"Cleaned scores:\", cleaned)\n\n# Alternative using clip\nclipped = np.clip(scores, 0, 100)\nprint(\"Clipped scores:\", clipped)",
    "solution_code": "import numpy as np\n\nscores = np.array([85, -5, 105, 92])\ncleaned = np.clip(scores, 0, 100)\nprint(f\"Cleaned: {cleaned}\")",
    "expected_output": "Original scores: [ 85  -5 105  92  78 110 -10  88]\nCleaned scores: [ 85   0 100  92  78 100   0  88]"
  },
  "195": {
    "title": "Matrix Operations",
    "chapter_title": "NumPy",
    "content": "# üî¢ Matrix Operations: The Heart of Data Science\n\n## Why Matrices Matter\n\nAlmost ALL data science involves matrices:\n- **Datasets**: Each row is a sample, each column is a feature\n- **Images**: 2D grids of pixel values\n- **Neural Networks**: Layers of weight matrices\n\n## Creating Matrices in NumPy\n\n```python\nimport numpy as np\n\n# From lists\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6]])\n\n# Special matrices\nzeros = np.zeros((3, 3))  # 3x3 of zeros\nones = np.ones((2, 4))    # 2x4 of ones\nidentity = np.eye(3)       # 3x3 identity matrix\n```\n\n## Essential Operations\n\n```python\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Element-wise operations\nA + B   # Add corresponding elements\nA * B   # Multiply corresponding elements\n\n# Matrix multiplication (dot product)\nA @ B   # or np.dot(A, B)\n\n# Transpose (flip rows and columns)\nA.T     # [[1, 3], [2, 4]]\n```\n\n## Matrix Multiplication Explained\n\n```\nA (2x2) @ B (2x2) = C (2x2)\n\nFor C[0,0]: 1*5 + 2*7 = 19\nFor C[0,1]: 1*6 + 2*8 = 22\nFor C[1,0]: 3*5 + 4*7 = 43\nFor C[1,1]: 3*6 + 4*8 = 50\n```\n\n## Real Data Science Uses\n\n```python\n# Linear regression: predictions = X @ weights\npredictions = X @ weights + bias\n\n# Normalize features\nX_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n```\n\n---\n\n## üéØ Your Task\n\nPerform matrix operations on sample data matrices.",
    "starter_code": "import numpy as np\n\n# Create two matrices\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Matrix multiplication\nproduct = A @ B\n\n# Transpose\nA_transposed = A.T\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nA @ B (matrix multiplication):\")\nprint(product)\nprint(\"\\nA transposed:\")\nprint(A_transposed)",
    "solution_code": "import numpy as np\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nproduct = A @ B\nA_transposed = A.T\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nA @ B (matrix multiplication):\")\nprint(product)\nprint(\"\\nA transposed:\")\nprint(A_transposed)",
    "expected_output": "Matrix A:\n[[1 2]\n [3 4]]\n\nMatrix B:\n[[5 6]\n [7 8]]\n\nA @ B (matrix multiplication):\n[[19 22]\n [43 50]]\n\nA transposed:\n[[1 3]\n [2 4]]"
  },
  "196": {
    "title": "Feature Scaling",
    "chapter_title": "NumPy",
    "content": "# ‚öñÔ∏è Feature Scaling: Level the Playing Field\n\n## The Problem with Different Scales\n\nImagine predicting house prices with:\n- **Square footage**: 500 - 5,000 sqft\n- **Number of bedrooms**: 1 - 6\n\nThe larger numbers dominate, making ML models perform poorly!\n\n## Two Scaling Methods\n\n### 1. Min-Max Normalization (0-1 scaling)\n```python\nX_normalized = (X - X.min()) / (X.max() - X.min())\n```\nMaps everything to [0, 1]\n\n### 2. Standardization (Z-score scaling)\n```python\nX_standardized = (X - X.mean()) / X.std()\n```\nMakes mean=0, std=1\n\n## When to Use Which?\n\n| Method | Use When |\n|--------|----------|\n| Min-Max | Need bounded values (0-1), e.g., image pixels |\n| Standardization | Normal-ish data, most ML algorithms |\n\n## NumPy Implementation\n\n```python\nimport numpy as np\n\n# Sample data\ndata = np.array([100, 200, 300, 400, 500])\n\n# Min-Max\nnormalized = (data - data.min()) / (data.max() - data.min())\n# [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Z-score\nstandardized = (data - data.mean()) / data.std()\n# [-1.41, -0.71, 0.0, 0.71, 1.41]\n```\n\n## Column-wise Scaling (Real Data)\n\n```python\n# Scale each feature (column) independently\nX = np.array([[100, 1], [200, 3], [300, 2], [400, 5]])\n\nX_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n```\n\n---\n\n## üéØ Your Task\n\nScale a dataset using both normalization and standardization methods.",
    "starter_code": "import numpy as np\n\n# Sample dataset: house prices and sizes\nprices = np.array([150000, 250000, 350000, 450000, 550000])\nsizes = np.array([1000, 1500, 2000, 2500, 3000])\n\n# Min-Max Normalization (0-1)\nprices_normalized = (prices - prices.min()) / (prices.max() - prices.min())\nsizes_normalized = (sizes - sizes.min()) / (sizes.max() - sizes.min())\n\n# Standardization (Z-score)\nprices_standardized = (prices - prices.mean()) / prices.std()\nsizes_standardized = (sizes - sizes.mean()) / sizes.std()\n\nprint(\"Normalized (0-1):\")\nprint(f\"  Prices: {prices_normalized}\")\nprint(f\"  Sizes: {sizes_normalized}\")\nprint(\"\\nStandardized (Z-score):\")\nprint(f\"  Prices: {np.round(prices_standardized, 2)}\")\nprint(f\"  Sizes: {np.round(sizes_standardized, 2)}\")",
    "solution_code": "import numpy as np\n\nprices = np.array([150000, 250000, 350000, 450000, 550000])\nsizes = np.array([1000, 1500, 2000, 2500, 3000])\n\nprices_normalized = (prices - prices.min()) / (prices.max() - prices.min())\nsizes_normalized = (sizes - sizes.min()) / (sizes.max() - sizes.min())\n\nprices_standardized = (prices - prices.mean()) / prices.std()\nsizes_standardized = (sizes - sizes.mean()) / sizes.std()\n\nprint(\"Normalized (0-1):\")\nprint(f\"  Prices: {prices_normalized}\")\nprint(\"\\nStandardized (Z-score):\")\nprint(f\"  Prices: {np.round(prices_standardized, 2)}\")",
    "expected_output": "Normalized (0-1):\n  Prices: [0.   0.25 0.5  0.75 1.  ]"
  },
  "197": {
    "title": "Distribution Analysis",
    "chapter_title": "NumPy",
    "content": "# üìä Distribution Analysis: Understand Your Data's Shape\n\n## Why Analyze Distributions?\n\nBefore modeling, you need to know your data's shape:\n- Is it normal (bell curve)?\n- Is it skewed (lopsided)?\n- Are there outliers?\n\n## Key Statistics\n\n```python\nimport numpy as np\n\ndata = np.array([10, 12, 23, 23, 16, 23, 21, 16])\n\n# Central tendency\nnp.mean(data)     # Average: 18.0\nnp.median(data)   # Middle value: 18.5\n# Mode: use scipy.stats.mode\n\n# Spread\nnp.std(data)      # Standard deviation\nnp.var(data)      # Variance\nnp.ptp(data)      # Range (peak-to-peak)\n```\n\n## Percentiles\n\n```python\n# Quartiles\nnp.percentile(data, 25)  # Q1 (25th percentile)\nnp.percentile(data, 50)  # Median (50th)\nnp.percentile(data, 75)  # Q3 (75th percentile)\n\n# Multiple at once\nnp.percentile(data, [25, 50, 75])\n```\n\n## Skewness Indicator\n\n```python\n# If mean > median: right-skewed (tail on right)\n# If mean < median: left-skewed (tail on left)\n# If mean ‚âà median: symmetric\n\nmean = np.mean(data)\nmedian = np.median(data)\nprint(\"Right-skewed\" if mean > median else \"Left-skewed or symmetric\")\n```\n\n## Five-Number Summary\n\n```python\ndef five_number_summary(data):\n    return {\n        'min': np.min(data),\n        'Q1': np.percentile(data, 25),\n        'median': np.median(data),\n        'Q3': np.percentile(data, 75),\n        'max': np.max(data)\n    }\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze the distribution of a dataset using NumPy statistics.",
    "starter_code": "import numpy as np\n\n# Sample data: exam scores\nscores = np.array([65, 70, 72, 75, 78, 80, 82, 85, 88, 90, 92, 95, 98, 100])\n\n# Calculate statistics\nmean = np.mean(scores)\nmedian = np.median(scores)\nstd = np.std(scores)\n\n# Quartiles\nq1 = np.percentile(scores, 25)\nq3 = np.percentile(scores, 75)\niqr = q3 - q1\n\nprint(\"Distribution Analysis:\")\nprint(f\"  Mean: {mean:.1f}\")\nprint(f\"  Median: {median:.1f}\")\nprint(f\"  Std Dev: {std:.1f}\")\nprint(f\"  Q1: {q1:.1f}\")\nprint(f\"  Q3: {q3:.1f}\")\nprint(f\"  IQR: {iqr:.1f}\")\n\n# Check skewness\nif mean > median:\n    print(\"\\n  Shape: Right-skewed\")\nelse:\n    print(\"\\n  Shape: Left-skewed or symmetric\")",
    "solution_code": "import numpy as np\n\nscores = np.array([65, 70, 75, 80, 85, 90, 95, 100])\nprint(f\"Mean: {np.mean(scores):.1f}\")\nprint(f\"Median: {np.median(scores):.1f}\")\nprint(f\"Std: {np.std(scores):.1f}\")",
    "expected_output": "Distribution Analysis:\n  Mean: 83.6\n  Median: 83.5\n  Shape: Right-skewed"
  },
  "198": {
    "title": "Correlation Check",
    "chapter_title": "NumPy",
    "content": "# üìà Correlation: Measuring Relationships\n\n## What is Correlation?\n\n**Correlation** measures how two variables move together:\n- **Positive (+1)**: When X goes up, Y goes up\n- **Negative (-1)**: When X goes up, Y goes down\n- **Zero (0)**: No relationship\n\n## Real-World Examples\n\n| Variables | Correlation |\n|-----------|-------------|\n| Study hours & Grades | Positive (~0.7) |\n| Temperature & Ice cream sales | Positive (~0.8) |\n| Exercise & Body fat | Negative (~-0.6) |\n| Shoe size & IQ | None (~0) |\n\n## Computing with NumPy\n\n```python\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 5, 4, 5])\n\n# Correlation matrix\ncorr_matrix = np.corrcoef(x, y)\n# [[1.   0.83]\n#  [0.83 1.  ]]\n\n# Get the correlation coefficient\nr = corr_matrix[0, 1]  # 0.83\n```\n\n## Interpreting Strength\n\n| |r| | Interpretation |\n|------|----------------|\n| 0.0 - 0.3 | Weak |\n| 0.3 - 0.7 | Moderate |\n| 0.7 - 1.0 | Strong |\n\n## ‚ö†Ô∏è Correlation ‚â† Causation!\n\nIce cream sales and drowning deaths are correlated. Does ice cream cause drowning? No! Both are caused by summer heat.\n\n---\n\n## üéØ Your Task\n\nCalculate correlation between advertising spend and sales.",
    "starter_code": "import numpy as np\n\n# Advertising spend (thousands $)\nad_spend = np.array([10, 15, 20, 25, 30, 35, 40])\n\n# Sales (thousands $)\nsales = np.array([100, 120, 140, 155, 175, 180, 200])\n\n# Calculate correlation\ncorr_matrix = np.corrcoef(ad_spend, sales)\ncorrelation = corr_matrix[0, 1]\n\nprint(f\"Correlation between Ad Spend and Sales: {correlation:.3f}\")\n\nif abs(correlation) > 0.7:\n    strength = \"strong\"\nelif abs(correlation) > 0.3:\n    strength = \"moderate\"\nelse:\n    strength = \"weak\"\n    \ndirection = \"positive\" if correlation > 0 else \"negative\"\nprint(f\"This is a {strength} {direction} relationship\")",
    "solution_code": "import numpy as np\n\nad_spend = np.array([10, 15, 20, 25, 30, 35, 40])\nsales = np.array([100, 120, 140, 155, 175, 180, 200])\n\ncorrelation = np.corrcoef(ad_spend, sales)[0, 1]\nprint(f\"Correlation: {correlation:.3f}\")",
    "expected_output": "Correlation between Ad Spend and Sales: 0.993\nThis is a strong positive relationship"
  },
  "199": {
    "title": "Column Selection",
    "chapter_title": "Pandas",
    "content": "# üìä Column Selection: Getting the Data You Need\n\n## Why Select Columns?\n\nReal datasets have dozens or hundreds of columns. You rarely need them all!\n\n## Basic Selection Methods\n\n### Single Column\n```python\n# Returns a Series\ndf['column_name']\ndf.column_name  # Dot notation (if no spaces)\n```\n\n### Multiple Columns\n```python\n# Returns a DataFrame\ndf[['col1', 'col2', 'col3']]\n```\n\n### By Column Type\n```python\n# Only numeric columns\ndf.select_dtypes(include=['number'])\n\n# Only string/object columns\ndf.select_dtypes(include=['object'])\n\n# Exclude certain types\ndf.select_dtypes(exclude=['datetime'])\n```\n\n## Using loc and iloc\n\n```python\n# loc: by label\ndf.loc[:, ['name', 'age']]  # All rows, specific columns\n\n# iloc: by position\ndf.iloc[:, 0:3]  # All rows, first 3 columns\n```\n\n## Column Filtering Patterns\n\n```python\n# Columns containing a word\ndf[[col for col in df.columns if 'price' in col]]\n\n# Columns starting with prefix\ndf.filter(like='sales')  # sales_jan, sales_feb, etc.\n\n# Columns matching regex\ndf.filter(regex='_2024$')  # Ends with _2024\n```\n\n## Real-World Example\n\n```python\n# From 50 columns, extract just what you need\ncustomer_view = df[['customer_id', 'name', 'email', 'total_spend']]\n\n# For ML: select only numeric features\nfeatures = df.select_dtypes(include=['number']).drop('target', axis=1)\n```\n\n---\n\n## üéØ Your Task\n\nSelect specific columns from a dataset for analysis.",
    "starter_code": "import pandas as pd\n\n# Large dataset with many columns\ndf = pd.DataFrame({\n    'customer_id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'email': ['a@x.com', 'b@x.com', 'c@x.com', 'd@x.com', 'e@x.com'],\n    'age': [25, 30, 35, 28, 42],\n    'city': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'],\n    'total_spend': [500, 1200, 800, 350, 950],\n    'signup_date': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-02']\n})\n\n# Select specific columns for analysis\nanalysis_cols = df[['name', 'age', 'total_spend']]\n\n# Select only numeric columns\nnumeric_only = df.select_dtypes(include=['number'])\n\nprint(\"Selected columns for analysis:\")\nprint(analysis_cols)\nprint(\"\\nNumeric columns only:\")\nprint(numeric_only)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'customer_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'spend': [500, 1200, 800]\n})\n\nanalysis = df[['name', 'age', 'spend']]\nprint(analysis)",
    "expected_output": "Selected columns for analysis:\n      name  age  total_spend\n0    Alice   25          500\n1      Bob   30         1200"
  },
  "200": {
    "title": "Complex Filtering",
    "chapter_title": "Pandas",
    "content": "# üîç Complex Filtering: Multiple Conditions\n\n## Beyond Simple Filters\n\nReal data analysis needs complex conditions:\n- \"Customers who spent > $100 AND live in California\"\n- \"Orders from 2024 OR marked as priority\"\n- \"Products NOT in the discontinued category\"\n\n## Pandas Filter Operators\n\n```python\n# AND: Both conditions must be true\ndf[(df['price'] > 100) & (df['quantity'] > 5)]\n\n# OR: Either condition can be true  \ndf[(df['status'] == 'shipped') | (df['priority'] == 'high')]\n\n# NOT: Invert the condition\ndf[~(df['category'] == 'discontinued')]\n```\n\n## ‚ö†Ô∏è Key Syntax Rules\n\n1. **Wrap each condition in parentheses**\n2. **Use & | ~ instead of and or not**\n3. **Use == for equality, not =**\n\n```python\n# WRONG\ndf[df['price'] > 100 and df['stock'] > 0]\n\n# CORRECT\ndf[(df['price'] > 100) & (df['stock'] > 0)]\n```\n\n## Multiple Conditions Made Readable\n\n```python\n# Create conditions separately for clarity\nexpensive = df['price'] > 100\nin_stock = df['stock'] > 0\nhigh_rated = df['rating'] >= 4.5\n\n# Combine them\npremium_products = df[expensive & in_stock & high_rated]\n```\n\n## Using isin() for Multiple Values\n\n```python\n# Filter rows where category is one of several\ndf[df['category'].isin(['Electronics', 'Books', 'Games'])]\n\n# Filter rows where status is NOT one of several\ndf[~df['status'].isin(['cancelled', 'returned'])]\n```\n\n---\n\n## üéØ Your Task\n\nFilter a product dataset using multiple conditions.",
    "starter_code": "import pandas as pd\n\n# Sample product data\ndf = pd.DataFrame({\n    'name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n    'price': [1200, 25, 75, 300, 150],\n    'stock': [5, 50, 30, 0, 15],\n    'rating': [4.5, 4.8, 4.2, 4.7, 3.9]\n})\n\n# Filter: price > 50 AND in stock AND rating >= 4.5\nfiltered = df[(df['price'] > 50) & (df['stock'] > 0) & (df['rating'] >= 4.5)]\n\nprint(\"Premium products (>$50, in stock, rating >= 4.5):\")\nprint(filtered)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n    'price': [1200, 25, 75, 300, 150],\n    'stock': [5, 50, 30, 0, 15],\n    'rating': [4.5, 4.8, 4.2, 4.7, 3.9]\n})\n\nfiltered = df[(df['price'] > 50) & (df['stock'] > 0) & (df['rating'] >= 4.5)]\nprint(\"Premium products:\")\nprint(filtered)",
    "expected_output": "Premium products (>$50, in stock, rating >= 4.5):\n     name  price  stock  rating\n0  Laptop   1200      5     4.5"
  },
  "201": {
    "title": "Pivot Analysis",
    "chapter_title": "Pandas",
    "content": "# üîÑ Pivot Tables: Reshaping Data for Insights\n\n## What is a Pivot Table?\n\nA **pivot table** reorganizes data to summarize it by categories. If you've used Excel pivot tables, pandas does the same thing‚Äîbut with code!\n\n## The Before and After\n\n**Before (raw data):**\n```\nDate       | Product | Sales\n2024-01-01 | Widget  | 100\n2024-01-01 | Gadget  | 150\n2024-01-02 | Widget  | 120\n2024-01-02 | Gadget  | 180\n```\n\n**After (pivoted):**\n```\nDate       | Widget | Gadget\n2024-01-01 | 100    | 150\n2024-01-02 | 120    | 180\n```\n\n## Creating a Pivot Table\n\n```python\npivot = df.pivot_table(\n    values='sales',      # What to aggregate\n    index='date',        # Row labels\n    columns='product',   # Column labels\n    aggfunc='sum'        # How to aggregate\n)\n```\n\n## Aggregation Functions\n\n```python\n# Sum (default)\naggfunc='sum'\n\n# Mean\naggfunc='mean'\n\n# Count\naggfunc='count'\n\n# Multiple at once!\naggfunc=['sum', 'mean', 'count']\n```\n\n## Real-World Examples\n\n```python\n# Sales by region and quarter\ndf.pivot_table(values='revenue', index='region', columns='quarter')\n\n# Average ratings by category\ndf.pivot_table(values='rating', index='category', aggfunc='mean')\n\n# Orders per customer per month\ndf.pivot_table(values='order_id', index='customer', columns='month', aggfunc='count')\n```\n\n---\n\n## üéØ Your Task\n\nCreate a pivot table showing sales by product and month.",
    "starter_code": "import pandas as pd\n\n# Sample sales data\ndf = pd.DataFrame({\n    'month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],\n    'product': ['Widget', 'Gadget', 'Widget', 'Gadget', 'Widget', 'Gadget'],\n    'sales': [100, 150, 120, 180, 140, 200]\n})\n\nprint(\"Original data:\")\nprint(df)\n\n# Create pivot table\npivot = df.pivot_table(\n    values='sales',\n    index='month',\n    columns='product',\n    aggfunc='sum'\n)\n\nprint(\"\\nPivot table (Sales by Month and Product):\")\nprint(pivot)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'month': ['Jan', 'Jan', 'Feb', 'Feb'],\n    'product': ['Widget', 'Gadget', 'Widget', 'Gadget'],\n    'sales': [100, 150, 120, 180]\n})\n\npivot = df.pivot_table(values='sales', index='month', columns='product')\nprint(pivot)",
    "expected_output": "Pivot table (Sales by Month and Product):\nproduct  Gadget  Widget\nmonth                  \nFeb         180     120\nJan         150     100\nMar         200     140"
  },
  "202": {
    "title": "Rolling Calculations",
    "chapter_title": "Pandas",
    "content": "# üìà Rolling Calculations: Smoothing Time Series\n\n## What Are Rolling Calculations?\n\nA **rolling window** slides across your data, computing statistics on a fixed-size chunk at a time. This smooths out noise and reveals trends!\n\n## Why Use Rolling Windows?\n\nRaw data can be noisy:\n```\nStock prices: [100, 105, 98, 110, 95, 108, 102...]\n```\nIt's hard to see the trend! But a 3-day moving average smooths it:\n```\nMoving avg:   [  -,   -,  101, 104, 101, 104, 102...]\n```\n\n## The Sliding Window Visual\n\n```\nData:     [10, 20, 30, 40, 50, 60]\nWindow=3:  [10, 20, 30] ‚Üí avg=20\n           [20, 30, 40] ‚Üí avg=30\n           [30, 40, 50] ‚Üí avg=40\n           [40, 50, 60] ‚Üí avg=50\nResult:   [NaN, NaN, 20, 30, 40, 50]\n```\n\n## Pandas Rolling\n\n```python\ndf['rolling_mean'] = df['value'].rolling(window=3).mean()\ndf['rolling_sum'] = df['value'].rolling(window=3).sum()\ndf['rolling_max'] = df['value'].rolling(window=3).max()\ndf['rolling_std'] = df['value'].rolling(window=3).std()\n```\n\n## Real-World Applications\n\n```python\n# 7-day moving average of COVID cases\ndf['7day_avg'] = df['cases'].rolling(7).mean()\n\n# 20-day moving average for stock trading\ndf['MA20'] = df['close'].rolling(20).mean()\n\n# Rolling correlation between two stocks\ndf['rolling_corr'] = df['stock_a'].rolling(30).corr(df['stock_b'])\n```\n\n---\n\n## üéØ Your Task\n\nCalculate a 3-day rolling average of daily sales data.",
    "starter_code": "import pandas as pd\n\n# Daily sales data\ndf = pd.DataFrame({\n    'day': [1, 2, 3, 4, 5, 6, 7],\n    'sales': [100, 150, 120, 180, 140, 200, 160]\n})\n\n# Calculate 3-day rolling average\ndf['rolling_avg'] = df['sales'].rolling(window=3).mean()\n\nprint(df)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'day': [1, 2, 3, 4, 5, 6, 7],\n    'sales': [100, 150, 120, 180, 140, 200, 160]\n})\n\ndf['rolling_avg'] = df['sales'].rolling(window=3).mean()\nprint(df)",
    "expected_output": "   day  sales  rolling_avg\n0    1    100          NaN\n1    2    150          NaN\n2    3    120   123.333333\n3    4    180   150.000000\n4    5    140   146.666667\n5    6    200   173.333333\n6    7    160   166.666667"
  },
  "203": {
    "title": "Data Merging",
    "chapter_title": "Pandas",
    "content": "# üîó Merging DataFrames: Combining Data Sources\n\n## Why Merge?\n\nReal data lives in multiple tables:\n- **Customers table**: customer_id, name, email\n- **Orders table**: order_id, customer_id, amount\n\nTo analyze \"customer spending,\" you need to merge them!\n\n## The Four Types of Merges\n\n| Type | Keeps |\n|------|-------|\n| Inner | Only matching rows |\n| Left | All left + matching right |\n| Right | All right + matching left |\n| Outer | All rows from both |\n\n## Basic Merge Syntax\n\n```python\nmerged = pd.merge(\n    left=orders,\n    right=customers,\n    on='customer_id',    # Column to match on\n    how='left'           # Type of merge\n)\n```\n\n## Visual Example\n\n```\nCustomers:           Orders:\nid | name            order_id | cust_id | amount\n1  | Alice           1        | 1       | 100\n2  | Bob             2        | 1       | 150\n                     3        | 3       | 200  (no matching customer!)\n\nInner merge: orders for Alice only (cust 1)\nLeft merge:  all orders, Alice's name filled in, cust 3 has NaN name\nOuter merge: all orders + Bob (even though no orders)\n```\n\n## When to Use Which\n\n| Scenario | Use |\n|----------|-----|\n| Only want complete records | Inner |\n| Keep all left data, add info from right | Left |\n| Keep everything, analyze gaps | Outer |\n\n---\n\n## üéØ Your Task\n\nMerge customer and order data to see order details with customer names.",
    "starter_code": "import pandas as pd\n\n# Customer data\ncustomers = pd.DataFrame({\n    'customer_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\n# Order data\norders = pd.DataFrame({\n    'order_id': [101, 102, 103, 104],\n    'customer_id': [1, 1, 2, 4],  # Note: customer 4 doesn't exist!\n    'amount': [50, 75, 100, 200]\n})\n\n# Merge with left join (keep all orders)\nmerged = pd.merge(orders, customers, on='customer_id', how='left')\n\nprint(\"Merged data:\")\nprint(merged)",
    "solution_code": "import pandas as pd\n\ncustomers = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\norders = pd.DataFrame({'order_id': [101, 102, 103], 'customer_id': [1, 1, 2], 'amount': [50, 75, 100]})\n\nmerged = pd.merge(orders, customers, on='customer_id', how='left')\nprint(merged)",
    "expected_output": "Merged data:\n   order_id  customer_id  amount     name\n0       101            1      50    Alice\n1       102            1      75    Alice\n2       103            2     100      Bob\n3       104            4     200      NaN"
  },
  "204": {
    "title": "Data Reshaping",
    "chapter_title": "Pandas",
    "content": "# üîÑ Reshaping Data: Melt and Pivot\n\n## Wide vs Long Format\n\n**Wide format** (good for reading):\n```\nStudent | Math | Science | English\nAlice   | 90   | 85      | 92\nBob     | 78   | 82      | 88\n```\n\n**Long format** (good for analysis):\n```\nStudent | Subject | Score\nAlice   | Math    | 90\nAlice   | Science | 85\nAlice   | English | 92\nBob     | Math    | 78\n...\n```\n\n## Converting Wide to Long: melt()\n\n```python\nlong = df.melt(\n    id_vars=['Student'],        # Keep these columns\n    value_vars=['Math', 'Science', 'English'],  # Unpivot these\n    var_name='Subject',         # Name for column headers\n    value_name='Score'          # Name for values\n)\n```\n\n## Converting Long to Wide: pivot()\n\n```python\nwide = df.pivot(\n    index='Student',     # Row identifiers\n    columns='Subject',   # New column headers\n    values='Score'       # Values to fill in\n)\n```\n\n## When to Use Which Format\n\n| Format | Best For |\n|--------|----------|\n| Wide | Human reading, small datasets |\n| Long | Visualization, groupby operations, ML |\n\n## Real-World Example\n\n```python\n# Financial data often comes wide\n#   Date | AAPL | GOOGL | MSFT\n# Need long format for analysis\n#   Date | Ticker | Price\n\nlong = stocks.melt(id_vars=['Date'], var_name='Ticker', value_name='Price')\n```\n\n---\n\n## üéØ Your Task\n\nReshape grade data from wide to long format for analysis.",
    "starter_code": "import pandas as pd\n\n# Wide format grades\ngrades_wide = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie'],\n    'Math': [90, 78, 85],\n    'Science': [85, 82, 90],\n    'English': [92, 88, 87]\n})\n\nprint(\"Wide format:\")\nprint(grades_wide)\n\n# Convert to long format\ngrades_long = grades_wide.melt(\n    id_vars=['Student'],\n    var_name='Subject',\n    value_name='Score'\n)\n\nprint(\"\\nLong format:\")\nprint(grades_long)",
    "solution_code": "import pandas as pd\n\ngrades_wide = pd.DataFrame({\n    'Student': ['Alice', 'Bob'],\n    'Math': [90, 78],\n    'Science': [85, 82]\n})\n\ngrades_long = grades_wide.melt(id_vars=['Student'], var_name='Subject', value_name='Score')\nprint(grades_long)",
    "expected_output": "Long format:\n   Student   Subject  Score\n0    Alice      Math     90\n1      Bob      Math     78\n2   Charlie     Math     85\n3    Alice   Science     85"
  },
  "205": {
    "title": "Multiple Series",
    "chapter_title": "Visualization",
    "content": "# üìà Multiple Series: Compare Data on One Chart\n\n## Why Multiple Series?\n\nOne line or bar is nice, but comparing is where insights happen:\n- \"How do sales compare across products?\"\n- \"Did conversion rates improve after the update?\"\n- \"Which region is growing fastest?\"\n\n## Line Charts with Multiple Series\n\n```python\nimport matplotlib.pyplot as plt\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr']\nproduct_a = [100, 120, 115, 140]\nproduct_b = [80, 95, 110, 135]\n\nplt.plot(months, product_a, 'b-o', label='Product A')\nplt.plot(months, product_b, 'r--s', label='Product B')\nplt.legend()\nplt.show()\n```\n\n## Line Styles and Markers\n\n| Style | Meaning |\n|-------|---------|\n| `-` | Solid line |\n| `--` | Dashed |\n| `-.` | Dash-dot |\n| `o` | Circle markers |\n| `s` | Square markers |\n| `^` | Triangle markers |\n\n## Bar Charts Side by Side\n\n```python\nimport numpy as np\n\nx = np.arange(len(months))\nwidth = 0.35\n\nplt.bar(x - width/2, product_a, width, label='A')\nplt.bar(x + width/2, product_b, width, label='B')\nplt.xticks(x, months)\n```\n\n## Stacked Bar Charts\n\n```python\nplt.bar(months, product_a, label='A')\nplt.bar(months, product_b, bottom=product_a, label='B')\n```\n\n## Essential: Always Add a Legend!\n\n```python\nplt.legend()  # Show what each color means\nplt.legend(loc='upper left')  # Specific position\n```\n\n---\n\n## üéØ Your Task\n\nCreate a multi-series line chart comparing two products.",
    "starter_code": "import matplotlib.pyplot as plt\n\n# Monthly data for two products\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nproduct_a = [100, 120, 115, 140, 160, 175]\nproduct_b = [80, 95, 110, 135, 150, 160]\n\n# Create multi-series line chart\nplt.figure(figsize=(10, 6))\nplt.plot(months, product_a, 'b-o', linewidth=2, label='Product A')\nplt.plot(months, product_b, 'r--s', linewidth=2, label='Product B')\n\nplt.title('Product Sales Comparison')\nplt.xlabel('Month')\nplt.ylabel('Sales ($K)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('multi_series.png')\nplt.close()\nprint(\"Multi-series chart saved!\")",
    "solution_code": "import matplotlib.pyplot as plt\n\nmonths = ['Jan', 'Feb', 'Mar']\na = [100, 120, 115]\nb = [80, 95, 110]\n\nplt.plot(months, a, 'b-o', label='A')\nplt.plot(months, b, 'r--s', label='B')\nplt.legend()\nplt.savefig('chart.png')\nprint(\"Saved!\")",
    "expected_output": "Multi-series chart saved!"
  },
  "206": {
    "title": "Annotated Charts",
    "chapter_title": "Visualization",
    "content": "# üìù Annotated Charts: Tell Stories with Data\n\n## Why Annotate?\n\nA chart without context is just shapes. **Annotations** turn data into stories:\n- \"Sales peaked here during Black Friday\"\n- \"This dip was due to a supply chain issue\"\n- \"Goal: reach this line by Q4\"\n\n## Types of Annotations\n\n### 1. Text Labels\n```python\nplt.annotate('Peak!', xy=(peak_x, peak_y), fontsize=12)\n```\n\n### 2. Arrows Pointing to Data\n```python\nplt.annotate('Important!',\n    xy=(x, y),                    # Point to here\n    xytext=(x+10, y+50),          # Text location\n    arrowprops=dict(arrowstyle='->')\n)\n```\n\n### 3. Horizontal/Vertical Lines\n```python\nplt.axhline(y=100, color='r', linestyle='--', label='Target')\nplt.axvline(x=5, color='g', linestyle=':')\n```\n\n### 4. Shaded Regions\n```python\nplt.axvspan(3, 5, alpha=0.3, color='yellow', label='Holiday period')\n```\n\n## Complete Example\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plot data\nplt.plot(months, sales)\n\n# Add target line\nplt.axhline(y=100, color='r', linestyle='--', label='Target: $100K')\n\n# Annotate peak\nmax_idx = sales.index(max(sales))\nplt.annotate(f'Peak: ${max(sales)}K',\n    xy=(months[max_idx], max(sales)),\n    xytext=(months[max_idx], max(sales) + 10),\n    arrowprops=dict(arrowstyle='->', color='green'))\n\nplt.legend()\nplt.show()\n```\n\n---\n\n## üéØ Your Task\n\nCreate an annotated chart highlighting key business metrics.",
    "starter_code": "import matplotlib.pyplot as plt\n\n# Monthly sales data\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nsales = [85, 92, 78, 105, 120, 115]\ntarget = 100\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(months, sales, 'b-o', linewidth=2, markersize=8)\n\n# Add target line\nplt.axhline(y=target, color='r', linestyle='--', label=f'Target: ${target}K')\n\n# Annotate peak month\npeak_idx = sales.index(max(sales))\nplt.annotate(f'Peak: ${max(sales)}K',\n    xy=(months[peak_idx], max(sales)),\n    xytext=(months[peak_idx], max(sales) + 8),\n    fontsize=10, ha='center',\n    arrowprops=dict(arrowstyle='->', color='green'))\n\nplt.title('Monthly Sales Performance')\nplt.ylabel('Sales ($K)')\nplt.legend()\nplt.savefig('annotated_chart.png')\nplt.close()\nprint(\"Annotated chart saved!\")",
    "solution_code": "import matplotlib.pyplot as plt\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May']\nsales = [85, 92, 78, 105, 120]\n\nplt.plot(months, sales, 'b-o')\nplt.axhline(y=100, color='r', linestyle='--', label='Target')\nplt.annotate('Peak!', xy=(4, 120))\nplt.legend()\nplt.savefig('chart.png')\nplt.close()\nprint(\"Chart saved!\")",
    "expected_output": "Annotated chart saved!"
  },
  "207": {
    "title": "Density Plot",
    "chapter_title": "Visualization",
    "content": "# üìä Density Plots: Smoother Histograms\n\n## Histogram vs Density Plot\n\nA **histogram** uses bars to show distribution. A **density plot** (KDE - Kernel Density Estimation) is a smooth version!\n\n```\nHistogram:      Density Plot:\n  ___            ___\n |   | __       /   \\\n_|   ||  |_   _/     \\_\n```\n\n## Why Use Density Plots?\n\n1. **Smoother**: No bin-size sensitivity\n2. **Comparable**: Easy to overlay multiple distributions\n3. **Continuous**: Shows the underlying shape better\n\n## Creating Density Plots\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simple density plot\ndf['column'].plot.kde()\n\n# With seaborn\nsns.kdeplot(data=df, x='column')\n\n# Histogram + density together\nsns.histplot(data=df, x='column', kde=True)\n```\n\n## Comparing Distributions\n\nDensity plots shine when comparing groups:\n\n```python\n# Compare salary distributions by department\nfor dept in df['department'].unique():\n    subset = df[df['department'] == dept]\n    sns.kdeplot(subset['salary'], label=dept)\nplt.legend()\n```\n\n## Real-World Uses\n\n- **A/B testing**: Compare conversion rate distributions\n- **Quality control**: Compare measurements before/after\n- **Demographics**: Compare age distributions across regions\n\n---\n\n## üéØ Your Task\n\nCreate a density plot to compare score distributions across two groups.",
    "starter_code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\ngroup_a = np.random.normal(70, 10, 100)\ngroup_b = np.random.normal(75, 8, 100)\n\n# Create density plots\nplt.figure(figsize=(10, 6))\n\n# Using pandas KDE\nimport pandas as pd\npd.Series(group_a).plot.kde(label='Group A', color='blue')\npd.Series(group_b).plot.kde(label='Group B', color='red')\n\nplt.xlabel('Score')\nplt.ylabel('Density')\nplt.title('Score Distribution: Group A vs Group B')\nplt.legend()\nplt.savefig('density_plot.png')\nplt.close()\nprint(\"Density plot saved!\")",
    "solution_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\ngroup_a = np.random.normal(70, 10, 100)\ngroup_b = np.random.normal(75, 8, 100)\n\npd.Series(group_a).plot.kde(label='Group A')\npd.Series(group_b).plot.kde(label='Group B')\nplt.legend()\nplt.savefig('density.png')\nprint(\"Saved!\")",
    "expected_output": "Density plot saved!"
  },
  "208": {
    "title": "Box Plot",
    "chapter_title": "Visualization",
    "content": "# üìä Box Plots: See Distribution at a Glance\n\n## Why Box Plots?\n\nA box plot shows you FIVE key statistics in one visual:\n1. **Minimum** (excluding outliers)\n2. **Q1** (25th percentile)\n3. **Median** (50th percentile)\n4. **Q3** (75th percentile)\n5. **Maximum** (excluding outliers)\n6. **Outliers** (dots beyond the whiskers)\n\n## Reading a Box Plot\n\n```\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ     ‚îÇ\n‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Median line\n    ‚îÇ     ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üë     ‚Üë\n   Q1    Q3\n    \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Whiskers extend to min/max (within 1.5√óIQR)\n\n    ‚Ä¢     ‚Ä¢     ‚Üê Outliers (beyond whiskers)\n```\n\n## Creating Box Plots\n\n```python\nimport matplotlib.pyplot as plt\n\n# Simple box plot\ndf['salary'].plot.box()\n\n# Multiple categories\ndf.boxplot(column='salary', by='department')\n\n# Using seaborn (prettier)\nimport seaborn as sns\nsns.boxplot(x='department', y='salary', data=df)\n```\n\n## Comparing Groups\n\nBox plots SHINE when comparing distributions:\n- \"Which department has the highest salaries?\"\n- \"Are scores more consistent in Class A or B?\"\n- \"Do outliers exist in certain categories?\"\n\n## Real-World Examples\n\n```python\n# Compare test scores across schools\ndf.boxplot(column='score', by='school')\n\n# Compare salaries by experience level\nsns.boxplot(x='experience', y='salary', data=df)\n\n# Identify outliers in sales data\ndf['revenue'].plot.box()\n```\n\n---\n\n## üéØ Your Task\n\nCreate a box plot to visualize and compare salary distributions across departments.",
    "starter_code": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Sample salary data by department\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'department': ['Sales']*20 + ['Engineering']*20 + ['Marketing']*20,\n    'salary': list(np.random.normal(60000, 10000, 20)) + \n              list(np.random.normal(90000, 15000, 20)) + \n              list(np.random.normal(55000, 8000, 20))\n})\n\n# Create box plot\ndf.boxplot(column='salary', by='department')\nplt.title('Salary Distribution by Department')\nplt.suptitle('')  # Remove automatic title\nplt.ylabel('Salary ($)')\nplt.savefig('box_plot.png', dpi=100, bbox_inches='tight')\nplt.close()\n\nprint(\"Box plot saved to box_plot.png\")\nprint(\"\\nSummary Statistics:\")\nprint(df.groupby('department')['salary'].describe().round(0))",
    "solution_code": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'department': ['Sales']*20 + ['Engineering']*20 + ['Marketing']*20,\n    'salary': list(np.random.normal(60000, 10000, 20)) + \n              list(np.random.normal(90000, 15000, 20)) + \n              list(np.random.normal(55000, 8000, 20))\n})\n\ndf.boxplot(column='salary', by='department')\nplt.savefig('box_plot.png')\nplt.close()\nprint(\"Box plot saved\")",
    "expected_output": "Box plot saved to box_plot.png"
  },
  "209": {
    "title": "Grouped Bars",
    "chapter_title": "Visualization",
    "content": "# üìä Grouped Bar Charts: Compare Categories Side by Side\n\n## When to Use Grouped Bars\n\nGrouped bars are perfect for comparing multiple categories across groups:\n- \"Sales by product AND by quarter\"\n- \"Performance by team AND by metric\"\n- \"Revenue by region AND by year\"\n\n## The Technique\n\nPlace bars side by side using position adjustment:\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncategories = ['Q1', 'Q2', 'Q3', 'Q4']\nproduct_a = [10, 15, 13, 17]\nproduct_b = [8, 12, 11, 14]\n\nx = np.arange(len(categories))  # [0, 1, 2, 3]\nwidth = 0.35\n\nfig, ax = plt.subplots()\nax.bar(x - width/2, product_a, width, label='Product A')\nax.bar(x + width/2, product_b, width, label='Product B')\n\nax.set_xticks(x)\nax.set_xticklabels(categories)\nax.legend()\n```\n\n## How the Positioning Works\n\n```\nWithout offset:  [A][B]  [A][B]  [A][B]  (bars overlap!)\n                  0       1       2\n\nWith offset:     [A]  [B] [A]  [B]\n                -0.2 0.2 0.8 1.2\n```\n\n## Adding More Groups\n\n```python\n# For 3 groups, divide the space into 3\nwidth = 0.25\nax.bar(x - width, group1, width)\nax.bar(x, group2, width)\nax.bar(x + width, group3, width)\n```\n\n## Best Practices\n\n‚úÖ Keep to 2-4 groups (more becomes unreadable)\n‚úÖ Use distinct colors\n‚úÖ Always include a legend\n‚úÖ Consider stacked bars if totals matter more\n\n---\n\n## üéØ Your Task\n\nCreate a grouped bar chart comparing quarterly sales for two products.",
    "starter_code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Quarterly sales data\nquarters = ['Q1', 'Q2', 'Q3', 'Q4']\nproduct_a = [120, 145, 130, 165]\nproduct_b = [100, 135, 120, 150]\n\n# Create grouped bar chart\nx = np.arange(len(quarters))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(10, 6))\nbars1 = ax.bar(x - width/2, product_a, width, label='Product A', color='#2196F3')\nbars2 = ax.bar(x + width/2, product_b, width, label='Product B', color='#FF9800')\n\nax.set_xlabel('Quarter')\nax.set_ylabel('Sales ($K)')\nax.set_title('Quarterly Sales Comparison')\nax.set_xticks(x)\nax.set_xticklabels(quarters)\nax.legend()\n\nplt.tight_layout()\nplt.savefig('grouped_bars.png')\nplt.close()\nprint(\"Grouped bar chart saved!\")",
    "solution_code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(4)\na = [120, 145, 130, 165]\nb = [100, 135, 120, 150]\n\nplt.bar(x - 0.2, a, 0.4, label='A')\nplt.bar(x + 0.2, b, 0.4, label='B')\nplt.legend()\nplt.savefig('grouped.png')\nprint(\"Saved!\")",
    "expected_output": "Grouped bar chart saved!"
  },
  "210": {
    "title": "Dashboard Layout",
    "chapter_title": "Visualization",
    "content": "# üìä Dashboard Layouts: Multiple Charts, One View\n\n## Why Dashboards?\n\nExecutives don't want to flip through 10 separate charts. A **dashboard** shows key metrics at a glance!\n\n## Creating Subplots\n\n```python\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Access individual plots\naxes[0, 0].plot(data)      # Top-left\naxes[0, 1].bar(x, y)       # Top-right\naxes[1, 0].pie(sizes)      # Bottom-left\naxes[1, 1].scatter(x, y)   # Bottom-right\n```\n\n## Layout Options\n\n```python\n# 1 row, 3 columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# 3 rows, 1 column\nfig, axes = plt.subplots(3, 1, figsize=(8, 12))\n\n# Unequal sizes with gridspec\nfrom matplotlib.gridspec import GridSpec\ngs = GridSpec(2, 2)\nax1 = fig.add_subplot(gs[0, :])  # Top: full width\nax2 = fig.add_subplot(gs[1, 0])  # Bottom-left\nax3 = fig.add_subplot(gs[1, 1])  # Bottom-right\n```\n\n## Dashboard Best Practices\n\n‚úÖ Most important metric: largest/top-left\n‚úÖ Related charts: group together\n‚úÖ Consistent colors: same meaning across charts\n‚úÖ Clear titles: each chart should stand alone\n\n---\n\n## üéØ Your Task\n\nCreate a 2x2 dashboard with different chart types.",
    "starter_code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nsales = [100, 120, 115, 140, 160, 155]\ncategories = ['A', 'B', 'C', 'D']\ncategory_sales = [35, 25, 25, 15]\n\n# Create 2x2 dashboard\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Sales Dashboard', fontsize=16)\n\n# Top-left: Line chart\naxes[0, 0].plot(months, sales, 'b-o', linewidth=2)\naxes[0, 0].set_title('Monthly Sales Trend')\naxes[0, 0].set_ylabel('Sales ($K)')\n\n# Top-right: Bar chart\naxes[0, 1].bar(categories, category_sales, color=['#4CAF50', '#2196F3', '#FF9800', '#E91E63'])\naxes[0, 1].set_title('Sales by Category')\n\n# Bottom-left: Pie chart\naxes[1, 0].pie(category_sales, labels=categories, autopct='%1.1f%%')\naxes[1, 0].set_title('Category Distribution')\n\n# Bottom-right: Scatter with trend\nx = np.random.rand(50) * 100\ny = x * 0.8 + np.random.rand(50) * 20\naxes[1, 1].scatter(x, y, alpha=0.6)\naxes[1, 1].set_title('Spend vs Revenue')\n\nplt.tight_layout()\nplt.savefig('dashboard.png', dpi=100)\nplt.close()\nprint(\"Dashboard saved!\")",
    "solution_code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\naxes[0, 0].plot([1, 2, 3], [1, 4, 2])\naxes[0, 1].bar(['A', 'B'], [3, 5])\naxes[1, 0].pie([30, 70])\naxes[1, 1].scatter([1, 2, 3], [1, 4, 2])\n\nplt.tight_layout()\nplt.savefig('dashboard.png')\nprint(\"Dashboard saved!\")",
    "expected_output": "Dashboard saved!"
  },
  "211": {
    "title": "Search Variants",
    "chapter_title": "Algorithms",
    "content": "# üîç Search Algorithms: Finding Data Fast\n\n## The Two Main Approaches\n\n### 1. Linear Search\nCheck every element one by one. Simple but slow.\n\n```python\ndef linear_search(arr, target):\n    for i, val in enumerate(arr):\n        if val == target:\n            return i\n    return -1\n```\n\n**Time Complexity**: O(n) - must check every element\n\n### 2. Binary Search\nDivide and conquer on sorted data. Much faster!\n\n```python\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n```\n\n**Time Complexity**: O(log n) - halves search space each step\n\n## When to Use Which\n\n| Method | Use When |\n|--------|----------|\n| Linear | Data is unsorted |\n| Linear | Small dataset (< 100 items) |\n| Binary | Data is sorted |\n| Binary | Large dataset |\n\n## Speed Comparison\n\n| Items | Linear (worst) | Binary (worst) |\n|-------|---------------|----------------|\n| 100 | 100 checks | 7 checks |\n| 1,000 | 1,000 checks | 10 checks |\n| 1,000,000 | 1,000,000 checks | 20 checks |\n\n## Built-in Options\n\n```python\n# Python's in operator (linear)\nif 5 in my_list:\n    print(\"Found!\")\n\n# Binary search with bisect\nimport bisect\nidx = bisect.bisect_left(sorted_list, target)\n```\n\n---\n\n## üéØ Your Task\n\nImplement and compare linear and binary search.",
    "starter_code": "import time\n\ndef linear_search(arr, target):\n    for i, val in enumerate(arr):\n        if val == target:\n            return i\n    return -1\n\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\n# Test both on sorted array\narr = list(range(10000))\ntarget = 9999\n\nidx_linear = linear_search(arr, target)\nidx_binary = binary_search(arr, target)\n\nprint(f\"Linear found at index: {idx_linear}\")\nprint(f\"Binary found at index: {idx_binary}\")",
    "solution_code": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\narr = list(range(1000))\nprint(f\"Found 999 at: {binary_search(arr, 999)}\")",
    "expected_output": "Linear found at index: 9999\nBinary found at index: 9999"
  },
  "212": {
    "title": "Rotated Search",
    "chapter_title": "Algorithms",
    "content": "# üîÑ Search in Rotated Array: Finding Order in Chaos\n\n## The Problem\n\nYou have a sorted array that was \"rotated\" at some point:\n\n```\nOriginal: [1, 2, 3, 4, 5, 6, 7]\nRotated:  [4, 5, 6, 7, 1, 2, 3]  ‚Üê Rotated at index 4\n```\n\nHow do you efficiently find a target value?\n\n## Why Not Just Linear Search?\n\nLinear search works, but it's O(n). For 1 million elements, that's 1 million checks! We can do better with **modified binary search**.\n\n## The Key Insight\n\nEven though the array is rotated, at least ONE HALF is always sorted:\n\n```\n[4, 5, 6, 7, 1, 2, 3]\n  ‚Üësorted‚Üë  ‚Üësorted‚Üë\n```\n\n## The Algorithm\n\n1. Find the middle element\n2. Determine which half is sorted\n3. Check if target is in the sorted half\n4. If yes, search there. If no, search the other half.\n\n## Step-by-Step Example\n\nFind 6 in [4, 5, 6, 7, 1, 2, 3]:\n\n```\nStep 1: mid = 7 (index 3)\n        Left [4,5,6,7] is sorted\n        Is 6 in [4,7]? YES\n        Search left half\n\nStep 2: mid = 5 (index 1)\n        Left [4,5] is sorted\n        Is 6 in [4,5]? NO (6 > 5)\n        Search right half\n\nStep 3: mid = 6 (index 2)\n        FOUND!\n```\n\n## Time Complexity\n\nO(log n) - Just like regular binary search!\n\n---\n\n## üéØ Your Task\n\nImplement binary search for a rotated sorted array.",
    "starter_code": "def search_rotated(nums, target):\n    left, right = 0, len(nums) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if nums[mid] == target:\n            return mid\n        \n        # Left half is sorted\n        if nums[left] <= nums[mid]:\n            if nums[left] <= target < nums[mid]:\n                right = mid - 1\n            else:\n                left = mid + 1\n        # Right half is sorted\n        else:\n            if nums[mid] < target <= nums[right]:\n                left = mid + 1\n            else:\n                right = mid - 1\n    \n    return -1\n\n# Test\nnums = [4, 5, 6, 7, 0, 1, 2]\ntarget = 0\nresult = search_rotated(nums, target)\nprint(f\"Found {target} at index: {result}\")",
    "solution_code": "def search_rotated(nums, target):\n    left, right = 0, len(nums) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if nums[mid] == target:\n            return mid\n        if nums[left] <= nums[mid]:\n            if nums[left] <= target < nums[mid]:\n                right = mid - 1\n            else:\n                left = mid + 1\n        else:\n            if nums[mid] < target <= nums[right]:\n                left = mid + 1\n            else:\n                right = mid - 1\n    return -1\n\nnums = [4, 5, 6, 7, 0, 1, 2]\nprint(f\"Found 0 at index: {search_rotated(nums, 0)}\")",
    "expected_output": "Found 0 at index: 4"
  },
  "213": {
    "title": "2D Search",
    "chapter_title": "Algorithms",
    "content": "# üîç 2D Search: Navigating Grid Data\n\n## Why Search 2D Data?\n\nReal data often comes in grids:\n- **Spreadsheets**: Rows and columns\n- **Images**: Pixel grids\n- **Games**: Chess boards, maps\n- **Databases**: Tables with rows and columns\n\nYou need to search efficiently through this 2D data!\n\n## Brute Force: Check Every Cell\n\n```python\n# O(n √ó m) time - check every cell\nfor row in range(len(grid)):\n    for col in range(len(grid[0])):\n        if grid[row][col] == target:\n            return (row, col)\n```\n\nWorks, but for a 1000√ó1000 grid, that's **1 million** checks!\n\n## Smart 2D Search: Sorted Grids\n\nIf rows are sorted AND columns are sorted, start from **top-right corner**:\n\n```\nExample: Find 35\n[1,  4,  7, 11, 15]\n[2,  5,  8, 12, 19]\n[3,  6,  9, 16, 22]\n[10, 13, 14, 17, 24]\n[18, 21, 23, 26, 30]\n\nStart at 15 (top-right):\n- 15 < 35? Move DOWN\n- 19 < 35? Move DOWN\n- 22 < 35? Move DOWN\n- 24 < 35? Move DOWN\n- 30 < 35? Move DOWN ‚Üí Off grid = NOT FOUND\n```\n\nEach step eliminates an entire row OR column!\n\n## Time Complexity Comparison\n\n| Method | Grid Size | Steps |\n|--------|-----------|-------|\n| Brute Force | 1000√ó1000 | 1,000,000 |\n| Smart Search | 1000√ó1000 | 2,000 max |\n\n## Real-World Application\n\nThis is how Excel finds values in sorted tables. It's also used in image processing to find patterns!\n\n---\n\n## üéØ Your Task\n\nSearch for a target value in a row-wise and column-wise sorted 2D grid.",
    "starter_code": "def search_2d(grid, target):\n    '''Search sorted 2D grid starting from top-right'''\n    if not grid:\n        return None\n    \n    rows, cols = len(grid), len(grid[0])\n    row, col = 0, cols - 1  # Start top-right\n    \n    while row < rows and col >= 0:\n        if grid[row][col] == target:\n            return (row, col)\n        elif grid[row][col] > target:\n            col -= 1  # Move left\n        else:\n            row += 1  # Move down\n    \n    return None  # Not found\n\n# Test\ngrid = [\n    [1,  4,  7, 11, 15],\n    [2,  5,  8, 12, 19],\n    [3,  6,  9, 16, 22],\n    [10, 13, 14, 17, 24],\n    [18, 21, 23, 26, 30]\n]\n\nresult = search_2d(grid, 16)\nprint(f\"Found 16 at: {result}\")",
    "solution_code": "def search_2d(grid, target):\n    if not grid:\n        return None\n    rows, cols = len(grid), len(grid[0])\n    row, col = 0, cols - 1\n    while row < rows and col >= 0:\n        if grid[row][col] == target:\n            return (row, col)\n        elif grid[row][col] > target:\n            col -= 1\n        else:\n            row += 1\n    return None\n\ngrid = [\n    [1, 4, 7, 11, 15],\n    [2, 5, 8, 12, 19],\n    [3, 6, 9, 16, 22],\n    [10, 13, 14, 17, 24]\n]\nprint(f\"Found 16 at: {search_2d(grid, 16)}\")",
    "expected_output": "Found 16 at: (2, 3)"
  },
  "214": {
    "title": "Selection Sort",
    "chapter_title": "Algorithms",
    "content": "# üîç Selection Sort: Find the Minimum, Swap It\n\n## How It Works\n\nSelection sort repeatedly finds the smallest element and moves it to the front:\n\n1. Find minimum in [0...n-1], swap with position 0\n2. Find minimum in [1...n-1], swap with position 1\n3. Find minimum in [2...n-1], swap with position 2\n...and so on!\n\n## Visual Example\n\n```\n[64, 25, 12, 22, 11]  ‚Üí Find min=11, swap with 64\n[11, 25, 12, 22, 64]  ‚Üí Find min=12, swap with 25\n[11, 12, 25, 22, 64]  ‚Üí Find min=22, swap with 25\n[11, 12, 22, 25, 64]  ‚Üí Find min=25, no swap needed\n[11, 12, 22, 25, 64]  ‚úÖ Sorted!\n```\n\n## The Algorithm\n\n```python\ndef selection_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        # Find index of minimum element in remaining array\n        min_idx = i\n        for j in range(i+1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        \n        # Swap minimum with current position\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    \n    return arr\n```\n\n## Time Complexity\n\n- **Best Case**: O(n¬≤) - still checks everything\n- **Average Case**: O(n¬≤)\n- **Worst Case**: O(n¬≤)\n\n## When to Use\n\n| Use Selection Sort | Don't Use |\n|-------------------|-----------|\n| Tiny arrays (<20 items) | Large arrays |\n| Memory is limited | Speed matters |\n| Teaching/learning | Production code |\n\n---\n\n## üéØ Your Task\n\nImplement selection sort and trace its execution.",
    "starter_code": "def selection_sort(arr):\n    '''Sort using selection sort algorithm'''\n    n = len(arr)\n    \n    for i in range(n):\n        # Find minimum element in remaining unsorted array\n        min_idx = i\n        for j in range(i + 1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        \n        # Swap the found minimum element with first element\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    \n    return arr\n\n# Test\ndata = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Original: {data}\")\nresult = selection_sort(data.copy())\nprint(f\"Sorted: {result}\")",
    "solution_code": "def selection_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        min_idx = i\n        for j in range(i + 1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    return arr\n\nprint(selection_sort([64, 34, 25, 12, 22, 11, 90]))",
    "expected_output": "Original: [64, 34, 25, 12, 22, 11, 90]\nSorted: [11, 12, 22, 25, 34, 64, 90]"
  },
  "215": {
    "title": "Insertion Sort",
    "chapter_title": "Algorithms",
    "content": "# üìù Insertion Sort: Building a Sorted Array\n\n## The Playing Cards Analogy\n\nInsertion sort works exactly like sorting cards in your hand:\n1. Pick up one card at a time\n2. Insert it into the correct position among cards you've already sorted\n3. Repeat until all cards are sorted\n\n## How It Works\n\n```\nStart:    [5, 2, 4, 1, 3]\n\nPass 1:   [2, 5, 4, 1, 3]  ‚Üê 2 inserted before 5\nPass 2:   [2, 4, 5, 1, 3]  ‚Üê 4 inserted between 2 and 5\nPass 3:   [1, 2, 4, 5, 3]  ‚Üê 1 inserted at beginning\nPass 4:   [1, 2, 3, 4, 5]  ‚Üê 3 inserted between 2 and 4\n\nSorted! ‚úÖ\n```\n\n## The Algorithm\n\n```python\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        \n        # Shift elements right to make room\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        \n        # Insert in correct position\n        arr[j + 1] = key\n```\n\n## When to Use Insertion Sort\n\n| Situation | Use Insertion Sort? |\n|-----------|-------------------|\n| Small arrays (<50 elements) | ‚úÖ Yes, it's fast |\n| Nearly sorted data | ‚úÖ Yes, O(n) best case! |\n| Large random arrays | ‚ùå No, too slow O(n¬≤) |\n| Online sorting (data arrives gradually) | ‚úÖ Yes, perfect for this |\n\n## Comparison with Other Sorts\n\n| Algorithm | Best | Average | Stable? |\n|-----------|------|---------|---------|\n| Insertion | O(n) | O(n¬≤) | Yes |\n| Merge | O(n log n) | O(n log n) | Yes |\n| Quick | O(n log n) | O(n log n) | No |\n\n---\n\n## üéØ Your Task\n\nImplement insertion sort and trace its execution.",
    "starter_code": "def insertion_sort(arr):\n    '''Sort array using insertion sort'''\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        \n        # Move elements greater than key one position ahead\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        \n        arr[j + 1] = key\n    return arr\n\n# Test\nnumbers = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Original: {numbers}\")\nsorted_nums = insertion_sort(numbers.copy())\nprint(f\"Sorted:   {sorted_nums}\")",
    "solution_code": "def insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n\nnumbers = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Sorted: {insertion_sort(numbers.copy())}\")",
    "expected_output": "Original: [64, 34, 25, 12, 22, 11, 90]\nSorted:   [11, 12, 22, 25, 34, 64, 90]"
  },
  "216": {
    "title": "Merge Sort",
    "chapter_title": "Algorithms",
    "content": "# üîÄ Merge Sort: Divide and Conquer\n\n## Why Learn Merge Sort?\n\nWhen you have millions of records to sort, the order you learned first (bubble sort, insertion sort) are too slow! Merge Sort is **guaranteed fast**‚Äîit always runs in O(n log n) time.\n\n## The Divide and Conquer Strategy\n\nMerge Sort follows a clever pattern:\n1. **Divide**: Split the list in half, recursively\n2. **Conquer**: Sort each half\n3. **Merge**: Combine the sorted halves\n\n## How It Works\n\n```\n[38, 27, 43, 3, 9, 82, 10]\n        ‚Üì Split\n[38, 27, 43, 3]  [9, 82, 10]\n    ‚Üì Split         ‚Üì Split\n[38, 27] [43, 3]  [9, 82] [10]\n    ‚Üì       ‚Üì        ‚Üì      ‚Üì\n[27, 38] [3, 43]  [9, 82] [10]\n    ‚Üì Merge          ‚Üì Merge\n[3, 27, 38, 43]  [9, 10, 82]\n           ‚Üì Final Merge\n[3, 9, 10, 27, 38, 43, 82]  ‚úÖ\n```\n\n## The Merge Step (Key Insight!)\n\nThe magic happens when merging two sorted lists. Since both are sorted, you just compare front elements:\n\n```python\nleft = [3, 27, 38]\nright = [9, 10, 43]\nresult = []\n\n# Compare fronts: 3 < 9, take 3\n# Compare fronts: 27 > 9, take 9\n# Compare fronts: 27 > 10, take 10\n# Compare fronts: 27 < 43, take 27\n# And so on...\n```\n\n## Time Complexity\n\n| Algorithm | Best | Average | Worst |\n|-----------|------|---------|-------|\n| Bubble Sort | O(n) | O(n¬≤) | O(n¬≤) |\n| Merge Sort | O(n log n) | O(n log n) | O(n log n) |\n\nFor 1 million items: Bubble = **1 trillion** operations, Merge = **20 million**!\n\n---\n\n## üéØ Your Task\n\nImplement the merge function that combines two sorted lists into one sorted list.",
    "starter_code": "def merge(left, right):\n    '''Merge two sorted lists into one sorted list'''\n    result = []\n    i = j = 0\n    \n    # Compare elements from both lists\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    # Add remaining elements\n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result\n\n# Test the merge function\nleft = [1, 3, 5, 7]\nright = [2, 4, 6, 8]\nmerged = merge(left, right)\nprint(f\"Merged: {merged}\")",
    "solution_code": "def merge(left, right):\n    result = []\n    i = j = 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result\n\nleft = [1, 3, 5, 7]\nright = [2, 4, 6, 8]\nprint(f\"Merged: {merge(left, right)}\")",
    "expected_output": "Merged: [1, 2, 3, 4, 5, 6, 7, 8]"
  },
  "217": {
    "title": "Quick Sort",
    "chapter_title": "Algorithms",
    "content": "# ‚ö° Quick Sort: The Speed Champion\n\n## Why Quick Sort?\n\nQuick Sort is often the **fastest sorting algorithm in practice**. It's so good that most programming languages use it as their default sort!\n\n## The Partition Strategy\n\nInstead of splitting in half like Merge Sort, Quick Sort picks a **pivot** and rearranges:\n- All elements **smaller** than pivot go left\n- All elements **larger** than pivot go right\n- The pivot is now in its final position!\n\n## How It Works\n\n```\n[8, 3, 7, 1, 5, 9, 2]  (pick 5 as pivot)\n        ‚Üì Partition\n[3, 1, 2] [5] [8, 7, 9]\n  ‚Üì         ‚Üì\n[1, 2, 3] [5] [7, 8, 9]\n        ‚Üì Combine\n[1, 2, 3, 5, 7, 8, 9]  ‚úÖ\n```\n\n## The Partition Step\n\n```python\n# Start with: [8, 3, 7, 1, 5, 9, 2], pivot = 5\n# \n# 8 > 5? Yes, stays right\n# 3 < 5? Yes, move left\n# 7 > 5? Yes, stays right\n# 1 < 5? Yes, move left\n# 9 > 5? Yes, stays right\n# 2 < 5? Yes, move left\n#\n# Result: [3, 1, 2] [5] [8, 7, 9]\n```\n\n## Quick Sort vs Merge Sort\n\n| Aspect | Quick Sort | Merge Sort |\n|--------|-----------|------------|\n| Average Case | O(n log n) | O(n log n) |\n| Worst Case | O(n¬≤) | O(n log n) |\n| Space | In-place (O(1)) | Needs extra (O(n)) |\n| In Practice | Usually fastest | Guaranteed consistent |\n\n## When to Use Which?\n\n- **Quick Sort**: Most general use cases (fast, memory efficient)\n- **Merge Sort**: When you need guaranteed performance (critical systems)\n\n---\n\n## üéØ Your Task\n\nImplement the partition function that rearranges a list around a pivot.",
    "starter_code": "def partition(arr, low, high):\n    '''Partition array around pivot (last element)'''\n    pivot = arr[high]\n    i = low - 1  # Index of smaller element\n    \n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    \n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1  # Pivot's final position\n\n# Test\narr = [8, 3, 7, 1, 5, 9, 2]\nprint(f\"Before: {arr}\")\npivot_idx = partition(arr, 0, len(arr) - 1)\nprint(f\"After:  {arr}\")\nprint(f\"Pivot (2) is now at index {pivot_idx}\")",
    "solution_code": "def partition(arr, low, high):\n    pivot = arr[high]\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\narr = [8, 3, 7, 1, 5, 9, 2]\nprint(f\"Before: {arr}\")\npivot_idx = partition(arr, 0, len(arr) - 1)\nprint(f\"After:  {arr}\")\nprint(f\"Pivot (2) is now at index {pivot_idx}\")",
    "expected_output": "Before: [8, 3, 7, 1, 5, 9, 2]\nAfter:  [1, 2, 7, 3, 5, 9, 8]"
  },
  "1162": {
    "id": 1162,
    "title": "Quiz: Window Functions I",
    "content": "# üß† Quiz: Ranking Logic\n\n## The Scenario\n\nYou are running a leaderboard. Two players have the exact same score of 100.\nThe next highest player has 90.\n\n## The Question\n\nWhich function generates the following ranks?\n1. Player A (100) -> Rank 1\n2. Player B (100) -> Rank 1\n3. Player C (90)  -> Rank 3\n\nA) ROW_NUMBER()\nB) DENSE_RANK()\nC) RANK()\nD) NTILE()\n\n## Think about it...\n\nDoes it skip numbers (1, 1, 3) or keep them tight (1, 1, 2)?\nThe \"Olympic\" standard skips numbers.\n\n---\n\n## üéØ Your Task\n\nSelect the correct function in the code editor to see if you're right!",
    "starter_code": "-- Get all column names from employees table\nSELECT column_name\nFROM information_schema.columns\nWHERE table_name = 'employees';",
    "solution_code": "SELECT column_name\nFROM information_schema.columns\nWHERE table_name = 'employees';",
    "expected_output": "id\nname\ndepartment\nsalary",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1163": {
    "id": 1163,
    "title": "Quiz: Window Functions II",
    "content": "# üß† Quiz: Moving Frames\n\n## The Scenario\n\nYou are calculating a 3-day moving average including the current day.\n\n## The Question\n\nWhat is the correct window frame definition?\n\nA) `ROWS BETWEEN 3 PRECEDING AND CURRENT ROW`\nB) `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`\nC) `ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING`\nD) `ROWS UNBOUNDED PRECEDING`\n\n## Think about it...\n\nCurrent row = 1.\nWe need 3 total.\nSo we need 2 before + 1 current = 3.\n\n---\n\n## üéØ Your Task\n\nImplement the correct frame to solve the puzzle.",
    "starter_code": "-- Describe the relationship\n-- One customer can have many orders\nSELECT c.name, COUNT(o.order_id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.name;",
    "solution_code": "SELECT c.name, COUNT(o.order_id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.name;",
    "expected_output": "Alice|5\nBob|3\nCharlie|0",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1164": {
    "id": 1164,
    "title": "Quiz: Window Functions III",
    "content": "# üß† Quiz: Partitioning\n\n## The Scenario\n\nYou wrote `AVG(Salary) OVER (PARTITION BY Dept)`.\n\n## The Question\n\nIf you have 5 Departments and 100 Employees total, how many unique average values will be calculated?\n\nA) 1\nB) 5\nC) 100\nD) 500\n\n## Think about it...\n\nPartition By breaks the data into independent chunks. Each chunk gets one result calculated for it (shared by everyone in the chunk).\n\n---\n\n## üéØ Your Task\n\nVerify your answer by running the query.",
    "starter_code": "-- Navigate FK relationships\nSELECT o.order_id, p.product_name, p.price\nFROM orders o\nJOIN products p ON o.product_id = p.id;",
    "solution_code": "SELECT o.order_id, p.product_name, p.price\nFROM orders o\nJOIN products p ON o.product_id = p.id;",
    "expected_output": "1|Widget|29.99\n2|Gadget|49.99",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1165": {
    "id": 1165,
    "title": "Quiz: CTEs I",
    "content": "# üß† Quiz: CTE Scope\n\n## The Question\n\nIs the following query valid?\n\n```sql\nWITH Users2023 AS (SELECT * FROM Users)\nSELECT * FROM Users2023;\n\nSELECT COUNT(*) FROM Users2023;\n```\n\nA) Yes, perfectly fine.\nB) No, the CTE persists for the session.\nC) No, the CTE exists only for the very first SELECT statement.\nD) Yes, but only if you use `CREATE TEMPORARY TABLE`.\n\n## Think about it...\n\nCTEs are ephemeral. They vaporize the moment the semicolon hits.\n\n---\n\n## üéØ Your Task\n\nFix the code structure to make it valid.",
    "starter_code": "-- Check cardinality\nSELECT parent_id, COUNT(*) as child_count\nFROM child_table\nGROUP BY parent_id;",
    "solution_code": "SELECT parent_id, COUNT(*) as child_count\nFROM child_table\nGROUP BY parent_id;",
    "expected_output": "1|3\n2|5\n3|1",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1166": {
    "id": 1166,
    "title": "Quiz: CTEs II",
    "content": "# üß† Quiz: CTE vs Subquery\n\n## The Question\n\nWhich of these is a valid reason to prefer a CTE over a Subquery?\n\nA) CTEs are always faster performance-wise.\nB) CTEs allow you to reference the same table logic multiple times in one query.\nC) CTEs can access file system data.\nD) Subqueries cannot handle WHERE clauses.\n\n## Think about it...\n\nDRY (Don't Repeat Yourself). If you need to Self-Join a filtered list, a CTE lets you define it once and join it to itself!\n\n---\n\n## üéØ Your Task\n\nProve it by writing a reusable CTE.",
    "starter_code": "-- Find NULL departments\nSELECT name, department\nFROM employees\nWHERE department IS NULL;",
    "solution_code": "SELECT name, department\nFROM employees\nWHERE department IS NULL;",
    "expected_output": "New Hire|NULL",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1167": {
    "id": 1167,
    "title": "Quiz: CTEs III",
    "content": "# üß† Quiz: Recursive CTEs\n\n## The Question\n\nWhat is the mandatory component of a Recursive CTE that stops it from running forever?\n\nA) The Anchor Member\nB) The Recursive Member\nC) The Termination Condition (usually in the WHERE clause)\nD) The UNION ALL\n\n## Think about it...\n\nIf `n + 1` never stops, you get an infinite loop (or a stack overflow error). You need `WHERE n < 10`.\n\n---\n\n## üéØ Your Task\n\nFix the broken recursive query to stop infinite looping.",
    "starter_code": "SELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE salary > 50000\nGROUP BY department\nHAVING COUNT(*) > 2\nORDER BY emp_count DESC;",
    "solution_code": "SELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE salary > 50000\nGROUP BY department\nHAVING COUNT(*) > 2\nORDER BY emp_count DESC;",
    "expected_output": "Engineering|5\nSales|3",
    "chapter_id": 200,
    "chapter_title": "Setup & Mental Model"
  },
  "1168": {
    "id": 1168,
    "title": "Quiz: Set Operations I",
    "content": "# üß† Quiz: UNION vs UNION ALL\n\n## The Question\n\nYou have two lists:\nList A: [1, 2, 2]\nList B: [2, 3]\n\nResult of `A UNION B`?\n\nA) [1, 2, 2, 2, 3]\nB) [1, 2, 3]\nC) [1, 2, 2, 3]\nD) [Error]\n\n## Think about it...\n\nUNION implies \"Unique\". It does the work of deduplication.\n\n---\n\n## üéØ Your Task\n\nWrite the query to get the unique set.",
    "starter_code": "-- Calculate total value per order\nSELECT product, price, quantity\n-- Add calculated column for total\nFROM orders;",
    "solution_code": "SELECT product, price, quantity, price * quantity AS total\nFROM orders;",
    "expected_output": "Widget|29.99|10|299.90\nGadget|49.99|5|249.95",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1169": {
    "id": 1169,
    "title": "Quiz: Set Operations II",
    "content": "# üß† Quiz: INTERSECT\n\n## The Question\n\n`INTERSECT` returns rows that appear in:\n\nA) The first query only.\nB) The second query only.\nC) Either query.\nD) Both queries.\n\n## Think about it...\n\nIntersection is the common ground. The overlapping area of the Venn diagram.\n\n---\n\n## üéØ Your Task\n\nFind the intersection of two datasets.",
    "starter_code": "-- Find unique departments\nSELECT \nFROM employees;",
    "solution_code": "SELECT DISTINCT department\nFROM employees;",
    "expected_output": "Engineering\nSales\nMarketing\nHR",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1170": {
    "id": 1170,
    "title": "Quiz: Set Operations III",
    "content": "# üß† Quiz: EXCEPT\n\n## The Question\n\n`SELECT A EXCEPT SELECT B` is effectively the same as:\n\nA) A LEFT JOIN B WHERE B IS NULL\nB) A INNER JOIN B\nC) A RIGHT JOIN B\nD) A FULL OUTER JOIN B\n\n## Think about it...\n\nYou want things in A that are NOT in B. That's exactly what a Left Join with a NULL check does!\n\n---\n\n## üéØ Your Task\n\nDemonstrate this equivalence with code.",
    "starter_code": "-- Create readable aliases\nSELECT emp_id, first_name, salary\nFROM employees;",
    "solution_code": "SELECT emp_id AS \"Employee ID\",\n       first_name AS \"First Name\",\n       salary AS \"Annual Salary\"\nFROM employees;",
    "expected_output": "Employee ID|First Name|Annual Salary\n1|John|75000\n2|Jane|82000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1171": {
    "id": 1171,
    "title": "Quiz: Date/Time I",
    "content": "# üß† Quiz: Truncation\n\n## The Question\n\nIf you `DATE_TRUNC('month', '2024-03-15')`, what do you get?\n\nA) '2024-03-15'\nB) '2024-03-01 00:00:00'\nC) 'Mar'\nD) 3\n\n## Think about it...\n\nTruncate means \"cut off the small parts\". It snaps to the starting line of the period.\n\n---\n\n## üéØ Your Task\n\nVerify the output of the truncation function.",
    "starter_code": "-- Get page 2 (items 11-20)\nSELECT *\nFROM products\nORDER BY id\n-- Add pagination;",
    "solution_code": "SELECT *\nFROM products\nORDER BY id\nLIMIT 10 OFFSET 10;",
    "expected_output": "Results 11-20 of products",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1172": {
    "id": 1172,
    "title": "Quiz: Date/Time II",
    "content": "# üß† Quiz: Intervals\n\n## The Question\n\nWhat happens if you add `INTERVAL '1 month'` to '2024-01-31'?\n\nA) 2024-02-31 (Error)\nB) 2024-03-01\nC) 2024-03-02\nD) 2024-02-28 (or 29 in leap years)\n\n## Think about it...\n\nSQL is smart about calendars. It usually snaps to the last valid day of the target month.\n\n---\n\n## üéØ Your Task\n\nTest this edge case calculation.",
    "starter_code": "-- Engineering or Sales, salary > 70000\nSELECT *\nFROM employees\nWHERE ;",
    "solution_code": "SELECT *\nFROM employees\nWHERE (department = 'Engineering' OR department = 'Sales')\n  AND salary > 70000;",
    "expected_output": "John|Engineering|75000\nJane|Sales|82000",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1173": {
    "id": 1173,
    "title": "Quiz: Date/Time III",
    "content": "# üß† Quiz: Filtering\n\n## The Question\n\nWhich WHERE clause correctly finds events in 2023?\n\nA) `WHERE Year(date) = 2023`\nB) `WHERE date BETWEEN '2023-01-01' AND '2023-12-31'`\nC) `WHERE date LIKE '2023%'`\nD) All of the above\n\n## Think about it...\n\nTechnically D might work depending on the DB, but B is the standard, SARGable (Index-friendly) way. A prevents index usage. C relies on implicit string conversion.\n\n---\n\n## üéØ Your Task\n\nWrite the most efficient filter for year 2023.",
    "starter_code": "-- Find non-active users\nSELECT name, status\nFROM users\nWHERE status <> 'active';",
    "solution_code": "SELECT name, status\nFROM users\nWHERE status <> 'active';",
    "expected_output": "Bob|inactive\nCharlie|pending",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1174": {
    "id": 1174,
    "title": "Quiz: Analytics I",
    "content": "# üß† Quiz: Aggregation\n\n## The Question\n\nWhat happens if you include a non-aggregated column in SELECT but miss it in GROUP BY?\n\nA) It picks a random value.\nB) It takes the first value.\nC) It throws a Syntax Error (in strict SQL).\nD) It automatically groups by it.\n\n## Think about it...\n\nMost modern standard databases (Postgres, SQL Server, Oracle) simply reject it. MySQL allowed it loosely in older versions but it's dangerous practice.\n\n---\n\n## üéØ Your Task\n\nTrigger this error to understand why it happens.",
    "starter_code": "-- Products starting with 'Pro'\nSELECT product_name\nFROM products\nWHERE product_name LIKE 'Pro%';",
    "solution_code": "SELECT product_name\nFROM products\nWHERE product_name LIKE 'Pro%';",
    "expected_output": "ProWidget\nProGadget",
    "chapter_id": 201,
    "chapter_title": "SELECT Basics"
  },
  "1175": {
    "title": "Quiz: Analytics II",
    "chapter_title": "Window Functions",
    "content": "# üß† Quiz: NULLs in Count\n\n## The Question\n\nCount(*) = 100. Count(ColumnA) = 90.\nWhat does this mean?\n\nA) There are 10 distinct values in ColumnA.\nB) There are 10 NULL values in ColumnA.\nC) There are 90 NULL values in ColumnA.\nD) The table has 90 rows.\n\n## Think about it...\n\n`COUNT(col)` only counts non-NULL values. The gap represents the NULLs.\n\n---\n\n## üéØ Your Task\n\nCalculate the percentage of NULL values in a dataset.",
    "starter_code": "-- Multiple window functions in one query\nSELECT \n    order_date,\n    customer_id,\n    amount,\n    -- Running total (all orders)\n    SUM(amount) OVER (ORDER BY order_date) as overall_running,\n    -- Running total per customer\n    SUM(amount) OVER (PARTITION BY customer_id ORDER BY order_date) as customer_running,\n    -- 7-day moving average\n    AVG(amount) OVER (ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d,\n    -- Rank within customer\n    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as customer_order_num\nFROM orders\nORDER BY order_date;",
    "solution_code": "SELECT amount, SUM(amount) OVER (ORDER BY date) as running, AVG(amount) OVER (ORDER BY date ROWS 2 PRECEDING) as ma FROM orders;",
    "expected_output": "order_date|amount|overall_running|customer_running|moving_avg_7d\n2024-01-01|100|100|100|100.00"
  },
  "1176": {
    "id": 1176,
    "title": "Quiz: Analytics III",
    "content": "# üß† Quiz: Having vs Where\n\n## The Question\n\nWhere do you filter 'Total Sales > 1000' (which is a generic sum)?\n\nA) WHERE clause\nB) HAVING clause\nC) JOIN clause\nD) LIMIT clause\n\n## Think about it...\n\n`Total Sales` is an aggregate (`SUM(Sales)`). Aggregates don't exist yet when WHERE runs. They exist after grouping, so you must use HAVING.\n\n---\n\n## üéØ Your Task\n\nFilter groups by their aggregate sum.",
    "starter_code": "SELECT product,\n       price,\n       ROUND(price * 1.08, 2) as price_with_tax\nFROM products;",
    "solution_code": "SELECT product,\n       price,\n       ROUND(price * 1.08, 2) as price_with_tax\nFROM products;",
    "expected_output": "Widget|29.99|32.39\nGadget|49.99|53.99",
    "chapter_id": 202,
    "chapter_title": "Data Types, NULLs & Calculations"
  },
  "1177": {
    "id": 1177,
    "title": "Rounding Methods",
    "content": "# üîµ Rounding Methods: ROUND, CEIL, FLOOR\n\n## Choosing Your Direction\n\nSometimes you need to control *how* a number is rounded.\n\n## The Three Main Functions\n\n**1. ROUND(value, decimals)**\nStandard rounding (0-4 down, 5-9 up).\n```sql\nROUND(10.5)  -> 11\nROUND(10.4)  -> 10\nROUND(10.555, 2) -> 10.56\n```\n\n**2. CEIL(value) / CEILING(value)**\nAlways rounds **UP** to nearest integer.\n```sql\nCEIL(10.1) -> 11\nCEIL(-10.1) -> -10\n```\n\n**3. FLOOR(value)**\nAlways rounds **DOWN** to nearest integer.\n```sql\nFLOOR(10.9) -> 10\nFLOOR(-10.1) -> -11\n```\n\n## Real-World Use Cases\n\n- **Pricing**: `CEIL` to ensure coverage (e.g. \"3.1 hours\" billed as 4).\n- **Pagination**: `CEIL(total_items / items_per_page)` = total pages.\n- **Discounts**: `FLOOR` to keep prices clean logic.\n\n---\n\n## üéØ Your Task\n\nCalculate shipping boxes needed. Each box holds 10 items. Even 1 extra item needs a full box!",
    "starter_code": "SELECT \n    order_id,\n    items_ordered,\n    -- Calculate boxes needed (capacity 10)\n    items_ordered / 10.0 as raw_ratio,\n    -- Use the right function here:\n    items_ordered as boxes_needed\nFROM orders;",
    "chapter_title": "Data Types, NULLs & Calculations",
    "solution_code": "SELECT order_id, CEIL(items_ordered / 10.0) as boxes_needed FROM orders;",
    "expected_output": "1001|25|3\n1002|10|1"
  },
  "1178": {
    "title": "Integer Division",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# ‚ûó Integer Division: The Hidden Trap\n\n## The Trap\n\nIn many SQL dialects (like Postgres and SQL Server), dividing two integers returns an integer:\n\n```sql\nSELECT 3 / 2;  -- Returns 1, NOT 1.5!\nSELECT 99 / 100; -- Returns 0!\n```\n\n## The Fix\n\nCast at least one number to a decimal/float **before** dividing.\n\n```sql\n-- Method 1: Literal decimal\nSELECT 3.0 / 2;  -- 1.5\n\n-- Method 2: Multiply by 1.0\nSELECT (3 * 1.0) / 2;\n\n-- Method 3: CAST / ::\nSELECT CAST(3 AS DECIMAL) / 2;\nSELECT 3::NUMERIC / 2;\n```\n\n## Modulo: The Remainder\n\nUse `%` (modulo) to get the remainder.\n```sql\nSELECT 10 % 3; -- Returns 1 (10 = 3*3 + 1)\n```\nUseful for: \"Every Nth row\", \"Is Even/Odd\".\n\n---\n\n## üéØ Your Task\n\nCalculate the percentage of budget used. Watch out for integer division!",
    "starter_code": "SELECT \n    project_name,\n    spent,\n    budget,\n    -- Fix this zero result:\n    (spent / budget) * 100 as pct_spent\nFROM projects;",
    "solution_code": "SELECT project_name, (spent::DECIMAL / budget) * 100 as pct_spent FROM projects;",
    "expected_output": "Alpha|5000|50.00\nBeta|200|20.00"
  },
  "1179": {
    "title": "String Functions",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# üî§ String Functions: Manipulating Text\n\n## Essential Toolkit\n\nText data is messy. You need tools to clean and format it.\n\n### 1. Length & Trimming\n```sql\nLENGTH(' apple ')   -- 7\nTRIM(' apple ')     -- 'apple' (Removes side spaces)\n```\n\n### 2. Case Conversion\n```sql\nUPPER('sql')        -- 'SQL'\nLOWER('SQL')        -- 'sql'\nINITCAP('hello world') -- 'Hello World'\n```\n\n### 3. Extraction\n```sql\n-- SUBSTRING(string, start, length)\nSUBSTRING('abcdef', 2, 3) -- 'bcd'\nLEFT('abcdef', 2)         -- 'ab'\n```\n\n### 4. Replacement\n```sql\nREPLACE('User_Id', '_', ' ') -- 'User Id'\n```\n\n## Concatenation\n\nJoining strings together:\n\n```sql\n-- Standard SQL\n'Hello' || ' ' || 'World'\n\n-- Function\nCONCAT('Hello', ' ', 'World') -- Safe with NULLs!\n```\n\n---\n\n## üéØ Your Task\n\nFormat user names as \"Last, First\" (e.g., \"Smith, John\") and convert to uppercase.",
    "starter_code": "SELECT \n    first_name,\n    last_name,\n    -- Expected: \"SMITH, JOHN\"\n    '' as formatted_name\nFROM users;",
    "solution_code": "SELECT UPPER(last_name || ', ' || first_name) as formatted_name FROM users;",
    "expected_output": "SMITH, JOHN\nDOE, JANE"
  },
  "1180": {
    "title": "Date Formatting",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# üìÖ Date Formatting: Strings to Dates and Back\n\n## The Date Type vs. String\n\nA date in the database is stored as a number (epoch). To humans, it needs formatting.\n\n## Formatting Dates (Date ‚Üí String)\n\nUse `TO_CHAR(date, format_string)`:\n\n```sql\nSELECT TO_CHAR(NOW(), 'YYYY-MM-DD');      -- '2024-01-30'\nSELECT TO_CHAR(NOW(), 'Month DD, YYYY');  -- 'January 30, 2024'\nSELECT TO_CHAR(NOW(), 'Day');             -- 'Tuesday'\n```\n\n## Common Patterns\n- `YYYY`: 4-digit year\n- `MM`: Month (01-12)\n- `Mon`: Abbreviated month (Jan)\n- `DD`: Day (01-31)\n- `HH24`: Hour (00-23)\n- `MI`: Minute\n\n## Parsing Dates (String ‚Üí Date)\n\nUse `TO_DATE()` or `CAST()`:\n\n```sql\nSELECT TO_DATE('01/30/2024', 'MM/DD/YYYY');\nSELECT CAST('2024-01-30' AS DATE);\n```\n\n---\n\n## üéØ Your Task\n\nFormat the order date to look like \"Jan 01, 2024\".",
    "starter_code": "SELECT \n    order_id,\n    order_date,\n    -- Format as 'Mon DD, YYYY'\n    order_date as formatted\nFROM orders;",
    "solution_code": "SELECT TO_CHAR(order_date, 'Mon DD, YYYY') as formatted FROM orders;",
    "expected_output": "101|Jan 15, 2024\n102|Feb 20, 2024"
  },
  "1181": {
    "title": "NULL Coalescing Chain",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# üîó NULL Coalescing: The Backup Plan\n\n## The Problem\n\nYou have multiple columns for contact info: `work_email`, `personal_email`, `phone`. You just want *one* valid contact method.\n\n## The Solution: COALESCE\n\n`COALESCE` takes a list of values and returns the **first non-NULL** one.\n\n```sql\nCOALESCE(work_email, personal_email, phone, 'No Contact Details')\n```\n\n## How It Works\n\n1. Check Value 1. Is it NULL? No ‚Üí Return it.\n2. Yes? Check Value 2. Is it NULL? No ‚Üí Return it.\n3. ...\n4. If all are NULL, return NULL (or the default provided at end).\n\n## Real-World Example\n\nDisplaying a display name:\n```sql\nSELECT COALESCE(nickname, full_name, username, 'Anonymous')\nFROM users;\n```\n\n---\n\n## üéØ Your Task\n\nFind the best contact number for each user using the priority: Mobile > Home > Work > 'N/A'.",
    "starter_code": "SELECT \n    name,\n    mobile_phone,\n    home_phone,\n    work_phone,\n    -- Fill this in:\n    NULL as best_contact\nFROM contacts;",
    "solution_code": "SELECT name, COALESCE(mobile_phone, home_phone, work_phone, 'N/A') FROM contacts;",
    "expected_output": "Alice|555-0100\nBob|555-0200\nCharlie|N/A"
  },
  "1182": {
    "title": "Type Casting",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# üîÑ Type Casting: Converting Types\n\n## Why Change Types?\n\nSometimes data comes in wrong:\n- IDs stored as strings (`'123'`)\n- Dates stored as text (`'20240101'`)\n- Decimals needed for division\n\n## The Syntax\n\n**Standard SQL:**\n```sql\nCAST(column AS type)\n```\n\n**Postgres Shortcut:**\n```sql\ncolumn::type\n```\n\n## Examples\n\n```sql\n-- String to Integer\nCAST('123' AS INTEGER)  -> 123\n\n-- String to Date\nCAST('2024-01-01' AS DATE)\n\n-- Integer to Decimal (for division)\nsales::DECIMAL / 2\n```\n\n## Implicit Casting\n\nSQL tries to be helpful. Comparing `'123' = 123` often works because it auto-converts. But don't rely on it! Be explicit.\n\n---\n\n## üéØ Your Task\n\nConvert the `string_price` column to a number and multiply by quantity.",
    "starter_code": "SELECT \n    product,\n    string_price, -- e.g. \"10.50\"\n    quantity,\n    -- Cast string_price to DECIMAL before multiplying\n    CAST(string_price AS DECIMAL) * quantity as total\nFROM raw_orders;",
    "solution_code": "SELECT product, CAST(string_price AS DECIMAL) * quantity as total FROM raw_orders;",
    "expected_output": "Widget|10.50"
  },
  "1183": {
    "title": "Safe Conversions",
    "chapter_title": "Data Types, NULLs & Calculations",
    "content": "# üõ°Ô∏è Safe Conversions: Handling Errors\n\n## The Explosion üí•\n\nWhat happens if you try this?\n\n```sql\nCAST('abc' AS INTEGER)\n```\n\n**ERROR!** The query crashes. This is bad for reports.\n\n## TRY_CAST (or Equivalent)\n\nMany modern SQL DBs support `TRY_CAST`:\n\n```sql\nTRY_CAST('abc' AS INTEGER)  -> NULL\nTRY_CAST('123' AS INTEGER)  -> 123\n```\n\nIt returns `NULL` on failure instead of crashing.\n\n## Regex Validation (Alternative)\n\nIf `TRY_CAST` isn't available, check format first:\n\n```sql\nCASE \n    WHEN string_val ~ '^[0-9]+$' THEN CAST(string_val AS INTEGER)\n    ELSE NULL \nEND\n```\n\n---\n\n## üéØ Your Task\n\nSafely convert a mix of numbers and garbage text to integers.",
    "starter_code": "SELECT \n    raw_value,\n    -- Use TRY_CAST or a CASE check to safely convert\n    -- For this exercise, assume standard CAST fails on errors\n    CASE \n        WHEN raw_value SIMILAR TO '[0-9]+' THEN CAST(raw_value AS INTEGER)\n        ELSE NULL \n    END as safe_number\nFROM sloppy_data;",
    "solution_code": "SELECT CASE WHEN raw_value SIMILAR TO '[0-9]+' THEN CAST(raw_value AS INTEGER) ELSE NULL END FROM sloppy_data;",
    "expected_output": "123|123\nabc|NULL"
  },
  "1184": {
    "title": "Conditional COUNT",
    "chapter_title": "Aggregations",
    "content": "# üìä Conditional COUNT: Count with Filters\n\n## The Problem\n\nYou want to count *only specific items* (e.g., \"VIP Users\") without filtering the whole query.\n\n```sql\n-- This filters EVERYTHING. You lose non-VIPs.\nSELECT COUNT(*) FROM users WHERE status = 'VIP';\n```\n\n## The Solution: CASE inside COUNT\n\n```sql\nSELECT \n    COUNT(*) as total_users,\n    COUNT(CASE WHEN status = 'VIP' THEN 1 END) as vip_count,\n    COUNT(CASE WHEN status = 'Inactive' THEN 1 END) as inactive_count\nFROM users;\n```\n\n## How It Works\n\n1. `CASE` returns `1` if matched, `NULL` if not.\n2. `COUNT(col)` ignores NULLs.\n3. Result: Only matches are counted!\n\n## The Postgres/SQLite Filter Syntax\n\nModern SQL has a cleaner shortcut:\n\n```sql\nCOUNT(*) FILTER (WHERE status = 'VIP')\n```\n\n## SUM(CASE) Pattern\n\nFor boolean flags (0/1), use SUM:\n```sql\nSUM(is_active)  -- Returns count of active users\n```\n\n---\n\n## üéØ Your Task\n\nCount total orders AND 'Urgent' orders in one query.",
    "starter_code": "SELECT \n    COUNT(*) as total_orders,\n    -- Count only orders with priority 'Urgent'\n    NULL as urgent_orders\nFROM orders;",
    "solution_code": "SELECT COUNT(*) as total_orders, COUNT(CASE WHEN priority = 'Urgent' THEN 1 END) as urgent_orders FROM orders;",
    "expected_output": "total_orders|urgent_orders\n100|15"
  },
  "1185": {
    "title": "NULL-aware SUM",
    "chapter_title": "Aggregations",
    "content": "# ‚ûï NULL-aware SUM: Handling Missing Data\n\n## The NULL Trap\n\n`SUM(column)` ignores NULLs. usually fine. But:\n`SUM(col_A + col_B)` is dangerous!\n\nIf A is 10 and B is NULL: `10 + NULL = NULL`. Result is ignored!\n\n## The Fix: COALESCE Inside\n\n```sql\n-- WRONG\nSUM(salary + bonus)\n\n-- RIGHT\nSUM(salary + COALESCE(bonus, 0))\n```\n\n## Summing Defaults\n\nIf *all* rows are NULL, `SUM()` returns `NULL`, not `0`.\n\n```sql\nCOALESCE(SUM(amount), 0)\n```\n\n## Real-World Example\n\nTotal Compensation = Salary + Commission (which might be null).\n\n---\n\n## üéØ Your Task\n\nCalculate total revenue, treating NULL amounts as 0.",
    "starter_code": "SELECT \n    region,\n    -- Fix potential NULL issues\n    SUM(amount + tax) as total_revenue\nFROM sales\nGROUP BY region;",
    "solution_code": "SELECT region, SUM(COALESCE(amount, 0) + COALESCE(tax, 0)) FROM sales GROUP BY region;",
    "expected_output": "North|15000\nSouth|12000"
  },
  "1186": {
    "title": "Range Calculation",
    "chapter_title": "Aggregations",
    "content": "# üìè Range Calculation: Min, Max, and Spread\n\n## Understanding Spread\n\nThe \"Spread\" (or Range) is `MAX - MIN`.\n\n## Example: Price Range\n\n```sql\nSELECT \n    category,\n    MIN(price) as cheapest,\n    MAX(price) as most_expensive,\n    MAX(price) - MIN(price) as price_spread\nFROM products\nGROUP BY category;\n```\n\n## Outlier Detection\n\nBig spread might mean outliers.\n\n## Date Ranges\n\n```sql\nSELECT \n    MAX(login_date) - MIN(login_date) as days_active\nFROM logins\nWHERE user_id = 123;\n```\n\n---\n\n## üéØ Your Task\n\nFind the difference between the highest and lowest salary per department.",
    "starter_code": "SELECT \n    department,\n    -- Calculate spread\n    NULL as salary_spread\nFROM employees\nGROUP BY department;",
    "solution_code": "SELECT department, MAX(salary) - MIN(salary) as salary_spread FROM employees GROUP BY department;",
    "expected_output": "Eng|50000\nSales|80000"
  },
  "1187": {
    "title": "Percentage Share",
    "chapter_title": "Aggregations",
    "content": "# üìä Percentage Share: Calculate Proportions\n\n## The Problem\n\n\"Electronics is 35% of revenue\". How do we get that calculation?\n\n## The Formula\n\n`Part / Total * 100`\n\n## Using Window Functions (Best Way)\n\n```sql\nSELECT \n    category,\n    sales,\n    sales / SUM(sales) OVER () * 100 as pct_share\nFROM category_sales;\n```\n\n## Using Subquery (Old Way)\n\n```sql\nSELECT \n    category,\n    sales,\n    sales / (SELECT SUM(sales) FROM category_sales) * 100\nFROM category_sales;\n```\n\n## Integer Division Warning!\n\nRemember to multiply by `100.0` (float) to avoid truncating to zero!\n\n---\n\n## üéØ Your Task\n\nCalculate the percentage of total votes for each candidate.",
    "starter_code": "SELECT \n    candidate,\n    votes,\n    -- Calculate percentage (formatted to 1 decimal)\n    ROUND(votes * 100.0 / SUM(votes) OVER (), 1) as pct\nFROM election_results\nORDER BY pct DESC;",
    "solution_code": "SELECT candidate, votes, ROUND(votes * 100.0 / SUM(votes) OVER (), 1) as pct FROM election_results;",
    "expected_output": "Alice|52.5\nBob|47.5"
  },
  "1188": {
    "title": "Running Average",
    "chapter_title": "Aggregations",
    "content": "# üìà Running Average: Smooth trends\n\n## Why Running Average?\n\nDaily data is noisy. A \"7-day moving average\" smooths it out to show trends.\n\n## The Syntax\n\n```sql\nAVG(amount) OVER (\n    ORDER BY date\n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n)\n```\n\n## Deconstructed\n- `ORDER BY`: Defines the timeline.\n- `ROWS BETWEEN`: Defines the window size (7 rows total: 6 previous + current).\n\n## Without Window (Group By)?\n\nHard to do! You need complex self-joins. Window functions make this trivial.\n\n---\n\n## üéØ Your Task\n\nCalculate a 3-day moving average of temperature readings.",
    "starter_code": "SELECT \n    date,\n    temperature,\n    -- 3-day moving average (2 preceding + current)\n    NULL as moving_avg\nFROM weather\nORDER BY date;",
    "solution_code": "SELECT date, temperature, AVG(temperature) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg FROM weather;",
    "expected_output": "2023-01-01|70|70.0\n2023-01-02|72|71.0\n2023-01-03|74|72.0"
  },
  "1189": {
    "title": "Multi-level Grouping",
    "chapter_title": "Aggregations",
    "content": "# üèóÔ∏è Multi-Level Grouping: GROUPING SETS, ROLLUP\n\n## The Need for Subtotals\n\nA standard `GROUP BY` gives one level. What if you want:\n- Sales by Region\n- Sales by Region AND Product\n- Grand Total\n...all in one result?\n\n## ROLLUP\n\n```sql\nGROUP BY ROLLUP (region, product)\n```\nGenerates: `(Region, Product)`, `(Region, NULL)`, `(NULL, NULL)`.\n\n## CUBE\n\n```sql\nGROUP BY CUBE (region, product)\n```\nGenerates ALL combinations (including `(NULL, Product)`).\n\n## GROUPING SETS\n\nExplicit control:\n```sql\nGROUP BY GROUPING SETS (\n    (region, product),\n    (region),\n    ()\n)\n```\n\n---\n\n## üéØ Your Task\n\nUse ROLLUP to generate sales totals by Country and City, plus a grand total.",
    "starter_code": "SELECT \n    country,\n    city,\n    SUM(sales)\nFROM international_sales\n-- Add ROLLUP\nGROUP BY country, city;",
    "solution_code": "SELECT country, city, SUM(sales) FROM international_sales GROUP BY ROLLUP(country, city);",
    "expected_output": "USA|NY|100\nUSA|NULL|100\nNULL|NULL|100"
  },
  "1190": {
    "title": "Three-Table Join",
    "chapter_title": "Joins",
    "content": "# üîó Three-Table Joins: Connecting the Dots\n\n## The Concept\n\nReal-world data is normalized. To get a complete picture, you often need to join 3, 4, or more tables.\n`Orders` -> `OrderItems` -> `Products`\n\n## The Syntax\n\nJust keep chaining!\n\n```sql\nSELECT c.name, o.date, p.product_name\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nJOIN order_items oi ON o.id = oi.order_id\nJOIN products p ON oi.product_id = p.id;\n```\n\n## Order of Operations\n\nSQL generally optimizes the join order for you. Logical path:\n1. Start with the central table (e.g., Orders).\n2. Join outwards to lookup tables (Customers, Products).\n\n## Performance Tip\n\nFilter early! `WHERE` clauses apply to the result, but limiting the dataset *before* joining massive tables is better (though the optimizer usually handles this).\n\n---\n\n## üéØ Your Task\n\nLink users, sessions, and pageviews to see who saw what.",
    "starter_code": "SELECT \n    u.username,\n    s.session_start,\n    p.page_url\nFROM users u\nJOIN sessions s ON u.id = s.user_id\n-- Add the third join to 'pageviews'\nJOIN pageviews p ON s.id = p.session_id\nORDER BY u.username;",
    "solution_code": "SELECT u.username, s.session_start, p.page_url FROM users u JOIN sessions s ON u.id = s.user_id JOIN pageviews p ON s.id = p.session_id;",
    "expected_output": "alice|2023-01-01|/home\nbob|2023-01-02|/pricing"
  },
  "1191": {
    "title": "Self-Referential",
    "chapter_title": "Joins",
    "content": "# ü™û Self Join: Query a Table Against Itself\n\n## What is a Self Join?\n\nA **self join** joins a table to itself. It's used when rows in the same table have relationships:\n- Employees and their managers (both in employees table)\n- Categories and parent categories\n- Friends (both users in same table)\n\n## The Pattern\n\n```sql\nSELECT \n    e.name as employee,\n    m.name as manager\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.id;\n```\n\n## Example: Organization Chart\n\n```sql\n-- employees table: id, name, manager_id\nSELECT \n    e.name as employee,\n    m.name as manager,\n    COALESCE(m.name, 'No Manager') as reports_to\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.id;\n```\n\nResult:\n```\nemployee | manager\n---------+---------\nAlice    | NULL (CEO)\nBob      | Alice\nCharlie  | Alice\nDiana    | Bob\n```\n\n## Finding Pairs\n\n```sql\n-- Find employees in same department\nSELECT a.name, b.name\nFROM employees a\nJOIN employees b ON a.department = b.department\nWHERE a.id < b.id;  -- Avoid duplicates (A,B) and (B,A)\n```\n\n## Common Use Cases\n\n| Scenario | Self Join On |\n|----------|-------------|\n| Org chart | manager_id = id |\n| Category tree | parent_id = id |\n| Friends | friend_id = user_id |\n| Sequential items | prev_id = id |\n\n---\n\n## üéØ Your Task\n\nWrite a self join to display employees with their managers.",
    "starter_code": "-- Self join: Employee and Manager\nSELECT \n    e.name as employee,\n    e.title as employee_title,\n    m.name as manager,\n    m.title as manager_title\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.id\nORDER BY m.name NULLS FIRST, e.name;",
    "solution_code": "SELECT e.name as employee, m.name as manager\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.id;",
    "expected_output": "employee|manager\nBob|Alice\nCharlie|Alice\nDiana|Bob\nAlice|NULL"
  },
  "1192": {
    "title": "Cross Join",
    "chapter_title": "Joins",
    "content": "# ‚úñÔ∏è Cross Join: All Possible Combinations\n\n## What is Cross Join?\n\nCombines every row from table A with every row from table B (Cartesian product):\n\n```\nTable A: 3 rows\nTable B: 4 rows\nCross Join: 3 √ó 4 = 12 rows\n```\n\n## Syntax\n\n```sql\n-- Explicit\nSELECT * FROM table_a CROSS JOIN table_b;\n\n-- Implicit (comma)\nSELECT * FROM table_a, table_b;\n```\n\n## Use Cases\n\n### Generate All Combinations\n```sql\n-- All color/size combinations\nSELECT colors.name, sizes.name\nFROM colors\nCROSS JOIN sizes;\n```\n\n### Date √ó Product Matrix\n```sql\n-- Ensure every product has a row for every date\nSELECT d.date, p.product_id\nFROM dates d\nCROSS JOIN products p;\n```\n\n### Fill Missing Data\n```sql\n-- Find missing combinations\nSELECT d.date, p.product_id\nFROM dates d\nCROSS JOIN products p\nLEFT JOIN sales s ON d.date = s.date AND p.product_id = s.product_id\nWHERE s.id IS NULL;  -- Missing sales\n```\n\n## ‚ö†Ô∏è Warning: Size Explosion!\n\n```sql\n-- 1000 √ó 1000 = 1,000,000 rows!\nSELECT * FROM large_a CROSS JOIN large_b;  -- Dangerous!\n```\n\nAlways check table sizes before cross joining!\n\n---\n\n## üéØ Your Task\n\nUse CROSS JOIN to generate all possible combinations.",
    "starter_code": "-- Create a calendar with all products (for zero-fill reporting)\nWITH date_range AS (\n    SELECT generate_series('2024-01-01'::date, '2024-01-07'::date, '1 day')::date as report_date\n),\nproducts AS (\n    SELECT product_id, name FROM products WHERE is_active = true\n)\n-- Cross join to get all date/product combinations\nSELECT \n    dr.report_date,\n    p.product_id,\n    p.name,\n    COALESCE(s.quantity, 0) as quantity_sold\nFROM date_range dr\nCROSS JOIN products p\nLEFT JOIN sales s ON dr.report_date = s.sale_date AND p.product_id = s.product_id\nORDER BY dr.report_date, p.product_id;",
    "solution_code": "SELECT c.color, s.size FROM colors c CROSS JOIN sizes s;",
    "expected_output": "report_date|product_id|name|quantity_sold\n2024-01-01|1|Widget|5\n2024-01-01|2|Gadget|0\n2024-01-02|1|Widget|3"
  },
  "1193": {
    "title": "Outer Join Uses",
    "chapter_title": "Joins",
    "content": "# ‚ÜîÔ∏è Outer Joins: Include Non-Matching Rows\n\n## Why Outer Joins?\n\nINNER JOIN excludes rows without matches. Outer joins INCLUDE them:\n- Show all customers, even those without orders\n- Show all products, even unsold ones\n- Show all dates, even those without data\n\n## LEFT JOIN\n\nKeep all rows from left table:\n```sql\nSELECT c.name, o.order_id\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id;\n-- Customers without orders appear with NULL order_id\n```\n\n## RIGHT JOIN\n\nKeep all rows from right table:\n```sql\nSELECT c.name, o.order_id\nFROM customers c\nRIGHT JOIN orders o ON c.id = o.customer_id;\n-- Orders without matching customers appear (rare, usually FK enforced)\n```\n\n## FULL OUTER JOIN\n\nKeep all rows from both tables:\n```sql\nSELECT c.name, o.order_id\nFROM customers c\nFULL OUTER JOIN orders o ON c.id = o.customer_id;\n-- Both unmatched customers AND unmatched orders appear\n```\n\n## Common Patterns\n\n### Find Missing Records\n```sql\n-- Customers who never ordered\nSELECT c.* FROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nWHERE o.id IS NULL;\n```\n\n### Zero-Fill Reports\n```sql\n-- All products with sales (including zeros)\nSELECT p.name, COALESCE(SUM(s.quantity), 0) as sold\nFROM products p\nLEFT JOIN sales s ON p.id = s.product_id\nGROUP BY p.name;\n```\n\n---\n\n## üéØ Your Task\n\nUse LEFT JOIN to include all records from the primary table.",
    "starter_code": "-- Find customers with and without orders\nSELECT \n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as order_count,\n    COALESCE(SUM(o.amount), 0) as total_spent,\n    CASE WHEN COUNT(o.order_id) = 0 THEN 'No Orders' ELSE 'Has Orders' END as status\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name\nORDER BY total_spent DESC;",
    "solution_code": "SELECT c.name, o.id FROM customers c LEFT JOIN orders o ON c.id = o.customer_id;",
    "expected_output": "customer_id|name|order_count|total_spent|status\n101|Alice|5|1250|Has Orders\n102|Bob|3|450|Has Orders\n103|Charlie|0|0|No Orders"
  },
  "1194": {
    "title": "Multi-value IN",
    "chapter_title": "Subqueries",
    "content": "# üìã Multi-Value IN: Match Against a List\n\n## Basic IN Clause\n\n```sql\nSELECT * FROM products\nWHERE category IN ('Electronics', 'Clothing', 'Home');\n```\n\n## IN with Subquery\n\n```sql\n-- Products that have been ordered\nSELECT * FROM products\nWHERE product_id IN (\n    SELECT DISTINCT product_id FROM order_items\n);\n```\n\n## Multiple Columns with IN\n\nMatch multiple columns at once:\n```sql\nSELECT * FROM sales\nWHERE (region, year) IN (\n    ('North', 2024),\n    ('South', 2024),\n    ('West', 2023)\n);\n```\n\n## IN vs JOIN\n\n```sql\n-- Using IN\nSELECT * FROM customers\nWHERE id IN (SELECT customer_id FROM orders);\n\n-- Using JOIN (often faster)\nSELECT DISTINCT c.* FROM customers c\nJOIN orders o ON c.id = o.customer_id;\n```\n\n## NOT IN Caution\n\n‚ö†Ô∏è NOT IN behaves unexpectedly with NULLs:\n\n```sql\n-- If subquery returns any NULL, NOT IN returns no rows!\nSELECT * FROM products\nWHERE id NOT IN (SELECT product_id FROM returns);\n\n-- Safer: Use NOT EXISTS\nSELECT * FROM products p\nWHERE NOT EXISTS (SELECT 1 FROM returns r WHERE r.product_id = p.id);\n```\n\n## Performance Tips\n\n| Size of List | Best Approach |\n|--------------|---------------|\n| <100 values | IN works fine |\n| 100-10000 | Consider JOIN |\n| 10000+ | Use temp table + JOIN |\n\n---\n\n## üéØ Your Task\n\nUse IN clauses with both static lists and subqueries.",
    "starter_code": "-- Find products in specific categories that have been ordered\nSELECT p.product_id, p.name, p.category\nFROM products p\nWHERE p.category IN ('Electronics', 'Home', 'Garden')\n  AND p.product_id IN (\n      SELECT DISTINCT product_id \n      FROM order_items \n      WHERE quantity > 0\n  )\nORDER BY p.category, p.name;",
    "solution_code": "SELECT * FROM products WHERE category IN ('Electronics', 'Home') AND id IN (SELECT product_id FROM orders);",
    "expected_output": "product_id|name|category\n101|Laptop|Electronics\n102|Phone|Electronics\n201|Lamp|Home"
  },
  "1195": {
    "title": "Scalar Comparison",
    "chapter_title": "Subqueries",
    "content": "# ‚öñÔ∏è Scalar Subqueries: Compare Against a Single Value\n\n## What is a Scalar Subquery?\n\nA subquery that returns exactly ONE value (one row, one column):\n\n```sql\nSELECT * FROM orders\nWHERE amount > (SELECT AVG(amount) FROM orders);\n```\n\n## Common Patterns\n\n### Compare to Aggregate\n```sql\n-- Orders above average\nWHERE amount > (SELECT AVG(amount) FROM orders)\n\n-- Products more expensive than average in their category\nWHERE price > (SELECT AVG(price) FROM products WHERE category = p.category)\n```\n\n### Compare to MAX/MIN\n```sql\n-- The highest order\nWHERE amount = (SELECT MAX(amount) FROM orders)\n\n-- Most recent order per customer\nWHERE order_date = (SELECT MAX(order_date) FROM orders o2 WHERE o2.customer_id = o.customer_id)\n```\n\n## Scalar in SELECT\n\n```sql\nSELECT \n    name,\n    salary,\n    salary - (SELECT AVG(salary) FROM employees) as diff_from_avg\nFROM employees;\n```\n\n## Scalar in CASE\n\n```sql\nSELECT \n    name,\n    amount,\n    CASE \n        WHEN amount > (SELECT AVG(amount) FROM orders) THEN 'Above Average'\n        ELSE 'Below Average'\n    END as performance\nFROM orders;\n```\n\n## ‚ö†Ô∏è Must Return One Value\n\n```sql\n-- ERROR: Returns multiple values\nWHERE amount > (SELECT amount FROM orders)\n\n-- OK: Returns single value\nWHERE amount > (SELECT MAX(amount) FROM orders)\n```\n\n---\n\n## üéØ Your Task\n\nUse scalar subqueries to compare rows against calculated values.",
    "starter_code": "-- Find products priced above category average\nSELECT \n    p.product_id,\n    p.name,\n    p.category,\n    p.price,\n    (SELECT AVG(price) FROM products WHERE category = p.category) as category_avg,\n    p.price - (SELECT AVG(price) FROM products WHERE category = p.category) as diff\nFROM products p\nWHERE p.price > (SELECT AVG(price) FROM products WHERE category = p.category)\nORDER BY diff DESC;",
    "solution_code": "SELECT * FROM orders WHERE amount > (SELECT AVG(amount) FROM orders);",
    "expected_output": "product_id|name|category|price|category_avg|diff\n101|Premium Laptop|Electronics|1500|800|700\n102|Designer Watch|Accessories|500|200|300"
  },
  "1196": {
    "title": "Double EXISTS",
    "chapter_title": "Subqueries",
    "content": "# üîó Double EXISTS: Multiple Relationship Checks\n\n## The Pattern\n\nCheck for multiple related conditions using nested EXISTS:\n\n```sql\n-- Customers who ordered both Product A AND Product B\nSELECT * FROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders o \n    JOIN order_items oi ON o.id = oi.order_id\n    WHERE o.customer_id = c.id AND oi.product_id = 'A'\n)\nAND EXISTS (\n    SELECT 1 FROM orders o \n    JOIN order_items oi ON o.id = oi.order_id\n    WHERE o.customer_id = c.id AND oi.product_id = 'B'\n);\n```\n\n## EXISTS with NOT EXISTS\n\n```sql\n-- Customers who ordered A but NOT B\nSELECT * FROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders WHERE customer_id = c.id AND product = 'A'\n)\nAND NOT EXISTS (\n    SELECT 1 FROM orders WHERE customer_id = c.id AND product = 'B'\n);\n```\n\n## Multiple Condition Patterns\n\n```sql\n-- Users who logged in this week AND made a purchase\nSELECT * FROM users u\nWHERE EXISTS (SELECT 1 FROM logins WHERE user_id = u.id AND date >= CURRENT_DATE - 7)\n  AND EXISTS (SELECT 1 FROM purchases WHERE user_id = u.id AND date >= CURRENT_DATE - 7);\n\n-- Users who logged in but NEVER purchased\nSELECT * FROM users u\nWHERE EXISTS (SELECT 1 FROM logins WHERE user_id = u.id)\n  AND NOT EXISTS (SELECT 1 FROM purchases WHERE user_id = u.id);\n```\n\n## Alternative: HAVING with COUNT\n\n```sql\n-- Same as double EXISTS, different approach\nSELECT customer_id FROM orders\nWHERE product IN ('A', 'B')\nGROUP BY customer_id\nHAVING COUNT(DISTINCT product) = 2;\n```\n\n---\n\n## üéØ Your Task\n\nUse multiple EXISTS clauses to find records matching complex criteria.",
    "starter_code": "-- Find customers who ordered Electronics AND Clothing\nSELECT c.customer_id, c.name\nFROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders o\n    JOIN products p ON o.product_id = p.product_id\n    WHERE o.customer_id = c.customer_id AND p.category = 'Electronics'\n)\nAND EXISTS (\n    SELECT 1 FROM orders o\n    JOIN products p ON o.product_id = p.product_id\n    WHERE o.customer_id = c.customer_id AND p.category = 'Clothing'\n);",
    "solution_code": "SELECT * FROM customers c WHERE EXISTS (SELECT 1 FROM orders WHERE customer_id = c.id AND product = 'A')\nAND EXISTS (SELECT 1 FROM orders WHERE customer_id = c.id AND product = 'B');",
    "expected_output": "customer_id|name\n101|Alice Smith\n205|Bob Johnson"
  },
  "1197": {
    "title": "NOT EXISTS Pattern",
    "chapter_title": "Subqueries",
    "content": "# üö´ NOT EXISTS: Find Missing Relationships\n\n## What Does NOT EXISTS Do?\n\n`NOT EXISTS` finds rows where a related row DOESN'T exist:\n- Customers with NO orders\n- Products that have NEVER been sold\n- Employees NOT assigned to any project\n\n## The Pattern\n\n```sql\nSELECT *\nFROM table_a a\nWHERE NOT EXISTS (\n    SELECT 1 FROM table_b b\n    WHERE b.a_id = a.id\n);\n```\n\n## Example: Customers Without Orders\n\n```sql\nSELECT c.name\nFROM customers c\nWHERE NOT EXISTS (\n    SELECT 1 \n    FROM orders o \n    WHERE o.customer_id = c.id\n);\n```\n\n## NOT EXISTS vs LEFT JOIN + NULL\n\nBoth work, but NOT EXISTS is often clearer:\n\n```sql\n-- NOT EXISTS (clearer intent)\nSELECT * FROM customers c\nWHERE NOT EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id);\n\n-- LEFT JOIN + NULL (equivalent)\nSELECT c.* FROM customers c\nLEFT JOIN orders o ON o.customer_id = c.id\nWHERE o.id IS NULL;\n```\n\n## Why Use SELECT 1?\n\nThe subquery result doesn't matter‚Äîonly existence does. `SELECT 1` is a convention that signals \"I only care if rows exist.\"\n\n## Common Use Cases\n\n| Find | Query Pattern |\n|------|---------------|\n| Inactive customers | NOT EXISTS (orders) |\n| Unsold products | NOT EXISTS (order_items) |\n| Unassigned tickets | NOT EXISTS (assignments) |\n\n---\n\n## üéØ Your Task\n\nFind products that have never been ordered using NOT EXISTS.",
    "starter_code": "-- Find products with no sales\nSELECT p.product_id, p.name\nFROM products p\nWHERE NOT EXISTS (\n    SELECT 1 \n    FROM order_items oi \n    WHERE oi.product_id = p.product_id\n);",
    "solution_code": "SELECT p.name FROM products p\nWHERE NOT EXISTS (SELECT 1 FROM order_items oi WHERE oi.product_id = p.product_id);",
    "expected_output": "product_id|name\n101|Widget Pro\n205|Gadget Plus"
  },
  "1198": {
    "title": "Correlated EXISTS",
    "chapter_title": "Subqueries",
    "content": "# üîó Correlated EXISTS: Check If Related Rows Exist\n\n## What is Correlated Subquery?\n\nA subquery that references the outer query. It runs once per outer row.\n\n```sql\nSELECT c.*\nFROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders o\n    WHERE o.customer_id = c.id  -- References outer query!\n);\n```\n\n## EXISTS vs IN\n\n```sql\n-- EXISTS: Stops at first match (faster for large sets)\nWHERE EXISTS (SELECT 1 FROM orders WHERE customer_id = c.id)\n\n-- IN: Builds full list first\nWHERE id IN (SELECT customer_id FROM orders)\n```\n\n## NOT EXISTS for Exclusion\n\n```sql\n-- Customers with NO orders\nSELECT * FROM customers c\nWHERE NOT EXISTS (\n    SELECT 1 FROM orders o WHERE o.customer_id = c.id\n);\n```\n\n## WITH Additional Conditions\n\n```sql\n-- Customers with orders over $1000\nSELECT * FROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders o \n    WHERE o.customer_id = c.id \n    AND o.amount > 1000\n);\n```\n\n## Performance Note\n\nCorrelated subqueries can be slow for large tables. Consider:\n- Adding indexes on correlation columns\n- Using JOINs instead when possible\n- Testing with EXPLAIN\n\n---\n\n## üéØ Your Task\n\nUse EXISTS to find rows with and without related records.",
    "starter_code": "-- Find customers with at least one high-value order\nSELECT c.customer_id, c.name\nFROM customers c\nWHERE EXISTS (\n    SELECT 1 \n    FROM orders o \n    WHERE o.customer_id = c.customer_id \n    AND o.amount > 500\n);",
    "solution_code": "SELECT * FROM customers c WHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id);",
    "expected_output": "customer_id|name\n101|Alice\n103|Charlie\n105|Eve"
  },
  "1199": {
    "title": "Inline Views",
    "chapter_title": "Subqueries",
    "content": "# üì¶ Inline Views: Subqueries in FROM Clause\n\n## What is an Inline View?\n\nA subquery in the FROM clause that acts like a temporary table:\n\n```sql\nSELECT *\nFROM (\n    SELECT customer_id, SUM(amount) as total\n    FROM orders\n    GROUP BY customer_id\n) AS customer_totals\nWHERE total > 1000;\n```\n\n## When to Use\n\n- Pre-aggregate before joining\n- Apply window functions then filter\n- Create derived columns for complex logic\n\n## Example: Filter After Window Function\n\nYou can't use window function results in WHERE directly, but inline views solve this:\n\n```sql\n-- WRONG: Can't filter on RANK in WHERE\nSELECT *, RANK() OVER (ORDER BY sales DESC) as rnk\nFROM products\nWHERE rnk <= 10;  -- ERROR!\n\n-- RIGHT: Use inline view\nSELECT * FROM (\n    SELECT *, RANK() OVER (ORDER BY sales DESC) as rnk\n    FROM products\n) ranked\nWHERE rnk <= 10;\n```\n\n## Inline View vs CTE\n\n```sql\n-- Inline View (older style)\nSELECT * FROM (SELECT ...) AS subquery;\n\n-- CTE (more readable)\nWITH subquery AS (SELECT ...)\nSELECT * FROM subquery;\n```\n\nBoth produce the same result, but CTEs are usually more readable.\n\n---\n\n## üéØ Your Task\n\nUse an inline view to filter on calculated/window function results.",
    "starter_code": "-- Inline view to filter on window function\nSELECT *\nFROM (\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as order_rank\n    FROM orders\n) ranked_orders\nWHERE order_rank <= 3;  -- Last 3 orders per customer",
    "solution_code": "SELECT * FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY amount DESC) as rn FROM orders) t WHERE rn <= 5;",
    "expected_output": "customer_id|order_date|amount|order_rank\n101|2024-03-15|250|1\n101|2024-03-10|180|2"
  },
  "1200": {
    "title": "SELECT Subqueries",
    "chapter_title": "Subqueries",
    "content": "# üìä Scalar Subqueries in SELECT: Add Calculated Columns\n\n## What Are Scalar Subqueries?\n\nSubqueries in the SELECT clause that return a single value:\n\n```sql\nSELECT \n    name,\n    (SELECT COUNT(*) FROM orders WHERE customer_id = c.id) as order_count\nFROM customers c;\n```\n\n## Common Patterns\n\n### Count Related Records\n```sql\nSELECT \n    product_id,\n    name,\n    (SELECT COUNT(*) FROM order_items WHERE product_id = p.id) as times_ordered\nFROM products p;\n```\n\n### Calculate Percentage\n```sql\nSELECT \n    category,\n    sales,\n    (sales / (SELECT SUM(sales) FROM products)) * 100 as pct_of_total\nFROM products;\n```\n\n### Get Latest Value\n```sql\nSELECT \n    customer_id,\n    (SELECT MAX(order_date) FROM orders WHERE customer_id = c.id) as last_order\nFROM customers c;\n```\n\n## Caution: Performance\n\nScalar subqueries execute once per row! For better performance:\n\n```sql\n-- Better: Use window function or JOIN\nSELECT \n    c.name,\n    COUNT(o.id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.name;\n```\n\n---\n\n## üéØ Your Task\n\nAdd calculated columns using scalar subqueries.",
    "starter_code": "-- Add calculated columns via subqueries\nSELECT \n    c.customer_id,\n    c.name,\n    -- Order count\n    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as total_orders,\n    -- Total spent\n    (SELECT COALESCE(SUM(amount), 0) FROM orders o WHERE o.customer_id = c.customer_id) as total_spent,\n    -- Last order date\n    (SELECT MAX(order_date) FROM orders o WHERE o.customer_id = c.customer_id) as last_order\nFROM customers c;",
    "solution_code": "SELECT name, (SELECT COUNT(*) FROM orders WHERE customer_id = c.id) as orders FROM customers c;",
    "expected_output": "customer_id|name|total_orders|total_spent|last_order\n101|Alice|5|1250|2024-03-15"
  },
  "1201": {
    "title": "Recursive Logic",
    "chapter_title": "CTEs",
    "content": "# üîÑ Recursive CTEs: Query Hierarchical Data\n\n## What is a Recursive CTE?\n\nA recursive CTE calls itself to traverse hierarchical data like:\n- Organization charts (employees ‚Üí managers)\n- File systems (folders ‚Üí subfolders)\n- Category trees (parent ‚Üí child categories)\n\n## The Structure\n\n```sql\nWITH RECURSIVE cte_name AS (\n    -- Base case (starting point)\n    SELECT ... WHERE parent_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case (refers to itself)\n    SELECT ... FROM cte_name JOIN ...\n)\nSELECT * FROM cte_name;\n```\n\n## Example: Organization Chart\n\n```sql\nWITH RECURSIVE org_chart AS (\n    -- Start with CEO (no manager)\n    SELECT id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Find each person's reports\n    SELECT e.id, e.name, e.manager_id, oc.level + 1\n    FROM employees e\n    JOIN org_chart oc ON e.manager_id = oc.id\n)\nSELECT name, level FROM org_chart ORDER BY level;\n```\n\n## Common Use Cases\n\n| Use Case | Base Case | Recursive Case |\n|----------|-----------|----------------|\n| Org Chart | CEO | Direct reports |\n| Categories | Root categories | Subcategories |\n| Bill of Materials | Final product | Components |\n\n## ‚ö†Ô∏è Avoid Infinite Loops\n\nAlways ensure recursion terminates! Use a `WHERE` clause or `level` limit.\n\n---\n\n## üéØ Your Task\n\nWrite a recursive CTE to traverse an organizational hierarchy.",
    "starter_code": "-- Recursive CTE for org chart\nWITH RECURSIVE org_chart AS (\n    -- Base: Start with CEO\n    SELECT id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive: Get each level's reports\n    SELECT e.id, e.name, e.manager_id, oc.level + 1\n    FROM employees e\n    JOIN org_chart oc ON e.manager_id = oc.id\n)\nSELECT name, level\nFROM org_chart\nORDER BY level, name;",
    "solution_code": "WITH RECURSIVE org_chart AS (\n    SELECT id, name, 1 as level FROM employees WHERE manager_id IS NULL\n    UNION ALL\n    SELECT e.id, e.name, oc.level + 1 FROM employees e JOIN org_chart oc ON e.manager_id = oc.id\n)\nSELECT * FROM org_chart ORDER BY level;",
    "expected_output": "name|level\nCEO|1\nVP Sales|2\nVP Eng|2\nManager|3"
  },
  "1202": {
    "title": "Union Dedup",
    "chapter_title": "Set Operations",
    "content": "# üîó UNION: Combine and Deduplicate Results\n\n## UNION vs UNION ALL\n\n```sql\n-- UNION: Removes duplicates (slower)\nSELECT name FROM customers\nUNION\nSELECT name FROM prospects;\n\n-- UNION ALL: Keeps all rows (faster)\nSELECT name FROM customers\nUNION ALL\nSELECT name FROM prospects;\n```\n\n## When to Use Each\n\n| Use | Behavior | Speed |\n|-----|----------|-------|\n| UNION | Removes dups | Slower |\n| UNION ALL | Keeps dups | Faster |\n\n## Rules for UNION\n\n1. Same number of columns\n2. Compatible data types\n3. Column names from first query\n\n```sql\n-- ‚úÖ Valid: Same structure\nSELECT name, email, 'customer' as type FROM customers\nUNION\nSELECT name, email, 'prospect' as type FROM prospects;\n\n-- ‚ùå Invalid: Different column count\nSELECT name, email FROM customers\nUNION\nSELECT name FROM prospects;\n```\n\n## Multiple UNIONs\n\n```sql\nSELECT * FROM orders_2022\nUNION ALL\nSELECT * FROM orders_2023\nUNION ALL\nSELECT * FROM orders_2024;\n```\n\n## With ORDER BY\n\nORDER BY applies to the entire result:\n\n```sql\nSELECT name FROM customers\nUNION\nSELECT name FROM prospects\nORDER BY name;  -- Sorts final combined result\n```\n\n---\n\n## üéØ Your Task\n\nCombine data from multiple tables using UNION operations.",
    "starter_code": "-- Combine customers and prospects, removing duplicates\nSELECT \n    email,\n    name,\n    'Customer' as source\nFROM customers\nUNION  -- Removes duplicates\nSELECT \n    email,\n    name,\n    'Prospect' as source\nFROM prospects\nORDER BY name;",
    "solution_code": "SELECT email FROM customers UNION SELECT email FROM prospects;",
    "expected_output": "email|name|source\nalice@x.com|Alice|Customer\nbob@x.com|Bob|Prospect"
  },
  "1203": {
    "title": "Intersection Find",
    "chapter_title": "Subqueries",
    "content": "# üîÄ Finding Intersections: Items in Multiple Sets\n\n## The Problem\n\nFind items that appear in MULTIPLE sets:\n- Customers who bought BOTH Product A AND Product B\n- Users who logged in on Day 1 AND Day 2\n- Products sold in BOTH North AND South regions\n\n## Method 1: INTERSECT\n\n```sql\nSELECT customer_id FROM orders WHERE product = 'A'\nINTERSECT\nSELECT customer_id FROM orders WHERE product = 'B';\n```\n\n## Method 2: JOIN with Multiple Subqueries\n\n```sql\nSELECT DISTINCT o1.customer_id\nFROM orders o1\nJOIN orders o2 ON o1.customer_id = o2.customer_id\nWHERE o1.product = 'A' AND o2.product = 'B';\n```\n\n## Method 3: GROUP BY + HAVING\n\n```sql\nSELECT customer_id\nFROM orders\nWHERE product IN ('A', 'B')\nGROUP BY customer_id\nHAVING COUNT(DISTINCT product) = 2;  -- Must have both!\n```\n\n## Which Method to Use?\n\n| Method | Best When |\n|--------|-----------|\n| INTERSECT | Simple, exact match |\n| JOIN | Need additional columns |\n| GROUP BY | Flexible count conditions |\n\n## Real-World Example\n\n```sql\n-- Users who visited on multiple specific days\nSELECT user_id\nFROM page_views\nWHERE visit_date IN ('2024-01-01', '2024-01-02', '2024-01-03')\nGROUP BY user_id\nHAVING COUNT(DISTINCT visit_date) = 3;\n```\n\n---\n\n## üéØ Your Task\n\nFind customers who purchased from multiple product categories.",
    "starter_code": "-- Find customers who bought Electronics AND Clothing\nSELECT customer_id\nFROM orders o\nJOIN products p ON o.product_id = p.id\nWHERE p.category IN ('Electronics', 'Clothing')\nGROUP BY customer_id\nHAVING COUNT(DISTINCT p.category) = 2;",
    "solution_code": "SELECT customer_id FROM orders WHERE product IN ('A', 'B')\nGROUP BY customer_id HAVING COUNT(DISTINCT product) = 2;",
    "expected_output": "customer_id\n101\n205\n312"
  },
  "1204": {
    "id": 1204,
    "title": "Problem Decomposition",
    "content": "# üß© Problem Decomposition: Divide and Conquer\n\n## The #1 Skill in SQL\n\nComplex questions are overwhelming.\n\"Find top 10 users by monthly growth rate who bought Electronics\"\n\n## The Strategy\n\nBreak it down into simple CTEs.\n\n1. **Step 1**: Who bought Electronics? (Simple SELECT)\n2. **Step 2**: Calculate Monthly Sales per user. (Group By)\n3. **Step 3**: Calculate Growth Rate (Window Function LAG).\n4. **Step 4**: Rank them (ORDER BY).\n\n## Don't write it all at once!\n\nWrite CTE 1. Test it.\nWrite CTE 2. Test it.\nCombine.\n\n---\n\n## üéØ Your Task\n\nBreak a complex requirement into 3 logical steps using CTEs.",
    "starter_code": "-- Use CTE for department totals\nWITH dept_totals AS (\n    -- Calculate sum of salaries per department\n)\nSELECT * FROM dept_totals;",
    "solution_code": "WITH dept_totals AS (\n    SELECT department, SUM(salary) as total_salary\n    FROM employees\n    GROUP BY department\n)\nSELECT * FROM dept_totals;",
    "expected_output": "Engineering|250000\nSales|180000",
    "chapter_id": 206,
    "chapter_title": "CTEs (WITH Clause)"
  },
  "1205": {
    "title": "Readable CTEs",
    "chapter_title": "CTEs",
    "content": "# üìñ Readable CTEs: Self-Documenting Queries\n\n## Why CTEs Make Queries Readable\n\nCompare these two approaches:\n\n### Without CTE (Nested Subqueries)\n```sql\nSELECT * FROM (\n    SELECT * FROM (\n        SELECT * FROM orders WHERE status = 'shipped'\n    ) shipped_orders\n    WHERE total > 100\n) big_shipped\nWHERE date > '2024-01-01';\n```\n\n### With CTEs (Named Steps)\n```sql\nWITH shipped_orders AS (\n    SELECT * FROM orders WHERE status = 'shipped'\n),\nbig_orders AS (\n    SELECT * FROM shipped_orders WHERE total > 100\n),\nrecent_orders AS (\n    SELECT * FROM big_orders WHERE date > '2024-01-01'\n)\nSELECT * FROM recent_orders;\n```\n\n## CTE Naming Best Practices\n\n| Bad Name | Good Name | Why |\n|----------|-----------|-----|\n| `t1` | `active_users` | Describes content |\n| `sub` | `monthly_totals` | Intention is clear |\n| `temp` | `filtered_orders` | Self-documenting |\n\n## Building Complex Queries Step by Step\n\n```sql\nWITH\n-- Step 1: Get base data\nbase_orders AS (\n    SELECT * FROM orders WHERE year = 2024\n),\n-- Step 2: Add customer info\nenriched AS (\n    SELECT o.*, c.name FROM base_orders o JOIN customers c ON o.customer_id = c.id\n),\n-- Step 3: Calculate metrics\nsummarized AS (\n    SELECT name, SUM(amount) as total FROM enriched GROUP BY name\n)\nSELECT * FROM summarized ORDER BY total DESC;\n```\n\n---\n\n## üéØ Your Task\n\nRefactor a complex query into readable CTEs with meaningful names.",
    "starter_code": "-- Convert nested query to readable CTEs\nWITH active_customers AS (\n    -- Step 1: Find active customers\n    SELECT customer_id, name\n    FROM customers\n    WHERE status = 'active'\n),\ncustomer_orders AS (\n    -- Step 2: Get their orders\n    SELECT c.name, o.amount, o.order_date\n    FROM active_customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n),\nmonthly_totals AS (\n    -- Step 3: Summarize by month\n    SELECT \n        name,\n        DATE_TRUNC('month', order_date) as month,\n        SUM(amount) as total\n    FROM customer_orders\n    GROUP BY name, DATE_TRUNC('month', order_date)\n)\nSELECT * FROM monthly_totals ORDER BY month, total DESC;",
    "solution_code": "WITH active_customers AS (SELECT * FROM customers WHERE status = 'active'),\ncustomer_orders AS (SELECT c.name, o.amount FROM active_customers c JOIN orders o ON c.customer_id = o.customer_id)\nSELECT name, SUM(amount) FROM customer_orders GROUP BY name;",
    "expected_output": "name|month|total\nAlice|2024-01|5000\nBob|2024-01|3500"
  },
  "1206": {
    "title": "CTE Pipeline",
    "chapter_title": "CTEs",
    "content": "# üîÑ CTE Pipelines: Chain Transformations\n\n## What is a CTE Pipeline?\n\nA series of CTEs where each step transforms the previous step's output:\n\n```sql\nWITH raw AS (...),\ncleaned AS (... FROM raw),\nenriched AS (... FROM cleaned),\naggregated AS (... FROM enriched)\nSELECT * FROM aggregated;\n```\n\n## Example: Data Pipeline\n\n```sql\nWITH \n-- Stage 1: Extract raw data\nraw_events AS (\n    SELECT * FROM events WHERE date >= '2024-01-01'\n),\n-- Stage 2: Filter valid events\nvalid_events AS (\n    SELECT * FROM raw_events WHERE user_id IS NOT NULL\n),\n-- Stage 3: Enrich with user data\nenriched AS (\n    SELECT e.*, u.name, u.country\n    FROM valid_events e\n    JOIN users u ON e.user_id = u.id\n),\n-- Stage 4: Aggregate\nsummary AS (\n    SELECT country, COUNT(*) as events\n    FROM enriched\n    GROUP BY country\n)\nSELECT * FROM summary ORDER BY events DESC;\n```\n\n## Benefits of Pipelines\n\n| Benefit | Description |\n|---------|-------------|\n| Readability | Each step has clear purpose |\n| Debuggability | Can inspect any stage |\n| Maintainability | Easy to modify one stage |\n| Reusability | Earlier stages can be reused |\n\n## Best Practice: Name Each Stage\n\n```sql\nWITH \nstage_1_extract AS (...),\nstage_2_transform AS (...),\nstage_3_load AS (...)\n```\n\n---\n\n## üéØ Your Task\n\nBuild a CTE pipeline for a multi-step data transformation.",
    "starter_code": "-- CTE Pipeline: Extract ‚Üí Clean ‚Üí Enrich ‚Üí Aggregate\nWITH \n-- Stage 1: Extract recent orders\nrecent_orders AS (\n    SELECT * FROM orders \n    WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n),\n-- Stage 2: Filter completed only\ncompleted_orders AS (\n    SELECT * FROM recent_orders \n    WHERE status = 'completed'\n),\n-- Stage 3: Enrich with customer data\nenriched AS (\n    SELECT o.*, c.name, c.tier\n    FROM completed_orders o\n    JOIN customers c ON o.customer_id = c.id\n),\n-- Stage 4: Aggregate by customer tier\ntier_summary AS (\n    SELECT \n        tier,\n        COUNT(*) as orders,\n        SUM(amount) as revenue\n    FROM enriched\n    GROUP BY tier\n)\nSELECT * FROM tier_summary ORDER BY revenue DESC;",
    "solution_code": "WITH raw AS (SELECT * FROM orders), cleaned AS (SELECT * FROM raw WHERE amount > 0)\nSELECT * FROM cleaned;",
    "expected_output": "tier|orders|revenue\nplatinum|45|125000\ngold|120|85000\nsilver|200|45000"
  },
  "1207": {
    "title": "Parallel CTEs",
    "chapter_title": "CTEs",
    "content": "# ‚ö° Parallel CTEs: Multiple Independent Queries\n\n## What Are Parallel CTEs?\n\nWhen CTEs don't depend on each other, the database can run them in parallel:\n\n```sql\nWITH\nsales_summary AS (SELECT ... FROM sales),\ninventory_check AS (SELECT ... FROM inventory),\ncustomer_stats AS (SELECT ... FROM customers)\nSELECT ...\n```\n\nThese three CTEs can execute simultaneously!\n\n## Parallel vs Sequential\n\n### Sequential (Each depends on previous)\n```sql\nWITH\nstep1 AS (SELECT ...),\nstep2 AS (SELECT ... FROM step1),  -- Depends on step1\nstep3 AS (SELECT ... FROM step2)   -- Depends on step2\n```\n\n### Parallel (Independent)\n```sql\nWITH\nsummary_a AS (SELECT ... FROM table_a),  -- Independent\nsummary_b AS (SELECT ... FROM table_b),  -- Independent\nsummary_c AS (SELECT ... FROM table_c)   -- Independent\nSELECT * FROM summary_a, summary_b, summary_c;\n```\n\n## Combining Results\n\n```sql\nWITH\nnorth_sales AS (SELECT SUM(amount) as total FROM sales WHERE region = 'North'),\nsouth_sales AS (SELECT SUM(amount) as total FROM sales WHERE region = 'South')\nSELECT \n    n.total as north,\n    s.total as south,\n    n.total + s.total as combined\nFROM north_sales n, south_sales s;\n```\n\n## Performance Benefits\n\n- Database optimizer can parallelize\n- Reduces overall query time\n- Better resource utilization\n\n---\n\n## üéØ Your Task\n\nWrite parallel CTEs to gather metrics from multiple tables simultaneously.",
    "starter_code": "-- Parallel CTEs for dashboard metrics\nWITH\ntotal_orders AS (\n    SELECT COUNT(*) as order_count, SUM(amount) as revenue\n    FROM orders\n    WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nactive_customers AS (\n    SELECT COUNT(DISTINCT customer_id) as customer_count\n    FROM orders\n    WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n),\ntop_products AS (\n    SELECT product_id, SUM(quantity) as units_sold\n    FROM order_items\n    WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY product_id\n    ORDER BY units_sold DESC\n    LIMIT 5\n)\n-- Combine all metrics\nSELECT \n    o.order_count,\n    o.revenue,\n    c.customer_count\nFROM total_orders o, active_customers c;",
    "solution_code": "WITH orders_sum AS (SELECT COUNT(*) as orders FROM orders),\ncustomers_sum AS (SELECT COUNT(*) as customers FROM customers)\nSELECT orders, customers FROM orders_sum, customers_sum;",
    "expected_output": "order_count|revenue|customer_count\n1250|125000|450"
  },
  "1208": {
    "title": "CTE Reuse",
    "chapter_title": "CTEs",
    "content": "# ‚ôªÔ∏è CTE Reuse: Reference Once, Use Many Times\n\n## Why Reuse CTEs?\n\nDefine a calculation once, use it multiple times:\n- No repeated subqueries\n- Cleaner code\n- Consistent results\n\n## Multiple References\n\n```sql\nWITH monthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) as month,\n        SUM(amount) as total\n    FROM orders\n    GROUP BY 1\n)\nSELECT \n    m1.month,\n    m1.total as current_month,\n    m2.total as previous_month,\n    m1.total - m2.total as growth\nFROM monthly_sales m1\nLEFT JOIN monthly_sales m2 \n    ON m1.month = m2.month + INTERVAL '1 month';\n```\n\n## Aggregating at Multiple Levels\n\n```sql\nWITH order_totals AS (\n    SELECT customer_id, SUM(amount) as total\n    FROM orders GROUP BY customer_id\n)\nSELECT \n    AVG(total) as avg_per_customer,\n    MAX(total) as max_customer,\n    SUM(total) as grand_total,\n    (SELECT COUNT(*) FROM order_totals WHERE total > 1000) as vip_count\nFROM order_totals;\n```\n\n## Compare Subsets\n\n```sql\nWITH all_data AS (\n    SELECT product_id, SUM(quantity) as total\n    FROM sales GROUP BY product_id\n)\n-- Compare different product groups using same CTE\nSELECT \n    (SELECT SUM(total) FROM all_data WHERE product_id LIKE 'A%') as a_products,\n    (SELECT SUM(total) FROM all_data WHERE product_id LIKE 'B%') as b_products;\n```\n\n---\n\n## üéØ Your Task\n\nUse a CTE multiple times for month-over-month comparison.",
    "starter_code": "-- Reuse CTE for self-comparison\nWITH monthly_metrics AS (\n    SELECT \n        DATE_TRUNC('month', order_date) as month,\n        COUNT(*) as orders,\n        SUM(amount) as revenue\n    FROM orders\n    GROUP BY 1\n)\nSELECT \n    current.month,\n    current.revenue as current_revenue,\n    previous.revenue as prev_revenue,\n    ROUND(100.0 * (current.revenue - previous.revenue) / previous.revenue, 2) as growth_pct\nFROM monthly_metrics current\nLEFT JOIN monthly_metrics previous \n    ON current.month = previous.month + INTERVAL '1 month'\nORDER BY current.month;",
    "solution_code": "WITH totals AS (SELECT month, SUM(amount) as total FROM orders GROUP BY month)\nSELECT t1.month, t2.total - t1.total as change FROM totals t1 JOIN totals t2 ON t1.month = t2.month + 1;",
    "expected_output": "month|current_revenue|prev_revenue|growth_pct\n2024-02|52000|45000|15.56\n2024-03|58000|52000|11.54"
  },
  "1209": {
    "title": "Debug with CTEs",
    "chapter_title": "CTEs",
    "content": "# üîß Debugging with CTEs: Inspect Each Step\n\n## Why CTEs for Debugging?\n\nCTEs let you break queries into steps and inspect each one:\n- See intermediate results\n- Isolate where logic goes wrong\n- Test assumptions\n\n## The Debugging Pattern\n\n```sql\nWITH step1 AS (\n    SELECT * FROM orders WHERE status = 'pending'\n),\nstep2 AS (\n    SELECT * FROM step1 WHERE amount > 100\n),\nstep3 AS (\n    SELECT customer_id, SUM(amount) as total FROM step2 GROUP BY customer_id\n)\n-- Debug: inspect any step\nSELECT * FROM step2;  -- Change to step1, step3 to inspect others\n```\n\n## Checking Row Counts\n\n```sql\nWITH base_data AS (\n    SELECT * FROM orders WHERE date >= '2024-01-01'\n),\nafter_filter AS (\n    SELECT * FROM base_data WHERE status = 'complete'\n)\n-- Check counts\nSELECT \n    (SELECT COUNT(*) FROM base_data) as before_filter,\n    (SELECT COUNT(*) FROM after_filter) as after_filter;\n```\n\n## Adding Debug Columns\n\n```sql\nWITH calculations AS (\n    SELECT *,\n        amount * 0.1 as tax,\n        amount * 0.9 as after_discount,\n        -- Debug: see intermediate calculation\n        'step1' as debug_stage\n    FROM orders\n)\nSELECT * FROM calculations WHERE id = 123;\n```\n\n## Production vs Debug\n\n```sql\n-- Debug version (show all columns)\nSELECT * FROM final_step;\n\n-- Production version (select specific columns)\nSELECT customer_id, total FROM final_step;\n```\n\n---\n\n## üéØ Your Task\n\nUse CTEs to debug a multi-step calculation.",
    "starter_code": "-- Debug complex calculation step by step\nWITH raw_orders AS (\n    -- Step 1: Get base data\n    SELECT order_id, customer_id, amount\n    FROM orders\n    WHERE order_date >= '2024-01-01'\n),\nwith_discount AS (\n    -- Step 2: Apply discount logic\n    SELECT *,\n        CASE WHEN amount > 500 THEN amount * 0.9 ELSE amount END as discounted\n    FROM raw_orders\n),\nwith_tax AS (\n    -- Step 3: Add tax\n    SELECT *,\n        discounted * 1.08 as final_amount\n    FROM with_discount\n)\n-- Debug: check each step\nSELECT * FROM with_tax WHERE order_id = 123;",
    "solution_code": "WITH step1 AS (SELECT * FROM orders), step2 AS (SELECT * FROM step1 WHERE amount > 100)\nSELECT * FROM step2;  -- Change to step1 to debug earlier",
    "expected_output": "order_id|customer_id|amount|discounted|final_amount\n123|456|600|540|583.20"
  },
  "1210": {
    "title": "Window vs GROUP BY",
    "chapter_title": "Window Functions",
    "content": "# ü™ü Window Functions vs GROUP BY\n\n## The Conceptual Leap\n\n*   **GROUP BY**: Collapses rows. Input 100 rows -> Output 5 rows. You lose individual row detail.\n*   **Window Functions**: Retains rows. Input 100 rows -> Output 100 rows, but with extra info attached.\n\n## When to Use Which?\n\n| Goal | Use... |\n| :--- | :--- |\n| \"Total sales per region\" | `GROUP BY region` |\n| \"This order's sales vs region average\" | `AVG(sales) OVER (PARTITION BY region)` |\n\n## Syntax\n\n```sql\nFUNCTION_NAME(col) OVER (\n    PARTITION BY group_col\n    ORDER BY sort_col\n)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate the average salary for each department, but attach it to **every employee row** so we can verify if they are above/below average.",
    "starter_code": "SELECT \n    name, \n    department, \n    salary,\n    -- Calculate dept average without collapsing rows\n    AVG(salary) OVER (PARTITION BY department) as dept_avg,\n    salary - AVG(salary) OVER (PARTITION BY department) as diff\nFROM employees\nORDER BY department, salary DESC;",
    "solution_code": "SELECT AVG(x) OVER() FROM t;",
    "expected_output": "name|department|salary|dept_avg|diff\nAlice|Eng|120000|110000|10000\nBob|Eng|100000|110000|-10000"
  },
  "1211": {
    "title": "Top N per Group",
    "chapter_title": "Window Functions",
    "content": "# üèÜ Top N Per Group: The Classic Interview Question\n\n## The Problem\n\n\"Find the top 3 highest-paid employees in **each** department.\"\n\nUsing `GROUP BY`, this is incredibly hard. You can find the Max, but not the \"Top 3 rows\".\n\n## The Solution: ROW_NUMBER()\n\n1.  Rank employees *within* their department.\n2.  Filter for rank <= 3.\n\n```sql\nWITH ranked AS (\n    SELECT \n        *, \n        ROW_NUMBER() OVER (PARTITION BY dept_id ORDER BY salary DESC) as rn\n    FROM employees\n)\nSELECT * FROM ranked WHERE rn <= 3;\n```\n\n## Understanding the Parts\n\n*   `PARTITION BY dept_id`: Restart numbering for each department.\n*   `ORDER BY salary DESC`: Highest salary gets number 1.\n\n---\n\n## üéØ Your Task\n\nFind the top 2 recent orders for every customer.",
    "starter_code": "WITH ranked_orders AS (\n    SELECT \n        customer_id,\n        order_id,\n        amount,\n        order_date,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date DESC\n        ) as rank_num\n    FROM orders\n)\nSELECT * \nFROM ranked_orders \nWHERE rank_num <= 2\nORDER BY customer_id, rank_num;",
    "solution_code": "SELECT * FROM (SELECT ROW_NUMBER() OVER() as rn FROM t) WHERE rn <= 2;",
    "expected_output": "customer_id|order_id|amount|rank_num\n1|105|500|1\n1|102|200|2\n2|106|300|1"
  },
  "1212": {
    "title": "Percentile Rank",
    "chapter_title": "Window Functions",
    "content": "# üìä Percentiles: Understanding Distribution\n\n## Who is in the Top 1%?\n\n`PERCENT_RANK()` calculates the relative rank of a row, from 0 to 1.\n\n*   `0.00` = Lowest value\n*   `1.00` = Highest value\n*   `0.95` = Higher than 95% of peers\n\n## Syntax\n\n```sql\nSELECT \n    student_name,\n    score,\n    PERCENT_RANK() OVER (ORDER BY score) as pct\nFROM exam_results;\n```\n\n## NTILE(n)\n\nAlternatively, break data into `n` buckets:\n\n*   `NTILE(4)` = Quartiles (1=Bottom 25%, 4=Top 25%)\n*   `NTILE(10)` = Deciles\n*   `NTILE(100)` = Percentiles\n\n---\n\n## üéØ Your Task\n\nIdentify orders that are in the top 10% (90th percentile) by amount.",
    "starter_code": "WITH percentiles AS (\n    SELECT \n        order_id,\n        amount,\n        PERCENT_RANK() OVER (ORDER BY amount) as p_rank\n    FROM orders\n)\nSELECT *\nFROM percentiles\nWHERE p_rank >= 0.90\nORDER BY amount DESC;",
    "solution_code": "SELECT PERCENT_RANK() OVER (ORDER BY val) FROM t;",
    "expected_output": "order_id|amount|p_rank\n99|5000|1.0\n55|4800|0.95"
  },
  "1213": {
    "title": "Dense vs Regular",
    "chapter_title": "Window Functions",
    "content": "# ü•á RANK vs DENSE_RANK: Handling Ties\n\n## The Olympics Problem\n\nIf two athletes tie for Gold, what happens to Silver?\n\n*   **ROW_NUMBER()**: Tie-breaker is arbitrary. 1, 2, 3.\n*   **RANK()**: Skips numbers. 1, 1, 3. (Silver is skipped!)\n*   **DENSE_RANK()**: No gaps. 1, 1, 2. (Next person gets Silver).\n\n## Visual Comparison\n\n| Score | ROW_NUMBER | RANK | DENSE_RANK |\n| :--- | :--- | :--- | :--- |\n| 100 | 1 | 1 | 1 |\n| 100 | 2 | 1 | 1 |\n| 90 | 3 | 3 | 2 |\n| 80 | 4 | 4 | 3 |\n\n## When to Use\n\n*   **Top N**: Usually `ROW_NUMBER` (gives exactly N rows).\n*   **Leaderboards**: Usually `DENSE_RANK` (everyone wants a rank).\n\n---\n\n## üéØ Your Task\n\nRank sales reps by revenue. If they tie, they should share the rank, and the next person should get the immediate next number.",
    "starter_code": "SELECT \n    rep_name,\n    revenue,\n    RANK() OVER (ORDER BY revenue DESC) as rnk,\n    DENSE_RANK() OVER (ORDER BY revenue DESC) as dense_rnk\nFROM sales_reps\nORDER BY revenue DESC;",
    "solution_code": "SELECT DENSE_RANK() OVER (ORDER BY col) FROM t;",
    "expected_output": "rep_name|revenue|rnk|dense_rnk\nAlice|1000|1|1\nBob|1000|1|1\nCharlie|900|3|2"
  },
  "1214": {
    "title": "Cumulative Sum",
    "chapter_title": "Window Functions",
    "content": "# üìà Cumulative Sum: The Running Total\n\n## The Growing Number\n\n\"What is our total revenue YTD at the end of each day?\"\n\nYou need a sum that grows with each row. This is a **window with an ORDER BY** inside.\n\n```sql\nSUM(amount) OVER (ORDER BY date)\n```\n\n## How It Works\n\nIt sums all rows from the start of the partition *up to the current row*.\n\n| Date | Daily Sales | Running Total |\n| :--- | :--- | :--- |\n| Jan 1 | 100 | 100 |\n| Jan 2 | 50 | 150 (100+50) |\n| Jan 3 | 200 | 350 (150+200) |\n\n## Partitioned Running Total\n\nRunning total *per customer*:\n\n```sql\nSUM(amount) OVER (PARTITION BY customer_id ORDER BY date)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate a running total of website visits day by day.",
    "starter_code": "SELECT \n    visit_date,\n    daily_visits,\n    SUM(daily_visits) OVER (ORDER BY visit_date) as total_visits_to_date\nFROM site_traffic\nORDER BY visit_date;",
    "solution_code": "SELECT SUM(val) OVER (ORDER BY date) FROM t;",
    "expected_output": "visit_date|daily_visits|total_visits_to_date\n2024-01-01|100|100\n2024-01-02|150|250\n2024-01-03|120|370"
  },
  "1215": {
    "title": "Rolling Window",
    "chapter_title": "Window Functions",
    "content": "# üé¢ Rolling Windows: Moving Averages\n\n## Smoothing the Noise\n\nDaily data is spiky. A \"7-day moving average\" smooths it out to show trends.\n\n## Defining the Frame\n\nWe need to specify *which* rows to include in the sum/avg.\n\n```sql\nAVG(amount) OVER (\n    ORDER BY date\n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n)\n```\n\n*   `ROWS BETWEEN`: Defines the window frame.\n*   `6 PRECEDING`: Look back 6 rows.\n*   `CURRENT ROW`: Include today.\n*   Total = 7 rows (6 past + 1 today).\n\n## Ranges vs Rows\n\n*   `ROWS`: Physical rows (literally the last 6 lines).\n*   `RANGE`: Logical values (e.g., dates within last 7 days).\n\n---\n\n## üéØ Your Task\n\nCalculate a 3-day moving average of temperature (today + 2 days prior).",
    "starter_code": "SELECT \n    date,\n    temperature,\n    AVG(temperature) OVER (\n        ORDER BY date\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) as moving_avg_3d\nFROM weather_data\nORDER BY date;",
    "solution_code": "SELECT AVG(val) OVER (ORDER BY d ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) FROM t;",
    "expected_output": "date|temperature|moving_avg_3d\n2024-01-01|20|20.0\n2024-01-02|22|21.0\n2024-01-03|24|22.0\n2024-01-04|20|22.0"
  },
  "1216": {
    "title": "YTD Calculations",
    "chapter_title": "Window Functions",
    "content": "# üóìÔ∏è YTD and Period-over-Period\n\n## Year-To-Date (YTD)\n\nWe want the running total to **reset** every year. We use `PARTITION BY` on the year.\n\n```sql\nSUM(sales) OVER (\n    PARTITION BY EXTRACT(YEAR FROM date) \n    ORDER BY date\n)\n```\n\nWhen the year changes (e.g., to 2025), the sum resets to 0 and starts over.\n\n## Month-To-Date (MTD)\n\nPartition by Month (and Year!):\n\n```sql\nPARTITION BY to_char(date, 'YYYY-MM')\n```\n\n---\n\n## üéØ Your Task\n\nCalculate cumulative revenue that resets at the start of each new year.",
    "starter_code": "SELECT \n    date,\n    revenue,\n    EXTRACT(YEAR FROM date) as year,\n    SUM(revenue) OVER (\n        PARTITION BY EXTRACT(YEAR FROM date) \n        ORDER BY date\n    ) as ytd_revenue\nFROM daily_sales\nORDER BY date;",
    "solution_code": "SELECT SUM(val) OVER (PARTITION BY year ORDER BY date) FROM t;",
    "expected_output": "date|revenue|year|ytd_revenue\n2023-12-30|100|2023|5000\n2023-12-31|100|2023|5100\n2024-01-01|200|2024|200"
  },
  "1217": {
    "title": "Period Comparison",
    "chapter_title": "Window Functions",
    "content": "# üï∞Ô∏è Period Comparisons: LAG and LEAD\n\n## Looking Backwards\n\n\"How much did we grow compared to yesterday?\"\n\n`LAG(col, n)` allows you to peek at the row `n` steps behind the current one.\n\n```sql\nSELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) as yesterday_sales,\n    sales - LAG(sales, 1) OVER (ORDER BY date) as growth\nFROM daily_stats;\n```\n\n## Looking Forwards\n\n`LEAD(col, n)` looks ahead. Useful for calculating \"days since next order\" or gap analysis.\n\n## Handling NULLs\n\nThe first row has no \"yesterday\", so LAG returns NULL. You can specify a default:\n\n`LAG(sales, 1, 0)` -> Returns 0 instead of NULL.\n\n---\n\n## üéØ Your Task\n\nCompare today's stock price with yesterday's to calculate the daily change.",
    "starter_code": "SELECT \n    date,\n    price,\n    LAG(price, 1) OVER (ORDER BY date) as prev_price,\n    price - LAG(price, 1) OVER (ORDER BY date) as change\nFROM stock_prices\nORDER BY date;",
    "solution_code": "SELECT LAG(col) OVER(ORDER BY col) FROM t;",
    "expected_output": "date|price|prev_price|change\n2024-01-01|100|null|null\n2024-01-02|105|100|5"
  },
  "1218": {
    "title": "Period Grouping",
    "chapter_title": "Time-Series",
    "content": "# üìÖ Period Grouping: Aggregate by Time Buckets\n\n## What is Period Grouping?\n\nGroup data into time buckets for analysis:\n- Daily totals\n- Weekly averages\n- Monthly summaries\n- Quarterly reports\n\n## DATE_TRUNC for Clean Periods\n\n```sql\n-- Group by month\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(amount) as monthly_total\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;\n```\n\n## Common Time Buckets\n\n| Function | Groups By |\n|----------|-----------|\n| DATE_TRUNC('day', date) | Daily |\n| DATE_TRUNC('week', date) | Weekly (starts Monday) |\n| DATE_TRUNC('month', date) | Monthly |\n| DATE_TRUNC('quarter', date) | Quarterly |\n| DATE_TRUNC('year', date) | Yearly |\n\n## Example: Daily Sales Report\n\n```sql\nSELECT \n    DATE_TRUNC('day', created_at) as day,\n    COUNT(*) as orders,\n    SUM(amount) as revenue,\n    AVG(amount) as avg_order_value\nFROM orders\nGROUP BY DATE_TRUNC('day', created_at)\nORDER BY day DESC\nLIMIT 7;  -- Last 7 days\n```\n\n## With Running Totals\n\n```sql\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(amount) as monthly,\n    SUM(SUM(amount)) OVER (ORDER BY DATE_TRUNC('month', order_date)) as ytd\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date);\n```\n\n---\n\n## üéØ Your Task\n\nCreate a monthly summary report with totals and counts.",
    "starter_code": "-- Monthly sales summary\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    COUNT(*) as order_count,\n    SUM(amount) as total_revenue,\n    AVG(amount) as avg_order,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM orders\nWHERE order_date >= '2024-01-01'\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;",
    "solution_code": "SELECT DATE_TRUNC('month', order_date) as month, SUM(amount) as total\nFROM orders GROUP BY 1 ORDER BY 1;",
    "expected_output": "month|order_count|total_revenue|avg_order\n2024-01|150|45000|300\n2024-02|175|52500|300"
  },
  "1219": {
    "title": "Date Range",
    "chapter_title": "Time-Series",
    "content": "# üìÜ Date Range Queries: Filter by Time Period\n\n## Basic Date Filtering\n\n```sql\n-- Specific date\nWHERE order_date = '2024-01-15'\n\n-- After a date\nWHERE order_date > '2024-01-01'\n\n-- Date range\nWHERE order_date BETWEEN '2024-01-01' AND '2024-12-31'\n```\n\n## Current Date Functions\n\n```sql\nCURRENT_DATE     -- Today's date (no time)\nCURRENT_TIMESTAMP -- Now with time\nNOW()            -- Same as CURRENT_TIMESTAMP\n```\n\n## Common Date Range Patterns\n\n```sql\n-- This month\nWHERE order_date >= DATE_TRUNC('month', CURRENT_DATE)\n  AND order_date < DATE_TRUNC('month', CURRENT_DATE) + INTERVAL '1 month'\n\n-- Last 30 days\nWHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n\n-- This year\nWHERE order_date >= DATE_TRUNC('year', CURRENT_DATE)\n\n-- Previous month\nWHERE order_date >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '1 month'\n  AND order_date < DATE_TRUNC('month', CURRENT_DATE)\n```\n\n## Named Periods\n\n```sql\n-- Q1 2024\nWHERE order_date >= '2024-01-01' AND order_date < '2024-04-01'\n\n-- Dynamic current quarter\nWHERE order_date >= DATE_TRUNC('quarter', CURRENT_DATE)\n```\n\n## ‚ö†Ô∏è BETWEEN Gotcha\n\n`BETWEEN` is inclusive! For timestamps, be careful:\n```sql\n-- Might miss records on end date\nBETWEEN '2024-01-01' AND '2024-01-31'\n\n-- Better for timestamps\n>= '2024-01-01' AND < '2024-02-01'\n```\n\n---\n\n## üéØ Your Task\n\nFilter orders using various date range conditions.",
    "starter_code": "-- Various date range queries\n-- This month's orders\nSELECT 'This Month' as period, COUNT(*) as orders\nFROM orders\nWHERE order_date >= DATE_TRUNC('month', CURRENT_DATE)\nUNION ALL\n-- Last month\nSELECT 'Last Month', COUNT(*)\nFROM orders\nWHERE order_date >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '1 month'\n  AND order_date < DATE_TRUNC('month', CURRENT_DATE)\nUNION ALL\n-- This quarter\nSELECT 'This Quarter', COUNT(*)\nFROM orders\nWHERE order_date >= DATE_TRUNC('quarter', CURRENT_DATE);",
    "solution_code": "SELECT * FROM orders WHERE order_date BETWEEN '2024-01-01' AND '2024-12-31';",
    "expected_output": "period|orders\nThis Month|85\nLast Month|124\nThis Quarter|320"
  },
  "1220": {
    "title": "Age Calculation",
    "chapter_title": "Time-Series",
    "content": "# üéÇ Age Calculation: Days, Months, Years Between Dates\n\n## Calculate Age from Birth Date\n\n```sql\n-- Age in years\nSELECT \n    name,\n    birth_date,\n    EXTRACT(YEAR FROM AGE(birth_date)) as age_years\nFROM customers;\n```\n\n## Different Age Calculations\n\n### Years\n```sql\nEXTRACT(YEAR FROM AGE(CURRENT_DATE, birth_date))\n-- or\nDATE_PART('year', AGE(birth_date))\n```\n\n### Months\n```sql\nEXTRACT(MONTH FROM AGE(event_date))\n-- or total months:\n(EXTRACT(YEAR FROM AGE(date)) * 12) + EXTRACT(MONTH FROM AGE(date))\n```\n\n### Days\n```sql\nCURRENT_DATE - past_date  -- Returns integer days\n-- or\nEXTRACT(DAY FROM (CURRENT_DATE - past_date))\n```\n\n## Common Use Cases\n\n```sql\n-- Customer age groups\nSELECT \n    CASE \n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 18 THEN 'Under 18'\n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 35 THEN '18-34'\n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 55 THEN '35-54'\n        ELSE '55+'\n    END as age_group,\n    COUNT(*) as customers\nFROM customers\nGROUP BY 1;\n```\n\n## Account Age\n\n```sql\nSELECT \n    user_id,\n    created_at,\n    (CURRENT_DATE - created_at::date) as account_age_days\nFROM users;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate customer ages and group them into segments.",
    "starter_code": "-- Calculate customer ages\nSELECT \n    customer_id,\n    name,\n    birth_date,\n    EXTRACT(YEAR FROM AGE(birth_date)) as age,\n    CASE \n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 25 THEN 'Young Adult'\n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 45 THEN 'Adult'\n        WHEN EXTRACT(YEAR FROM AGE(birth_date)) < 65 THEN 'Middle Age'\n        ELSE 'Senior'\n    END as age_group\nFROM customers\nORDER BY age;",
    "solution_code": "SELECT name, EXTRACT(YEAR FROM AGE(birth_date)) as age FROM customers;",
    "expected_output": "customer_id|name|age|age_group\n1|Alice|28|Adult\n2|Bob|42|Adult"
  },
  "1221": {
    "title": "Business Days",
    "chapter_title": "Time-Series",
    "content": "# üìÖ Business Days: Excluding Weekends and Holidays\n\n## Why Business Days?\n\nMany business metrics exclude weekends:\n- SLA calculations\n- Business day shipping\n- Working day counts\n\n## Check if Weekend\n\n```sql\nSELECT \n    order_date,\n    EXTRACT(DOW FROM order_date) as day_of_week,\n    CASE \n        WHEN EXTRACT(DOW FROM order_date) IN (0, 6) THEN 'Weekend'\n        ELSE 'Weekday'\n    END as day_type\nFROM orders;\n```\n\n## Filter to Business Days Only\n\n```sql\nSELECT * FROM orders\nWHERE EXTRACT(DOW FROM order_date) NOT IN (0, 6);\n-- 0 = Sunday, 6 = Saturday\n```\n\n## Count Business Days Between Dates\n\n```sql\nWITH date_series AS (\n    SELECT generate_series(\n        '2024-01-01'::date, \n        '2024-01-31'::date, \n        '1 day'\n    )::date as d\n)\nSELECT COUNT(*) as business_days\nFROM date_series\nWHERE EXTRACT(DOW FROM d) NOT IN (0, 6);\n```\n\n## With Holiday Table\n\n```sql\nSELECT COUNT(*) as working_days\nFROM date_series d\nLEFT JOIN holidays h ON d.date = h.holiday_date\nWHERE EXTRACT(DOW FROM d.date) NOT IN (0, 6)\n  AND h.holiday_date IS NULL;\n```\n\n## Add N Business Days\n\n```sql\n-- Add 5 business days to a date (simplified)\nSELECT order_date + interval '7 days' as estimated_delivery\nFROM orders;  -- Rough estimate: 5 business ‚âà 7 calendar\n```\n\n---\n\n## üéØ Your Task\n\nFilter data to business days and count working days in a period.",
    "starter_code": "-- Analyze business days only\nSELECT \n    DATE_TRUNC('week', order_date) as week,\n    COUNT(*) as total_orders,\n    SUM(CASE WHEN EXTRACT(DOW FROM order_date) NOT IN (0, 6) THEN 1 ELSE 0 END) as weekday_orders,\n    SUM(CASE WHEN EXTRACT(DOW FROM order_date) IN (0, 6) THEN 1 ELSE 0 END) as weekend_orders\nFROM orders\nWHERE order_date >= '2024-01-01'\nGROUP BY 1\nORDER BY 1;",
    "solution_code": "SELECT * FROM orders WHERE EXTRACT(DOW FROM order_date) NOT IN (0, 6);",
    "expected_output": "week|total_orders|weekday_orders|weekend_orders\n2024-01-01|85|65|20\n2024-01-08|92|72|20"
  },
  "1222": {
    "title": "Date Intervals",
    "chapter_title": "Time-Series",
    "content": "# ‚è±Ô∏è Date Intervals: Add and Subtract Time\n\n## Adding Time to Dates\n\n```sql\n-- Add days\nCURRENT_DATE + INTERVAL '7 days'\n\n-- Add months\nCURRENT_DATE + INTERVAL '3 months'\n\n-- Add years\nCURRENT_DATE + INTERVAL '1 year'\n```\n\n## Subtracting Time\n\n```sql\n-- 30 days ago\nCURRENT_DATE - INTERVAL '30 days'\n\n-- Start of last month\nDATE_TRUNC('month', CURRENT_DATE) - INTERVAL '1 month'\n```\n\n## Common Interval Patterns\n\n| Expression | Result |\n|------------|--------|\n| + INTERVAL '1 hour' | Add 1 hour |\n| + INTERVAL '7 days' | Add 1 week |\n| + INTERVAL '1 month' | Add 1 month |\n| - INTERVAL '1 year' | Subtract 1 year |\n\n## Real-World Examples\n\n```sql\n-- Orders in last 30 days\nSELECT * FROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '30 days';\n\n-- Subscriptions expiring in 7 days\nSELECT * FROM subscriptions\nWHERE expiry_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '7 days';\n\n-- Users inactive for 90+ days\nSELECT * FROM users\nWHERE last_login < CURRENT_DATE - INTERVAL '90 days';\n```\n\n## Calculating Intervals Between Dates\n\n```sql\n-- Days between two dates\nend_date - start_date  -- Returns integer\n\n-- Months between\nEXTRACT(YEAR FROM AGE(end_date, start_date)) * 12 + \nEXTRACT(MONTH FROM AGE(end_date, start_date))\n```\n\n---\n\n## üéØ Your Task\n\nUse date intervals to filter and calculate time-based data.",
    "starter_code": "-- Find orders in different time windows\n-- Last 7 days\nSELECT 'Last 7 days' as period, COUNT(*) as orders\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '7 days'\nUNION ALL\n-- Last 30 days\nSELECT 'Last 30 days', COUNT(*)\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\nUNION ALL\n-- Last 90 days\nSELECT 'Last 90 days', COUNT(*)\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '90 days';",
    "solution_code": "SELECT * FROM orders WHERE order_date >= CURRENT_DATE - INTERVAL '30 days';",
    "expected_output": "period|orders\nLast 7 days|45\nLast 30 days|180\nLast 90 days|520"
  },
  "1223": {
    "title": "First-Time Users",
    "chapter_title": "Analytics",
    "content": "# üÜï First-Time Users: Identify New Visitors\n\n## The Problem\n\nDistinguish between first-time and returning users for analytics:\n- Conversion rate for new vs returning\n- First purchase behavior\n- Acquisition tracking\n\n## Method 1: Subquery for First Event\n\n```sql\nSELECT *\nFROM user_activity ua\nWHERE event_date = (\n    SELECT MIN(event_date) \n    FROM user_activity \n    WHERE user_id = ua.user_id\n);\n```\n\n## Method 2: Window Function\n\n```sql\nWITH ranked AS (\n    SELECT *,\n        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_date) as visit_num\n    FROM user_activity\n)\nSELECT * FROM ranked WHERE visit_num = 1;\n```\n\n## First Purchase Tracking\n\n```sql\nWITH first_orders AS (\n    SELECT \n        customer_id,\n        MIN(order_date) as first_order_date\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT \n    DATE_TRUNC('month', first_order_date) as cohort_month,\n    COUNT(*) as new_customers\nFROM first_orders\nGROUP BY 1\nORDER BY 1;\n```\n\n## Flagging First-Time vs Returning\n\n```sql\nSELECT \n    user_id,\n    event_date,\n    CASE WHEN visit_num = 1 THEN 'New' ELSE 'Returning' END as user_type\nFROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_date) as visit_num\n    FROM events\n) ranked;\n```\n\n---\n\n## üéØ Your Task\n\nIdentify first-time customers and analyze their acquisition by month.",
    "starter_code": "-- First-time customer analysis\nWITH first_orders AS (\n    SELECT \n        customer_id,\n        MIN(order_date) as first_order_date,\n        MIN(amount) as first_order_amount\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT \n    DATE_TRUNC('month', first_order_date) as acquisition_month,\n    COUNT(*) as new_customers,\n    AVG(first_order_amount) as avg_first_order\nFROM first_orders\nGROUP BY 1\nORDER BY 1;",
    "solution_code": "SELECT customer_id, MIN(order_date) as first_order FROM orders GROUP BY customer_id;",
    "expected_output": "acquisition_month|new_customers|avg_first_order\n2024-01|85|125.50\n2024-02|92|132.00"
  },
  "1224": {
    "title": "Return Visitors",
    "chapter_title": "Analytics",
    "content": "# üîÑ Return Visitors: Track User Retention\n\n## What Are Return Visitors?\n\nUsers who come back after their first visit. Key retention metric!\n\n## Calculating Return Rate\n\n```sql\nWITH visit_counts AS (\n    SELECT \n        user_id,\n        COUNT(*) as total_visits\n    FROM sessions\n    GROUP BY user_id\n)\nSELECT \n    CASE WHEN total_visits = 1 THEN '1 visit'\n         WHEN total_visits <= 5 THEN '2-5 visits'\n         ELSE '6+ visits'\n    END as visit_bucket,\n    COUNT(*) as user_count\nFROM visit_counts\nGROUP BY 1;\n```\n\n## Return Within N Days\n\n```sql\nWITH first_visit AS (\n    SELECT user_id, MIN(visit_date) as first\n    FROM visits GROUP BY user_id\n),\nreturn_visits AS (\n    SELECT \n        fv.user_id,\n        MIN(v.visit_date) as first_return\n    FROM first_visit fv\n    JOIN visits v ON fv.user_id = v.user_id\n    WHERE v.visit_date > fv.first\n    GROUP BY fv.user_id\n)\nSELECT \n    COUNT(DISTINCT rv.user_id) as returned,\n    COUNT(DISTINCT fv.user_id) as total,\n    ROUND(100.0 * COUNT(DISTINCT rv.user_id) / COUNT(DISTINCT fv.user_id), 2) as return_pct\nFROM first_visit fv\nLEFT JOIN return_visits rv ON fv.user_id = rv.user_id;\n```\n\n## Day-1, Day-7, Day-30 Retention\n\n```sql\n-- Users who returned within 7 days\nSELECT COUNT(DISTINCT v2.user_id) as d7_retained\nFROM visits v1\nJOIN visits v2 ON v1.user_id = v2.user_id\nWHERE v2.visit_date BETWEEN v1.first_visit + INTERVAL '1 day' \n                        AND v1.first_visit + INTERVAL '7 days';\n```\n\n---\n\n## üéØ Your Task\n\nCalculate return visitor rates and retention metrics.",
    "starter_code": "-- Return visitor analysis\nWITH user_visits AS (\n    SELECT \n        user_id,\n        MIN(visit_date) as first_visit,\n        MAX(visit_date) as last_visit,\n        COUNT(*) as total_visits\n    FROM sessions\n    GROUP BY user_id\n)\nSELECT \n    CASE \n        WHEN total_visits = 1 THEN 'One-time'\n        WHEN total_visits <= 3 THEN 'Occasional'\n        WHEN total_visits <= 10 THEN 'Regular'\n        ELSE 'Power User'\n    END as user_type,\n    COUNT(*) as users,\n    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM user_visits\nGROUP BY 1\nORDER BY users DESC;",
    "solution_code": "SELECT user_id, COUNT(*) as visits FROM sessions GROUP BY user_id HAVING COUNT(*) > 1;",
    "expected_output": "user_type|users|pct\nOne-time|1200|48.00\nOccasional|750|30.00\nRegular|400|16.00\nPower User|150|6.00"
  },
  "1225": {
    "title": "Funnel Metrics",
    "chapter_title": "Analytics",
    "content": "# üìä Funnel Analysis: Track Conversion Steps\n\n## What is a Funnel?\n\nUsers progress through stages. Track drop-off at each step:\n- Visit ‚Üí Sign Up ‚Üí Purchase\n- Page View ‚Üí Add to Cart ‚Üí Checkout ‚Üí Payment\n\n## Basic Funnel Query\n\n```sql\nSELECT \n    COUNT(DISTINCT CASE WHEN event = 'page_view' THEN user_id END) as view,\n    COUNT(DISTINCT CASE WHEN event = 'add_cart' THEN user_id END) as add_cart,\n    COUNT(DISTINCT CASE WHEN event = 'checkout' THEN user_id END) as checkout,\n    COUNT(DISTINCT CASE WHEN event = 'purchase' THEN user_id END) as purchase\nFROM events\nWHERE date >= '2024-01-01';\n```\n\n## Conversion Rates\n\n```sql\nWITH funnel AS (\n    SELECT \n        COUNT(DISTINCT CASE WHEN event = 'view' THEN user_id END) as step1,\n        COUNT(DISTINCT CASE WHEN event = 'cart' THEN user_id END) as step2,\n        COUNT(DISTINCT CASE WHEN event = 'purchase' THEN user_id END) as step3\n    FROM events\n)\nSELECT \n    step1 as views,\n    step2 as carts,\n    step3 as purchases,\n    ROUND(100.0 * step2 / step1, 2) as view_to_cart_pct,\n    ROUND(100.0 * step3 / step2, 2) as cart_to_purchase_pct,\n    ROUND(100.0 * step3 / step1, 2) as overall_conversion\nFROM funnel;\n```\n\n## Strict Sequence Funnels\n\nOnly count users who did steps in order:\n```sql\n-- User must have done step 1 before step 2\nSELECT user_id\nFROM events e1\nJOIN events e2 ON e1.user_id = e2.user_id\nWHERE e1.event = 'view' \n  AND e2.event = 'purchase'\n  AND e1.timestamp < e2.timestamp;\n```\n\n---\n\n## üéØ Your Task\n\nBuild a conversion funnel with step-by-step drop-off analysis.",
    "starter_code": "-- Conversion funnel analysis\nWITH funnel AS (\n    SELECT \n        COUNT(DISTINCT CASE WHEN event = 'page_view' THEN user_id END) as step1_view,\n        COUNT(DISTINCT CASE WHEN event = 'add_cart' THEN user_id END) as step2_cart,\n        COUNT(DISTINCT CASE WHEN event = 'checkout' THEN user_id END) as step3_checkout,\n        COUNT(DISTINCT CASE WHEN event = 'purchase' THEN user_id END) as step4_purchase\n    FROM events\n    WHERE event_date >= '2024-01-01'\n)\nSELECT \n    step1_view as \"Page Views\",\n    step2_cart as \"Add to Cart\",\n    ROUND(100.0 * step2_cart / step1_view, 1) as \"View‚ÜíCart %\",\n    step3_checkout as \"Checkout\",\n    ROUND(100.0 * step3_checkout / step2_cart, 1) as \"Cart‚ÜíCheckout %\",\n    step4_purchase as \"Purchase\",\n    ROUND(100.0 * step4_purchase / step1_view, 1) as \"Overall Conv %\"\nFROM funnel;",
    "solution_code": "SELECT event, COUNT(DISTINCT user_id) as users FROM events GROUP BY event;",
    "expected_output": "Page Views|Add to Cart|View‚ÜíCart %|Checkout|Cart‚ÜíCheckout %|Purchase|Overall Conv %\n10000|2500|25.0|1500|60.0|1200|12.0"
  },
  "1226": {
    "title": "Fuzzy Duplicates",
    "chapter_title": "Data Cleaning",
    "content": "# üîç Fuzzy Duplicate Detection: Find Similar Records\n\n## The Problem\n\nExact matching misses near-duplicates:\n- \"John Smith\" vs \"Jon Smith\"\n- \"123 Main St\" vs \"123 Main Street\"\n- \"Acme Corp\" vs \"Acme Corporation\"\n\n## String Similarity Functions\n\n### Levenshtein Distance\nEdit distance‚Äînumber of character changes needed:\n```sql\nSELECT levenshtein('John', 'Jon');  -- 1 (one deletion)\n```\n\n### Soundex\nPhonetic matching:\n```sql\nSELECT \n    name,\n    soundex(name)\nFROM customers\nWHERE soundex(name) = soundex('Smith');\n-- Matches: Smith, Smyth, Smithe\n```\n\n### Trigram Similarity (PostgreSQL)\n```sql\n-- Enable extension\nCREATE EXTENSION pg_trgm;\n\nSELECT similarity('John Smith', 'Jon Smith');  -- 0.75\nSELECT 'John Smith' % 'Jon Smith';  -- true if similar\n```\n\n## Finding Fuzzy Duplicates\n\n```sql\nSELECT \n    a.id, a.name,\n    b.id, b.name,\n    similarity(a.name, b.name) as sim_score\nFROM customers a\nJOIN customers b ON a.id < b.id\nWHERE similarity(a.name, b.name) > 0.7;\n```\n\n## Clustering Duplicates\n\n```sql\n-- Group by Soundex\nSELECT soundex(name) as group_key, array_agg(name)\nFROM customers\nGROUP BY soundex(name)\nHAVING COUNT(*) > 1;\n```\n\n---\n\n## üéØ Your Task\n\nFind potential duplicate customer records using fuzzy matching.",
    "starter_code": "-- Find fuzzy duplicate customers\nSELECT \n    a.customer_id as id_1,\n    a.name as name_1,\n    b.customer_id as id_2,\n    b.name as name_2,\n    similarity(a.name, b.name) as name_similarity\nFROM customers a\nJOIN customers b ON a.customer_id < b.customer_id\nWHERE similarity(a.name, b.name) > 0.6\n   OR levenshtein(LOWER(a.name), LOWER(b.name)) <= 2\nORDER BY name_similarity DESC;",
    "solution_code": "SELECT a.name, b.name, similarity(a.name, b.name) FROM customers a JOIN customers b ON a.id < b.id WHERE similarity(a.name, b.name) > 0.7;",
    "expected_output": "id_1|name_1|id_2|name_2|name_similarity\n101|John Smith|205|Jon Smith|0.82\n103|Acme Corp|210|Acme Corporation|0.75"
  },
  "1227": {
    "title": "Keep Latest",
    "chapter_title": "Data Cleaning",
    "content": "# üïê Keep Latest Record: Deduplication by Recency\n\n## The Problem\n\nMultiple records for the same entity exist. You want only the most recent:\n- Latest order per customer\n- Most recent login per user\n- Current status (not historical)\n\n## Method 1: DISTINCT ON (PostgreSQL)\n\n```sql\nSELECT DISTINCT ON (customer_id) *\nFROM orders\nORDER BY customer_id, order_date DESC;\n```\n\nDISTINCT ON keeps the first row for each customer_id after ordering.\n\n## Method 2: Window Function + Filter\n\n```sql\nWITH ranked AS (\n    SELECT *,\n        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rn\n    FROM orders\n)\nSELECT * FROM ranked WHERE rn = 1;\n```\n\n## Method 3: Subquery with MAX\n\n```sql\nSELECT o.*\nFROM orders o\nJOIN (\n    SELECT customer_id, MAX(order_date) as latest\n    FROM orders\n    GROUP BY customer_id\n) latest_orders\nON o.customer_id = latest_orders.customer_id \nAND o.order_date = latest_orders.latest;\n```\n\n## Which to Use?\n\n| Method | Pros | Cons |\n|--------|------|------|\n| DISTINCT ON | Simple, fast | PostgreSQL only |\n| ROW_NUMBER | Standard SQL | More verbose |\n| Subquery + MAX | Works everywhere | Can return ties |\n\n---\n\n## üéØ Your Task\n\nGet the most recent order for each customer.",
    "starter_code": "-- Get latest order per customer using window function\nWITH ranked_orders AS (\n    SELECT \n        customer_id,\n        order_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date DESC\n        ) as rn\n    FROM orders\n)\nSELECT \n    customer_id,\n    order_id,\n    order_date,\n    amount\nFROM ranked_orders\nWHERE rn = 1\nORDER BY customer_id;",
    "solution_code": "SELECT DISTINCT ON (customer_id) * FROM orders ORDER BY customer_id, order_date DESC;",
    "expected_output": "customer_id|order_id|order_date|amount\n1|1001|2024-03-15|250\n2|1050|2024-03-14|175"
  },
  "1228": {
    "title": "IQR Method",
    "chapter_title": "Data Cleaning",
    "content": "# üìä IQR Method: Statistical Outlier Detection\n\n## What is IQR?\n\n**Interquartile Range (IQR)** = Q3 - Q1, measuring the spread of the middle 50% of data.\n\n## The Rule\n\nOutliers are values:\n- Below Q1 - 1.5 √ó IQR\n- Above Q3 + 1.5 √ó IQR\n\n## SQL Implementation\n\n```sql\nWITH stats AS (\n    SELECT \n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY value) as q1,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY value) as q3\n    FROM data\n),\nbounds AS (\n    SELECT \n        q1,\n        q3,\n        q3 - q1 as iqr,\n        q1 - 1.5 * (q3 - q1) as lower_bound,\n        q3 + 1.5 * (q3 - q1) as upper_bound\n    FROM stats\n)\nSELECT d.*\nFROM data d, bounds b\nWHERE d.value BETWEEN b.lower_bound AND b.upper_bound;\n```\n\n## Finding Outliers\n\n```sql\n-- Find outliers (not filter them out)\nSELECT * FROM data d, bounds b\nWHERE d.value < b.lower_bound OR d.value > b.upper_bound;\n```\n\n## Why 1.5√óIQR?\n\n| Multiplier | Catches |\n|------------|---------|\n| 1.5 √ó IQR | Mild outliers |\n| 3 √ó IQR | Extreme outliers only |\n\n---\n\n## üéØ Your Task\n\nIdentify and remove outliers using the IQR method.",
    "starter_code": "-- Detect outliers using IQR method\nWITH stats AS (\n    SELECT \n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY amount) as q1,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY amount) as q3\n    FROM orders\n),\nbounds AS (\n    SELECT \n        q1,\n        q3,\n        (q3 - q1) as iqr,\n        q1 - 1.5 * (q3 - q1) as lower_bound,\n        q3 + 1.5 * (q3 - q1) as upper_bound\n    FROM stats\n)\n-- Show outliers\nSELECT o.order_id, o.amount, \n    CASE \n        WHEN o.amount < b.lower_bound THEN 'Low Outlier'\n        WHEN o.amount > b.upper_bound THEN 'High Outlier'\n    END as outlier_type\nFROM orders o, bounds b\nWHERE o.amount < b.lower_bound OR o.amount > b.upper_bound;",
    "solution_code": "WITH stats AS (SELECT PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY amount) as q1,\nPERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY amount) as q3 FROM orders)\nSELECT * FROM orders WHERE amount BETWEEN (SELECT q1-1.5*(q3-q1) FROM stats) AND (SELECT q3+1.5*(q3-q1) FROM stats);",
    "expected_output": "order_id|amount|outlier_type\n1001|5000|High Outlier\n1050|2|Low Outlier"
  },
  "1229": {
    "title": "Z-Score Filter",
    "chapter_title": "Data Cleaning",
    "content": "# üìä Z-Score: Statistical Outlier Detection\n\n## What is Z-Score?\n\nZ-score measures how many standard deviations a value is from the mean:\n```\nZ = (value - mean) / standard_deviation\n```\n\n## Interpretation\n\n| Z-Score | Meaning |\n|---------|---------|\n| 0 | At the mean |\n| ¬±1 | Within 1 std dev (68% of data) |\n| ¬±2 | Within 2 std dev (95% of data) |\n| ¬±3 | Within 3 std dev (99.7% of data) |\n\n## Calculate Z-Scores in SQL\n\n```sql\nWITH stats AS (\n    SELECT \n        AVG(amount) as mean_val,\n        STDDEV(amount) as std_val\n    FROM orders\n)\nSELECT \n    o.order_id,\n    o.amount,\n    (o.amount - s.mean_val) / NULLIF(s.std_val, 0) as z_score\nFROM orders o, stats s;\n```\n\n## Filter Outliers (|Z| > 3)\n\n```sql\nWITH stats AS (\n    SELECT AVG(amount) as mean, STDDEV(amount) as std\n    FROM orders\n)\nSELECT *\nFROM orders o, stats s\nWHERE ABS((o.amount - s.mean) / NULLIF(s.std, 0)) <= 3;\n```\n\n## Find Outliers\n\n```sql\n-- Show only outliers\nWHERE ABS((o.amount - s.mean) / NULLIF(s.std, 0)) > 3;\n```\n\n## Z-Score by Group\n\n```sql\nWITH group_stats AS (\n    SELECT category, AVG(price) as mean, STDDEV(price) as std\n    FROM products\n    GROUP BY category\n)\nSELECT p.*, (p.price - g.mean) / NULLIF(g.std, 0) as z_score\nFROM products p\nJOIN group_stats g ON p.category = g.category;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate Z-scores and filter outliers using statistical methods.",
    "starter_code": "-- Calculate Z-scores and flag outliers\nWITH stats AS (\n    SELECT \n        AVG(amount) as mean_amount,\n        STDDEV(amount) as std_amount\n    FROM orders\n)\nSELECT \n    order_id,\n    amount,\n    ROUND((amount - mean_amount) / NULLIF(std_amount, 0), 2) as z_score,\n    CASE \n        WHEN ABS((amount - mean_amount) / NULLIF(std_amount, 0)) > 3 THEN 'Extreme Outlier'\n        WHEN ABS((amount - mean_amount) / NULLIF(std_amount, 0)) > 2 THEN 'Mild Outlier'\n        ELSE 'Normal'\n    END as status\nFROM orders, stats\nORDER BY ABS(z_score) DESC\nLIMIT 10;",
    "solution_code": "WITH stats AS (SELECT AVG(amount) as m, STDDEV(amount) as s FROM orders)\nSELECT *, (amount - m) / s as z FROM orders, stats WHERE ABS((amount - m) / s) > 3;",
    "expected_output": "order_id|amount|z_score|status\n1001|5000|4.25|Extreme Outlier\n1050|4500|3.82|Extreme Outlier"
  },
  "1230": {
    "title": "Standard Deviation",
    "chapter_title": "Data Cleaning",
    "content": "# üìä Standard Deviation: Measure Data Spread\n\n## What is Standard Deviation?\n\nMeasures how spread out values are from the mean:\n- Low std dev = clustered around mean\n- High std dev = wide spread\n\n## SQL Functions\n\n```sql\n-- Population standard deviation\nSTDDEV_POP(column)\n\n-- Sample standard deviation (most common)\nSTDDEV(column)  -- or STDDEV_SAMP(column)\n```\n\n## Example\n\n```sql\nSELECT \n    department,\n    AVG(salary) as avg_salary,\n    STDDEV(salary) as salary_stddev,\n    MIN(salary) as min_salary,\n    MAX(salary) as max_salary\nFROM employees\nGROUP BY department;\n```\n\n## Coefficient of Variation\n\nRelative standard deviation (for comparison across groups):\n\n```sql\nSELECT \n    category,\n    AVG(price) as avg_price,\n    STDDEV(price) as stddev,\n    ROUND(100.0 * STDDEV(price) / AVG(price), 2) as cv_percent\nFROM products\nGROUP BY category;\n```\n\n## With Variance\n\n```sql\nSELECT \n    VARIANCE(amount) as variance,  -- Squared units\n    STDDEV(amount) as stddev       -- Original units\nFROM orders;\n```\n\n## Detecting Unusual Values\n\nValues more than 2 std devs from mean are unusual:\n\n```sql\nWITH stats AS (\n    SELECT AVG(amount) as mean, STDDEV(amount) as std FROM orders\n)\nSELECT * FROM orders, stats\nWHERE amount > mean + 2 * std OR amount < mean - 2 * std;\n```\n\n---\n\n## üéØ Your Task\n\nCalculate standard deviation to analyze data spread.",
    "starter_code": "-- Practice exercise\\nSELECT * FROM sample_table;",
    "solution_code": "SELECT * FROM sample_table;",
    "expected_output": "sample output"
  },
  "1231": {
    "title": "Cohort Retention",
    "chapter_title": "Analytics",
    "content": "# üìä Cohort Retention: Track Users Over Time\n\n## What is Cohort Analysis?\n\nGroup users by their signup/acquisition month, then track their behavior over time.\n\n## The Classic Retention Table\n\n```\n         M0    M1    M2    M3\nJan-24  100%   60%   45%   35%\nFeb-24  100%   55%   40%   --\nMar-24  100%   58%   --    --\n```\n\n## Building Cohort Retention\n\n```sql\nWITH cohorts AS (\n    SELECT \n        user_id,\n        DATE_TRUNC('month', MIN(event_date)) as cohort_month\n    FROM events\n    GROUP BY user_id\n),\nactivity AS (\n    SELECT \n        c.cohort_month,\n        DATE_TRUNC('month', e.event_date) as activity_month,\n        COUNT(DISTINCT e.user_id) as users\n    FROM events e\n    JOIN cohorts c ON e.user_id = c.user_id\n    GROUP BY 1, 2\n)\nSELECT \n    cohort_month,\n    activity_month,\n    users,\n    EXTRACT(MONTH FROM age(activity_month, cohort_month)) as months_since\nFROM activity\nORDER BY 1, 2;\n```\n\n## Calculating Retention Percentage\n\n```sql\nWITH base AS (\n    -- Get cohort base counts\n    SELECT cohort_month, COUNT(DISTINCT user_id) as cohort_size\n    FROM cohorts GROUP BY 1\n)\nSELECT \n    a.cohort_month,\n    a.months_since,\n    ROUND(100.0 * a.users / b.cohort_size, 1) as retention_pct\nFROM activity a\nJOIN base b ON a.cohort_month = b.cohort_month;\n```\n\n---\n\n## üéØ Your Task\n\nBuild a cohort retention analysis for user engagement.",
    "starter_code": "-- Cohort retention analysis\nWITH cohorts AS (\n    SELECT \n        user_id,\n        DATE_TRUNC('month', MIN(order_date)) as cohort_month\n    FROM orders\n    GROUP BY user_id\n),\ncohort_size AS (\n    SELECT cohort_month, COUNT(DISTINCT user_id) as size\n    FROM cohorts GROUP BY 1\n),\nmonthly_activity AS (\n    SELECT \n        c.cohort_month,\n        DATE_TRUNC('month', o.order_date) as order_month,\n        COUNT(DISTINCT o.customer_id) as active_users\n    FROM orders o\n    JOIN cohorts c ON o.customer_id = c.user_id\n    GROUP BY 1, 2\n)\nSELECT \n    ma.cohort_month,\n    ma.order_month,\n    ma.active_users,\n    ROUND(100.0 * ma.active_users / cs.size, 1) as retention_pct\nFROM monthly_activity ma\nJOIN cohort_size cs ON ma.cohort_month = cs.cohort_month\nORDER BY 1, 2;",
    "solution_code": "SELECT cohort_month, months_since, retention_pct FROM cohort_retention;",
    "expected_output": "cohort_month|order_month|retention_pct\n2024-01|2024-01|100.0\n2024-01|2024-02|65.5\n2024-01|2024-03|52.3"
  },
  "1232": {
    "title": "Completeness Check",
    "chapter_title": "Data Cleaning",
    "content": "# ‚úÖ Data Completeness: Measure Missing Values\n\n## Why Completeness Matters\n\nMissing data affects analysis quality:\n- Averages may be skewed\n- Joins may exclude records\n- Reports may undercount\n\n## Measure Null Percentage\n\n```sql\nSELECT \n    COUNT(*) as total_rows,\n    COUNT(email) as email_populated,\n    COUNT(*) - COUNT(email) as email_missing,\n    ROUND(100.0 * (COUNT(*) - COUNT(email)) / COUNT(*), 2) as email_null_pct\nFROM customers;\n```\n\n## Check All Columns\n\n```sql\nSELECT \n    'email' as column_name,\n    COUNT(*) - COUNT(email) as nulls,\n    ROUND(100.0 * (COUNT(*) - COUNT(email)) / COUNT(*), 2) as null_pct\nFROM customers\nUNION ALL\nSELECT 'phone', COUNT(*) - COUNT(phone), \n       ROUND(100.0 * (COUNT(*) - COUNT(phone)) / COUNT(*), 2)\nFROM customers\nUNION ALL\nSELECT 'address', COUNT(*) - COUNT(address),\n       ROUND(100.0 * (COUNT(*) - COUNT(address)) / COUNT(*), 2)\nFROM customers;\n```\n\n## Completeness by Segment\n\n```sql\nSELECT \n    customer_tier,\n    ROUND(100.0 * COUNT(email) / COUNT(*), 2) as email_complete_pct,\n    ROUND(100.0 * COUNT(phone) / COUNT(*), 2) as phone_complete_pct\nFROM customers\nGROUP BY customer_tier;\n```\n\n## Completeness Over Time\n\n```sql\nSELECT \n    DATE_TRUNC('month', created_at) as month,\n    ROUND(100.0 * COUNT(email) / COUNT(*), 2) as email_completeness\nFROM customers\nGROUP BY 1\nORDER BY 1;\n```\n\n---\n\n## üéØ Your Task\n\nCreate a completeness report showing null percentages for key columns.",
    "starter_code": "-- Data completeness report\nSELECT \n    'customer_name' as column_name,\n    COUNT(*) as total_rows,\n    COUNT(customer_name) as populated,\n    COUNT(*) - COUNT(customer_name) as missing,\n    ROUND(100.0 * COUNT(customer_name) / COUNT(*), 1) as completeness_pct\nFROM customers\nUNION ALL\nSELECT 'email', COUNT(*), COUNT(email), COUNT(*) - COUNT(email),\n       ROUND(100.0 * COUNT(email) / COUNT(*), 1)\nFROM customers\nUNION ALL\nSELECT 'phone', COUNT(*), COUNT(phone), COUNT(*) - COUNT(phone),\n       ROUND(100.0 * COUNT(phone) / COUNT(*), 1)\nFROM customers\nORDER BY completeness_pct;",
    "solution_code": "SELECT COUNT(*) - COUNT(email) as missing_emails, 100.0 * COUNT(email)/COUNT(*) as pct FROM customers;",
    "expected_output": "column_name|total_rows|populated|missing|completeness_pct\nphone|1000|650|350|65.0\nemail|1000|920|80|92.0\ncustomer_name|1000|1000|0|100.0"
  },
  "1233": {
    "title": "Consistency Check",
    "chapter_title": "Data Cleaning",
    "content": "# ‚úÖ Data Consistency Checks: Validate Your Data\n\n## Why Consistency Checks?\n\nData pipelines can fail silently. Validate with checks:\n- Referential integrity\n- Expected ranges\n- Business rules\n- Completeness\n\n## Common Checks\n\n### Referential Integrity\n```sql\n-- Orders with no matching customer\nSELECT o.order_id\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL;\n```\n\n### Value Range\n```sql\n-- Prices outside expected range\nSELECT * FROM products\nWHERE price < 0 OR price > 10000;\n```\n\n### Null Checks\n```sql\n-- Required fields that are null\nSELECT * FROM customers\nWHERE email IS NULL OR name IS NULL;\n```\n\n### Duplicate Checks\n```sql\n-- Duplicate primary keys\nSELECT id, COUNT(*) \nFROM table \nGROUP BY id \nHAVING COUNT(*) > 1;\n```\n\n## Building a Validation Report\n\n```sql\nSELECT \n    'Orphan Orders' as check_name,\n    COUNT(*) as issues\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL\nUNION ALL\nSELECT 'Invalid Prices', COUNT(*)\nFROM products WHERE price < 0\nUNION ALL\nSELECT 'Missing Emails', COUNT(*)\nFROM customers WHERE email IS NULL;\n```\n\n---\n\n## üéØ Your Task\n\nCreate data validation checks for common data quality issues.",
    "starter_code": "-- Data consistency validation report\nSELECT 'Orphan Orders' as check_name, \n       COUNT(*) as issue_count,\n       CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END as status\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL\nUNION ALL\nSELECT 'Negative Amounts', COUNT(*),\n       CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END\nFROM orders WHERE amount < 0\nUNION ALL\nSELECT 'Missing Customer Email', COUNT(*),\n       CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END\nFROM customers WHERE email IS NULL\nUNION ALL\nSELECT 'Duplicate Order IDs', COUNT(*),\n       CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END\nFROM (SELECT order_id FROM orders GROUP BY order_id HAVING COUNT(*) > 1) dupes;",
    "solution_code": "SELECT 'Missing Emails' as check, COUNT(*) FROM customers WHERE email IS NULL;",
    "expected_output": "check_name|issue_count|status\nOrphan Orders|0|PASS\nNegative Amounts|3|FAIL\nMissing Customer Email|15|FAIL"
  },
  "1234": {
    "title": "Dependency Analysis",
    "chapter_title": "Database Design",
    "content": "# üîó Dependency Analysis: Understand Table Relationships\n\n## Why Analyze Dependencies?\n\nBefore deleting, modifying, or migrating data, understand what depends on it!\n\n## Find Foreign Key Dependencies\n\n```sql\n-- PostgreSQL: Find all FKs pointing to a table\nSELECT \n    tc.table_name as child_table,\n    kcu.column_name as fk_column,\n    ccu.table_name as parent_table,\n    ccu.column_name as pk_column\nFROM information_schema.table_constraints tc\nJOIN information_schema.key_column_usage kcu \n    ON tc.constraint_name = kcu.constraint_name\nJOIN information_schema.constraint_column_usage ccu \n    ON tc.constraint_name = ccu.constraint_name\nWHERE tc.constraint_type = 'FOREIGN KEY'\n  AND ccu.table_name = 'customers';\n```\n\n## Count Child Records\n\n```sql\n-- How many orders per customer?\nSELECT \n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name;\n```\n\n## Identify Orphan Risks\n\n```sql\n-- Records with no children (safe to delete)\nSELECT c.*\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nWHERE o.order_id IS NULL;\n\n-- Records with children (deletion blocked)\nSELECT c.*, COUNT(o.order_id) as child_count\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id;\n```\n\n## Cascade Impact Analysis\n\n```sql\n-- If I delete this customer, how much is affected?\nSELECT \n    COUNT(DISTINCT o.order_id) as orders_affected,\n    COUNT(DISTINCT oi.item_id) as items_affected,\n    SUM(oi.amount) as revenue_affected\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nWHERE c.customer_id = 123;\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze table dependencies before making schema changes.",
    "starter_code": "-- Analyze dependencies for customers table\nWITH customer_dependencies AS (\n    SELECT \n        c.customer_id,\n        c.name,\n        COUNT(DISTINCT o.order_id) as order_count,\n        COUNT(DISTINCT r.review_id) as review_count,\n        COALESCE(SUM(o.amount), 0) as total_spent\n    FROM customers c\n    LEFT JOIN orders o ON c.customer_id = o.customer_id\n    LEFT JOIN reviews r ON c.customer_id = r.customer_id\n    GROUP BY c.customer_id, c.name\n)\nSELECT \n    CASE \n        WHEN order_count = 0 AND review_count = 0 THEN 'Safe to delete'\n        WHEN order_count > 0 THEN 'Has orders - cascade required'\n        ELSE 'Has reviews only'\n    END as deletion_risk,\n    COUNT(*) as customer_count\nFROM customer_dependencies\nGROUP BY 1;",
    "solution_code": "SELECT c.customer_id, COUNT(o.order_id) as orders FROM customers c LEFT JOIN orders o ON c.customer_id = o.customer_id GROUP BY c.customer_id;",
    "expected_output": "deletion_risk|customer_count\nSafe to delete|150\nHas orders - cascade required|800\nHas reviews only|50"
  },
  "1235": {
    "title": "Dimension Building",
    "chapter_title": "Analytics",
    "content": "# üì¶ Dimension Tables: Reference Data for Analysis\n\n## What Are Dimensions?\n\n**Dimension tables** contain descriptive attributes for analysis:\n- Date dimension (day, week, month, quarter, year)\n- Product dimension (name, category, price tier)\n- Customer dimension (segment, region, tier)\n\n## Date Dimension Example\n\n```sql\nCREATE TABLE dim_date AS\nSELECT \n    date::date as date_key,\n    EXTRACT(YEAR FROM date) as year,\n    EXTRACT(QUARTER FROM date) as quarter,\n    EXTRACT(MONTH FROM date) as month,\n    EXTRACT(DOW FROM date) as day_of_week,\n    TO_CHAR(date, 'Day') as day_name,\n    CASE WHEN EXTRACT(DOW FROM date) IN (0,6) THEN true ELSE false END as is_weekend\nFROM generate_series('2020-01-01'::date, '2030-12-31'::date, '1 day') as date;\n```\n\n## Using Dimensions in Queries\n\n```sql\nSELECT \n    d.year,\n    d.quarter,\n    SUM(f.amount) as revenue\nFROM fact_sales f\nJOIN dim_date d ON f.order_date = d.date_key\nGROUP BY d.year, d.quarter\nORDER BY 1, 2;\n```\n\n## Benefits of Dimensions\n\n| Benefit | Example |\n|---------|---------|\n| Consistent labels | \"Q1 2024\" everywhere |\n| Easy filtering | `WHERE is_weekend = true` |\n| Pre-calculated values | Fiscal year, is_holiday |\n| No repeated logic | Logic lives in dimension |\n\n---\n\n## üéØ Your Task\n\nCreate and use a date dimension for enhanced reporting.",
    "starter_code": "-- Create a date dimension\nWITH date_dim AS (\n    SELECT \n        date::date as date_key,\n        EXTRACT(YEAR FROM date) as year,\n        EXTRACT(QUARTER FROM date) as quarter,\n        TO_CHAR(date, 'Month') as month_name,\n        EXTRACT(DOW FROM date) as day_of_week,\n        CASE WHEN EXTRACT(DOW FROM date) IN (0,6) THEN 'Weekend' ELSE 'Weekday' END as day_type\n    FROM generate_series('2024-01-01'::date, '2024-12-31'::date, '1 day') as date\n)\n-- Use the dimension for analysis\nSELECT \n    dd.day_type,\n    COUNT(*) as order_count,\n    SUM(o.amount) as total_revenue\nFROM orders o\nJOIN date_dim dd ON o.order_date = dd.date_key\nGROUP BY dd.day_type;",
    "solution_code": "SELECT EXTRACT(QUARTER FROM order_date) as quarter, SUM(amount) FROM orders GROUP BY 1;",
    "expected_output": "day_type|order_count|total_revenue\nWeekday|850|125000\nWeekend|150|22500"
  },
  "1236": {
    "title": "Fact Table Design",
    "chapter_title": "Database Design",
    "content": "# üìä Fact Tables: The Heart of Analytics\n\n## What is a Fact Table?\n\nA **fact table** stores the measurable events of your business‚Äîthe things you count, sum, and analyze:\n- **Sales transactions** (amounts, quantities)\n- **Web page views** (timestamps, durations)\n- **Customer interactions** (calls, emails)\n\n## Facts vs Dimensions\n\n| Fact Table | Dimension Table |\n|------------|-----------------|\n| Measurements | Descriptions |\n| Numbers you calculate | Text you filter by |\n| Many rows (transactions) | Fewer rows (entities) |\n| Has foreign keys | Has primary keys |\n\n## Anatomy of a Fact Table\n\n```sql\nCREATE TABLE fact_sales (\n    -- Foreign keys to dimensions\n    date_id       INT REFERENCES dim_date,\n    product_id    INT REFERENCES dim_product,\n    customer_id   INT REFERENCES dim_customer,\n    store_id      INT REFERENCES dim_store,\n    \n    -- Measures (the \"facts\")\n    quantity      INT,\n    unit_price    DECIMAL(10,2),\n    total_amount  DECIMAL(10,2),\n    discount      DECIMAL(10,2)\n);\n```\n\n## The 3 Types of Facts\n\n| Type | Example | How Used |\n|------|---------|----------|\n| **Additive** | Revenue, Quantity | SUM across any dimension |\n| **Semi-Additive** | Account Balance | SUM across some dimensions |\n| **Non-Additive** | Ratio, Percentage | Cannot SUM, must recalculate |\n\n## Design Tips\n\n‚úÖ **Keep facts skinny**: Only keys and measures\n‚úÖ **Use surrogate keys**: Integers are faster than strings\n‚úÖ **Pre-calculate**: Store amounts, not just \"calculate later\"\n‚ùå **Don't store text**: That goes in dimensions!\n\n---\n\n## üéØ Your Task\n\nDesign a fact table for an e-commerce order system with proper foreign keys and measures.",
    "starter_code": "-- Design a fact table for e-commerce orders\nCREATE TABLE fact_orders (\n    -- Surrogate key\n    order_fact_id    SERIAL PRIMARY KEY,\n    \n    -- Foreign keys to dimensions\n    order_date_id    INT REFERENCES dim_date(date_id),\n    customer_id      INT REFERENCES dim_customer(customer_id),\n    product_id       INT REFERENCES dim_product(product_id),\n    \n    -- Measures (additive facts)\n    quantity         INT,\n    unit_price       DECIMAL(10,2),\n    total_amount     DECIMAL(10,2),\n    shipping_cost    DECIMAL(10,2),\n    discount_amount  DECIMAL(10,2)\n);",
    "solution_code": "CREATE TABLE fact_orders (\n    order_fact_id    SERIAL PRIMARY KEY,\n    order_date_id    INT REFERENCES dim_date(date_id),\n    customer_id      INT REFERENCES dim_customer(customer_id),\n    product_id       INT REFERENCES dim_product(product_id),\n    quantity         INT,\n    unit_price       DECIMAL(10,2),\n    total_amount     DECIMAL(10,2),\n    shipping_cost    DECIMAL(10,2),\n    discount_amount  DECIMAL(10,2)\n);",
    "expected_output": "Table created successfully"
  },
  "1237": {
    "title": "Constraint Testing",
    "chapter_title": "Database Design",
    "content": "# üîí Testing Constraints: Verify Data Integrity\n\n## Why Test Constraints?\n\nConstraints prevent bad data... but are they working? Test them!\n\n## Types of Constraints\n\n| Constraint | Purpose |\n|------------|---------|\n| PRIMARY KEY | Unique identifier |\n| UNIQUE | No duplicates |\n| NOT NULL | Required values |\n| FOREIGN KEY | Valid references |\n| CHECK | Custom validation |\n\n## Testing Primary Key\n\n```sql\n-- Should return 0 rows if PK works\nSELECT id, COUNT(*) \nFROM table \nGROUP BY id \nHAVING COUNT(*) > 1;\n```\n\n## Testing Foreign Key\n\n```sql\n-- Find orphan records (should be 0 if FK enforced)\nSELECT child.id\nFROM child_table child\nLEFT JOIN parent_table parent ON child.parent_id = parent.id\nWHERE parent.id IS NULL;\n```\n\n## Testing NOT NULL\n\n```sql\n-- Check for nulls in required columns\nSELECT COUNT(*) as null_count\nFROM customers\nWHERE email IS NULL;\n```\n\n## Testing CHECK Constraints\n\n```sql\n-- Verify CHECK (age >= 0)\nSELECT * FROM users WHERE age < 0;  -- Should be empty\n```\n\n## Constraint Validation Report\n\n```sql\nSELECT 'Duplicate PKs' as check, COUNT(*) FROM (\n    SELECT id FROM orders GROUP BY id HAVING COUNT(*) > 1\n) dupes\nUNION ALL\nSELECT 'Orphan FKs', COUNT(*) FROM orders o \nLEFT JOIN customers c ON o.customer_id = c.id WHERE c.id IS NULL\nUNION ALL\nSELECT 'Null Required', COUNT(*) FROM orders WHERE amount IS NULL;\n```\n\n---\n\n## üéØ Your Task\n\nCreate tests to verify database constraints are enforced.",
    "starter_code": "-- Constraint validation tests\nSELECT \n    'Duplicate Order IDs' as constraint_test,\n    CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END as result,\n    COUNT(*) as violations\nFROM (\n    SELECT order_id FROM orders GROUP BY order_id HAVING COUNT(*) > 1\n) dupes\nUNION ALL\nSELECT \n    'Orphan Customer References',\n    CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,\n    COUNT(*)\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.customer_id\nWHERE c.customer_id IS NULL\nUNION ALL\nSELECT \n    'Null Order Amounts',\n    CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END,\n    COUNT(*)\nFROM orders WHERE amount IS NULL;",
    "solution_code": "SELECT 'Orphan FKs' as test, COUNT(*) FROM orders o LEFT JOIN customers c ON o.customer_id = c.id WHERE c.id IS NULL;",
    "expected_output": "constraint_test|result|violations\nDuplicate Order IDs|PASS|0\nOrphan Customer References|PASS|0\nNull Order Amounts|FAIL|5"
  },
  "1238": {
    "title": "Index Creation",
    "chapter_title": "Database Design",
    "content": "# üöÄ Index Creation: Speed Up Queries\n\n## What is an Index?\n\nAn index is like a book's index‚Äîlets you find data without scanning every page (row).\n\n## Create Basic Index\n\n```sql\nCREATE INDEX idx_customer_id ON orders(customer_id);\n```\n\n## Index Types\n\n### B-tree (Default)\nBest for: equality, range queries, ORDER BY\n```sql\nCREATE INDEX idx_date ON orders(order_date);\n```\n\n### Composite Index\nMultiple columns:\n```sql\nCREATE INDEX idx_cust_date ON orders(customer_id, order_date);\n```\n\n### Unique Index\nEnforces uniqueness:\n```sql\nCREATE UNIQUE INDEX idx_email ON users(email);\n```\n\n### Partial Index\nOnly index relevant rows:\n```sql\nCREATE INDEX idx_active ON users(email) WHERE status = 'active';\n```\n\n## When to Create Indexes\n\n| Create Index On | Why |\n|-----------------|-----|\n| WHERE columns | Filter speed |\n| JOIN columns | Join speed |\n| ORDER BY columns | Sort speed |\n| Foreign keys | Join + constraint speed |\n\n## When NOT to Index\n\n- Very small tables\n- Columns with few unique values\n- Tables with heavy INSERT/UPDATE\n\n## Managing Indexes\n\n```sql\n-- List indexes\nSELECT * FROM pg_indexes WHERE tablename = 'orders';\n\n-- Drop index\nDROP INDEX idx_customer_id;\n\n-- Rename\nALTER INDEX old_name RENAME TO new_name;\n```\n\n---\n\n## üéØ Your Task\n\nCreate appropriate indexes to optimize query performance.",
    "starter_code": "-- Create indexes for common query patterns\n\n-- Index for customer lookups\nCREATE INDEX idx_orders_customer ON orders(customer_id);\n\n-- Composite index for date range + customer queries  \nCREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);\n\n-- Index for status filtering\nCREATE INDEX idx_orders_status ON orders(status) WHERE status != 'cancelled';\n\n-- Verify indexes\nSELECT indexname, indexdef \nFROM pg_indexes \nWHERE tablename = 'orders';",
    "solution_code": "CREATE INDEX idx_customer ON orders(customer_id);",
    "expected_output": "CREATE INDEX"
  },
  "1239": {
    "title": "Table Design",
    "chapter_title": "Database Design",
    "content": "# üèóÔ∏è Table Design: Structure Your Data Right\n\n## Key Principles\n\n1. **Normalization**: Eliminate redundancy\n2. **Primary Keys**: Unique identifier for each row\n3. **Foreign Keys**: Enforce relationships\n4. **Data Types**: Choose appropriate types\n\n## Basic Table Creation\n\n```sql\nCREATE TABLE customers (\n    customer_id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n## Relationships\n\n### One-to-Many\n```sql\nCREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    customer_id INT REFERENCES customers(customer_id),\n    order_date DATE NOT NULL\n);\n```\n\n### Many-to-Many (Junction Table)\n```sql\nCREATE TABLE product_categories (\n    product_id INT REFERENCES products(id),\n    category_id INT REFERENCES categories(id),\n    PRIMARY KEY (product_id, category_id)\n);\n```\n\n## Common Column Patterns\n\n| Column | Type | Constraints |\n|--------|------|-------------|\n| id | SERIAL | PRIMARY KEY |\n| email | VARCHAR(255) | UNIQUE NOT NULL |\n| name | VARCHAR(100) | NOT NULL |\n| amount | DECIMAL(10,2) | |\n| created_at | TIMESTAMP | DEFAULT NOW() |\n| is_active | BOOLEAN | DEFAULT true |\n| status | VARCHAR(20) | CHECK (...) |\n\n## Naming Conventions\n\n- Tables: plural (orders, customers)\n- Columns: snake_case (order_date, customer_id)\n- Primary key: table_id (customer_id)\n- Foreign key: referenced_table_id\n\n---\n\n## üéØ Your Task\n\nDesign a well-structured table with appropriate constraints.",
    "starter_code": "-- Well-designed orders table\nCREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    customer_id INT NOT NULL REFERENCES customers(customer_id),\n    order_date DATE NOT NULL DEFAULT CURRENT_DATE,\n    status VARCHAR(20) NOT NULL DEFAULT 'pending'\n        CHECK (status IN ('pending', 'processing', 'shipped', 'delivered', 'cancelled')),\n    subtotal DECIMAL(10,2) NOT NULL CHECK (subtotal >= 0),\n    tax DECIMAL(10,2) NOT NULL DEFAULT 0 CHECK (tax >= 0),\n    total DECIMAL(10,2) GENERATED ALWAYS AS (subtotal + tax) STORED,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add indexes for common queries\nCREATE INDEX idx_orders_customer ON orders(customer_id);\nCREATE INDEX idx_orders_date ON orders(order_date);\nCREATE INDEX idx_orders_status ON orders(status);",
    "solution_code": "CREATE TABLE orders (id SERIAL PRIMARY KEY, customer_id INT REFERENCES customers(id), amount DECIMAL(10,2));",
    "expected_output": "CREATE TABLE"
  },
  "1240": {
    "title": "SCD Implementation",
    "chapter_title": "Database Design",
    "content": "# üìö Slowly Changing Dimensions: Track Historical Data\n\n## What is SCD?\n\n**Slowly Changing Dimensions** track how dimension data changes over time. Customer addresses, product prices, employee departments all change!\n\n## SCD Types\n\n### Type 1: Overwrite\nJust update the record. History is lost.\n```sql\nUPDATE customers SET address = 'New Address' WHERE id = 1;\n```\n\n### Type 2: Add New Row\nKeep history with date ranges:\n```sql\n-- Current record\nINSERT INTO customers_scd2 \nVALUES (1, 'Alice', 'Old Address', '2024-01-01', '2024-06-30', FALSE);\n-- New record\nINSERT INTO customers_scd2 \nVALUES (1, 'Alice', 'New Address', '2024-07-01', '9999-12-31', TRUE);\n```\n\n## Type 2 Structure\n\n```sql\nCREATE TABLE customers_scd2 (\n    customer_id INT,\n    name VARCHAR(100),\n    address VARCHAR(200),\n    effective_start DATE,\n    effective_end DATE,\n    is_current BOOLEAN\n);\n```\n\n## Querying SCD2\n\n```sql\n-- Current values only\nSELECT * FROM customers_scd2 WHERE is_current = TRUE;\n\n-- Value at a specific date\nSELECT * FROM customers_scd2 \nWHERE '2024-03-15' BETWEEN effective_start AND effective_end;\n```\n\n## SCD2 Update Pattern\n\n```sql\n-- Step 1: Close current record\nUPDATE customers_scd2 \nSET effective_end = CURRENT_DATE - 1, is_current = FALSE\nWHERE customer_id = 1 AND is_current = TRUE;\n\n-- Step 2: Insert new current record\nINSERT INTO customers_scd2 \nVALUES (1, 'Alice', 'New Address', CURRENT_DATE, '9999-12-31', TRUE);\n```\n\n---\n\n## üéØ Your Task\n\nImplement SCD Type 2 to track historical changes.",
    "starter_code": "-- Query historical data from SCD2 table\n-- Get current values\nSELECT customer_id, name, address\nFROM customers_scd2\nWHERE is_current = TRUE;\n\n-- Get values as of a specific date\nSELECT customer_id, name, address\nFROM customers_scd2\nWHERE '2024-03-15' BETWEEN effective_start AND effective_end;\n\n-- See full history for one customer\nSELECT *\nFROM customers_scd2\nWHERE customer_id = 101\nORDER BY effective_start;",
    "solution_code": "SELECT * FROM customers_scd2 WHERE customer_id = 1 ORDER BY effective_start;",
    "expected_output": "customer_id|name|address|effective_start|effective_end|is_current\n101|Alice|123 Main St|2023-01-01|2024-06-30|false\n101|Alice|456 Oak Ave|2024-07-01|9999-12-31|true"
  },
  "1241": {
    "title": "Audit Columns",
    "chapter_title": "Data Cleaning",
    "content": "# üìã Audit Columns: Track Data Lineage\n\n## What Are Audit Columns?\n\nStandard columns that track when and how data was created/modified:\n- `created_at`: When record was inserted\n- `updated_at`: When last modified\n- `created_by`: Who created it\n- `updated_by`: Who modified it\n\n## Adding Audit Columns\n\n```sql\nALTER TABLE orders \nADD COLUMN created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\nADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\nADD COLUMN created_by VARCHAR(100),\nADD COLUMN updated_by VARCHAR(100);\n```\n\n## Automatic Updated_at with Trigger\n\n```sql\nCREATE OR REPLACE FUNCTION update_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = CURRENT_TIMESTAMP;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_at\nBEFORE UPDATE ON orders\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp();\n```\n\n## Querying Audit Data\n\n```sql\n-- Recently modified records\nSELECT * FROM orders\nWHERE updated_at > CURRENT_TIMESTAMP - INTERVAL '1 hour';\n\n-- Find who made changes\nSELECT created_by, COUNT(*) as records_created\nFROM orders\nWHERE created_at >= '2024-01-01'\nGROUP BY created_by;\n```\n\n## ETL Audit Columns\n\n| Column | Purpose |\n|--------|---------|\n| etl_load_id | Batch identifier |\n| etl_load_time | When loaded |\n| source_system | Where data came from |\n| is_deleted | Soft delete flag |\n\n---\n\n## üéØ Your Task\n\nQuery audit columns to track data changes over time.",
    "starter_code": "-- Analyze data changes using audit columns\nSELECT \n    DATE_TRUNC('day', updated_at) as update_date,\n    COUNT(*) as records_modified,\n    COUNT(DISTINCT updated_by) as users_updating\nFROM orders\nWHERE updated_at >= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY 1\nORDER BY 1 DESC;",
    "solution_code": "SELECT * FROM orders WHERE updated_at > CURRENT_DATE - INTERVAL '1 day';",
    "expected_output": "update_date|records_modified|users_updating\n2024-03-15|125|3\n2024-03-14|98|2"
  },
  "1242": {
    "title": "Query Logging",
    "chapter_title": "Performance",
    "content": "# üìù Query Logging: Track Slow Queries\n\n## Why Log Queries?\n\nFind performance problems before users complain:\n- Identify slow queries\n- Find heavy resource usage\n- Track query patterns\n\n## PostgreSQL: Slow Query Log\n\n```sql\n-- In postgresql.conf\nlog_min_duration_statement = 1000  -- Log queries > 1 second\nlog_statement = 'all'  -- Log all queries (dev only!)\n```\n\n## Query the Log\n\n```sql\n-- PostgreSQL: Recent slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements\nORDER BY total_time DESC\nLIMIT 10;\n```\n\n## Enable pg_stat_statements\n\n```sql\n-- Enable the extension\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Reset statistics\nSELECT pg_stat_statements_reset();\n```\n\n## Key Metrics\n\n| Metric | What It Tells You |\n|--------|-------------------|\n| total_time | Cumulative impact |\n| mean_time | Per-call cost |\n| calls | How often it runs |\n| rows | Data volume |\n\n## Common Findings\n\n```sql\n-- Queries consuming most time\nSELECT query, total_time / 1000 as seconds\nFROM pg_stat_statements\nORDER BY total_time DESC LIMIT 10;\n\n-- Most frequently called\nSELECT query, calls\nFROM pg_stat_statements\nORDER BY calls DESC LIMIT 10;\n```\n\n---\n\n## üéØ Your Task\n\nQuery performance statistics to identify slow queries.",
    "starter_code": "-- Find slow and frequent queries\nSELECT \n    LEFT(query, 100) as query_preview,\n    calls,\n    ROUND(total_time::numeric, 2) as total_ms,\n    ROUND(mean_time::numeric, 2) as avg_ms,\n    rows\nFROM pg_stat_statements\nWHERE query NOT LIKE '%pg_stat%'\nORDER BY total_time DESC\nLIMIT 10;",
    "solution_code": "SELECT query, total_time, calls FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;",
    "expected_output": "query_preview|calls|total_ms|avg_ms|rows\nSELECT * FROM orders WHERE...|15000|45000|3.00|750000"
  },
  "1243": {
    "title": "Cost Analysis",
    "chapter_title": "Performance",
    "content": "# üí∞ Query Cost Analysis: Read EXPLAIN Output\n\n## What is Query Cost?\n\nEvery query has a \"cost\" representing estimated work:\n- **Startup cost**: Time before first row returns\n- **Total cost**: Time for all rows\n\n## Reading EXPLAIN\n\n```sql\nEXPLAIN SELECT * FROM orders WHERE status = 'shipped';\n\n-- Output:\n-- Seq Scan on orders (cost=0.00..1500.00 rows=50000 width=100)\n--                           ‚Üë    ‚Üë       ‚Üë\n--               startup   total   estimated rows\n```\n\n## Cost Units\n\nCost is relative, not absolute time. Compare costs between queries:\n\n| Cost | Meaning |\n|------|---------|\n| 0.00 | No startup cost |\n| 1.00 | ~One sequential page read |\n| 100+ | Moderate work |\n| 10000+ | Heavy operation |\n\n## EXPLAIN ANALYZE\n\nAdd actual execution times:\n```sql\nEXPLAIN ANALYZE SELECT * FROM orders WHERE status = 'shipped';\n\n-- Seq Scan on orders (cost=0.00..1500.00 rows=50000) \n--   (actual time=0.015..45.234 rows=48532 loops=1)\n```\n\n## Compare Before/After\n\n```sql\n-- Without index (high cost)\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n-- Seq Scan: cost=0.00..15000.00\n\n-- With index (low cost)\nCREATE INDEX idx_customer ON orders(customer_id);\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n-- Index Scan: cost=0.00..8.50\n```\n\n---\n\n## üéØ Your Task\n\nUse EXPLAIN to analyze and compare query costs.",
    "starter_code": "-- Analyze query costs with EXPLAIN\nEXPLAIN ANALYZE\nSELECT \n    customer_id,\n    SUM(amount) as total_spent\nFROM orders\nWHERE order_date >= '2024-01-01'\nGROUP BY customer_id\nORDER BY total_spent DESC\nLIMIT 10;",
    "solution_code": "EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 123;",
    "expected_output": "QUERY PLAN\nSeq Scan on orders (cost=0.00..1234.00 rows=100)"
  },
  "1244": {
    "title": "Join Order",
    "chapter_title": "Performance",
    "content": "# üîó Join Order: Which Table First?\n\n## Why Join Order Matters\n\nThe order of joins can dramatically affect performance. Start with smaller result sets!\n\n## The Query Planner\n\nModern databases usually optimize join order automatically, but understanding helps:\n\n```sql\n-- These produce the same result, but performance may differ\nSELECT * FROM large_table l\nJOIN small_table s ON l.id = s.id;\n\nSELECT * FROM small_table s\nJOIN large_table l ON s.id = l.id;\n```\n\n## General Principles\n\n1. **Start small**: Join smallest table first\n2. **Filter early**: Apply WHERE before JOINs\n3. **Index foreign keys**: Speed up the join condition\n\n## Viewing Join Order in EXPLAIN\n\n```sql\nEXPLAIN SELECT ...\n\n-- Hash Join (cost=100.00..500.00)\n--   -> Seq Scan on orders    -- Outer table (scanned)\n--   -> Hash                   -- Inner table (hashed)\n--        -> Seq Scan on customers\n```\n\n## Join Algorithm Types\n\n| Type | Best For |\n|------|----------|\n| Nested Loop | Small outer table |\n| Hash Join | Equi-joins, medium tables |\n| Merge Join | Pre-sorted data |\n\n## Forcing Join Order (When Needed)\n\n```sql\nSET join_collapse_limit = 1;  -- PostgreSQL: prevent reordering\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze join order in a query execution plan.",
    "starter_code": "-- View join order in execution plan\nEXPLAIN\nSELECT \n    c.name,\n    o.order_date,\n    p.product_name\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nWHERE c.status = 'active';",
    "solution_code": "EXPLAIN SELECT * FROM small_table s JOIN large_table l ON s.id = l.id;",
    "expected_output": "QUERY PLAN\nHash Join (cost=25.00..500.00)"
  },
  "1245": {
    "title": "Scan Types",
    "chapter_title": "Performance",
    "content": "# üîç Table Scan Types: How SQL Finds Your Data\n\n## Sequential Scan (Seq Scan)\n\nReads **every row** in the table. Like reading a book cover to cover.\n\n```sql\nEXPLAIN SELECT * FROM orders WHERE status = 'shipped';\n-- Seq Scan on orders (cost=0.00..1500.00)\n```\n\n**Good for**: Full table, no index, <10-20% of rows\n\n## Index Scan\n\nUses an index like a book's index‚Äîjump directly to relevant pages.\n\n```sql\nEXPLAIN SELECT * FROM orders WHERE order_id = 12345;\n-- Index Scan using orders_pkey (cost=0.00..8.50)\n```\n\n**Good for**: Selective queries (<5% of rows)\n\n## Index Only Scan\n\nEven better‚Äîdata comes entirely from index, no table access!\n\n```sql\nEXPLAIN SELECT customer_id FROM orders WHERE customer_id = 100;\n-- Index Only Scan using idx_customer (cost=0.00..4.50)\n```\n\n## Bitmap Scan\n\nFor medium selectivity‚Äîbuilds a bitmap of matching rows.\n\n```sql\n-- Bitmap Heap Scan on orders\n--   -> Bitmap Index Scan on idx_status\n```\n\n**Good for**: 5-20% of rows\n\n## Scan Comparison\n\n| Scan Type | Speed | Best When |\n|-----------|-------|-----------|\n| Index Only | Fastest | All columns in index |\n| Index Scan | Fast | Selective queries |\n| Bitmap | Medium | Moderate selectivity |\n| Seq Scan | Slow* | Full table scans |\n\n*Slow only for large tables with selective predicates\n\n---\n\n## üéØ Your Task\n\nIdentify scan types in execution plans and optimize accordingly.",
    "starter_code": "-- Compare scan types with and without index\n-- First: Without index (Seq Scan)\nEXPLAIN SELECT * FROM orders WHERE amount > 1000;\n\n-- Create index\nCREATE INDEX idx_amount ON orders(amount);\n\n-- After: With index (Index Scan or Bitmap Scan)\nEXPLAIN SELECT * FROM orders WHERE amount > 1000;",
    "solution_code": "EXPLAIN SELECT * FROM orders WHERE order_id = 123;",
    "expected_output": "QUERY PLAN\nIndex Scan using orders_pkey on orders (cost=0.00..8.50 rows=1)"
  },
  "1246": {
    "title": "Query Rewrite",
    "chapter_title": "Performance",
    "content": "# ‚úèÔ∏è Query Rewriting: Same Result, Better Performance\n\n## Why Rewrite Queries?\n\nDifferent SQL can produce identical results with vastly different performance.\n\n## Common Rewrites\n\n### 1. Subquery ‚Üí JOIN\n```sql\n-- Slow: Correlated subquery\nSELECT * FROM orders o\nWHERE customer_id IN (SELECT id FROM customers WHERE status = 'active');\n\n-- Fast: JOIN\nSELECT o.* FROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE c.status = 'active';\n```\n\n### 2. OR ‚Üí UNION\n```sql\n-- Slow: OR prevents index use\nSELECT * FROM orders WHERE status = 'shipped' OR amount > 1000;\n\n-- Fast: UNION (each part uses index)\nSELECT * FROM orders WHERE status = 'shipped'\nUNION\nSELECT * FROM orders WHERE amount > 1000;\n```\n\n### 3. NOT IN ‚Üí NOT EXISTS\n```sql\n-- Risky: NOT IN with NULLs\nSELECT * FROM customers\nWHERE id NOT IN (SELECT customer_id FROM orders);\n\n-- Safe: NOT EXISTS\nSELECT * FROM customers c\nWHERE NOT EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id);\n```\n\n### 4. DISTINCT ‚Üí GROUP BY\n```sql\n-- Sometimes better with GROUP BY\nSELECT DISTINCT customer_id FROM orders;\n-- vs\nSELECT customer_id FROM orders GROUP BY customer_id;\n```\n\n---\n\n## ÔøΩÔøΩ Your Task\n\nRewrite a slow query to improve performance.",
    "starter_code": "-- Original slow query (correlated subquery)\n-- SELECT * FROM products p\n-- WHERE price > (SELECT AVG(price) FROM products WHERE category = p.category);\n\n-- Rewritten: Use CTE/JOIN for better performance\nWITH category_avg AS (\n    SELECT category, AVG(price) as avg_price\n    FROM products\n    GROUP BY category\n)\nSELECT p.*\nFROM products p\nJOIN category_avg ca ON p.category = ca.category\nWHERE p.price > ca.avg_price;",
    "solution_code": "-- Rewrite subquery as JOIN\nSELECT o.* FROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE c.status = 'active';",
    "expected_output": "product_id|name|price|category\n1|Premium Widget|150|Electronics\n5|Deluxe Gadget|250|Electronics"
  },
  "1247": {
    "title": "Sargable Queries",
    "chapter_title": "Performance",
    "content": "# ‚ö° Sargable Queries: Enable Index Usage\n\n## What is Sargable?\n\n**SARG** = Search ARGument. A query is \"sargable\" if it can use indexes effectively.\n\n## Non-Sargable (Bad)\n\nFunctions on the column prevent index use:\n\n```sql\n-- ‚ùå Function on column - no index\nWHERE YEAR(order_date) = 2024\nWHERE LOWER(email) = 'test@example.com'\nWHERE amount + 10 > 100\n```\n\n## Sargable (Good)\n\nKeep the column \"naked\":\n\n```sql\n-- ‚úÖ Column untouched - uses index\nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01'\nWHERE email = 'test@example.com'  -- with case-sensitive data\nWHERE amount > 90\n```\n\n## Common Fixes\n\n| Non-Sargable | Sargable |\n|--------------|----------|\n| `YEAR(date) = 2024` | `date >= '2024-01-01' AND date < '2025-01-01'` |\n| `LOWER(col) = 'x'` | Use case-insensitive collation or functional index |\n| `col * 2 > 10` | `col > 5` |\n| `col IS NOT NULL` | Often sargable |\n\n## Functional Indexes\n\nWhen you MUST use a function, create a functional index:\n\n```sql\nCREATE INDEX idx_lower_email ON users(LOWER(email));\n\n-- Now this is sargable!\nSELECT * FROM users WHERE LOWER(email) = 'test@example.com';\n```\n\n---\n\n## üéØ Your Task\n\nConvert non-sargable queries to sargable equivalents.",
    "starter_code": "-- Non-sargable queries converted to sargable\n\n-- ‚ùå Bad: YEAR() function\n-- SELECT * FROM orders WHERE YEAR(order_date) = 2024;\n\n-- ‚úÖ Good: Range comparison\nSELECT * FROM orders \nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n\n-- ‚ùå Bad: Calculation on column\n-- SELECT * FROM products WHERE price * 1.1 > 100;\n\n-- ‚úÖ Good: Calculation on value\nSELECT * FROM products WHERE price > 90.91;",
    "solution_code": "-- Sargable version\nSELECT * FROM orders WHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';",
    "expected_output": "order_id|order_date|amount\n1|2024-01-15|150\n2|2024-02-20|200"
  },
  "1248": {
    "title": "Upsert Pattern",
    "chapter_title": "DML",
    "content": "# üîÑ Upsert: INSERT or UPDATE\n\n## What is Upsert?\n\nInsert a row if it doesn't exist, update it if it does. Also called \"MERGE\" or \"ON CONFLICT\".\n\n## PostgreSQL: ON CONFLICT\n\n```sql\nINSERT INTO products (id, name, price)\nVALUES (1, 'Widget', 29.99)\nON CONFLICT (id) DO UPDATE SET\n    name = EXCLUDED.name,\n    price = EXCLUDED.price;\n```\n\n## MySQL: ON DUPLICATE KEY\n\n```sql\nINSERT INTO products (id, name, price)\nVALUES (1, 'Widget', 29.99)\nON DUPLICATE KEY UPDATE\n    name = VALUES(name),\n    price = VALUES(price);\n```\n\n## Do Nothing on Conflict\n\n```sql\nINSERT INTO products (id, name, price)\nVALUES (1, 'Widget', 29.99)\nON CONFLICT (id) DO NOTHING;\n```\n\n## Bulk Upsert\n\n```sql\nINSERT INTO products (id, name, price)\nVALUES \n    (1, 'Widget', 29.99),\n    (2, 'Gadget', 49.99),\n    (3, 'Tool', 19.99)\nON CONFLICT (id) DO UPDATE SET\n    name = EXCLUDED.name,\n    price = EXCLUDED.price;\n```\n\n## Use Cases\n\n| Scenario | Pattern |\n|----------|---------|\n| Sync from external source | Upsert all |\n| Increment counter | ON CONFLICT add to existing |\n| Update timestamps | ON CONFLICT set updated_at |\n\n---\n\n## üéØ Your Task\n\nImplement an upsert to sync product data.",
    "starter_code": "-- Upsert product data\nINSERT INTO products (product_id, name, price, updated_at)\nVALUES \n    (101, 'Widget Pro', 49.99, NOW()),\n    (102, 'Gadget Plus', 79.99, NOW()),\n    (103, 'Tool Kit', 29.99, NOW())\nON CONFLICT (product_id) DO UPDATE SET\n    name = EXCLUDED.name,\n    price = EXCLUDED.price,\n    updated_at = NOW();",
    "solution_code": "INSERT INTO products (id, name) VALUES (1, 'Widget') ON CONFLICT (id) DO UPDATE SET name = EXCLUDED.name;",
    "expected_output": "INSERT 0 3"
  },
  "1249": {
    "title": "Default Values",
    "chapter_title": "DML",
    "content": "# üéØ Default Values: Automatic Column Population\n\n## Why Defaults?\n\nDefault values simplify inserts and ensure consistency:\n- Timestamps (created_at)\n- Status fields (status = 'pending')\n- Counters (view_count = 0)\n\n## Setting Defaults\n\n```sql\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    status VARCHAR(20) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    is_active BOOLEAN DEFAULT true,\n    amount DECIMAL DEFAULT 0.00\n);\n\n-- Insert without specifying defaults\nINSERT INTO orders (id) VALUES (1);\n-- status='pending', created_at=now(), is_active=true, amount=0\n```\n\n## Using DEFAULT in INSERT\n\n```sql\nINSERT INTO orders (id, status, created_at)\nVALUES (1, DEFAULT, DEFAULT);\n```\n\n## Common Default Patterns\n\n| Column | Default | Purpose |\n|--------|---------|---------|\n| created_at | CURRENT_TIMESTAMP | Track creation |\n| updated_at | CURRENT_TIMESTAMP | Track modification |\n| status | 'pending' | Initial state |\n| is_active | true | Soft delete support |\n| sequence_num | nextval() | Auto-increment |\n\n## Modifying Defaults\n\n```sql\n-- Add default to existing column\nALTER TABLE orders ALTER COLUMN status SET DEFAULT 'new';\n\n-- Remove default\nALTER TABLE orders ALTER COLUMN status DROP DEFAULT;\n```\n\n---\n\n## üéØ Your Task\n\nCreate a table with appropriate default values for common columns.",
    "starter_code": "-- Create table with smart defaults\nCREATE TABLE tasks (\n    id SERIAL PRIMARY KEY,\n    title VARCHAR(255) NOT NULL,\n    description TEXT,\n    status VARCHAR(50) DEFAULT 'todo',\n    priority INTEGER DEFAULT 3,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    created_by VARCHAR(100),\n    is_archived BOOLEAN DEFAULT false\n);\n\n-- Insert minimal data (defaults fill in the rest)\nINSERT INTO tasks (title, created_by)\nVALUES ('Review code', 'alice');\n\n-- Check the result\nSELECT * FROM tasks;",
    "solution_code": "CREATE TABLE tasks (id SERIAL, status VARCHAR DEFAULT 'pending', created_at TIMESTAMP DEFAULT NOW());",
    "expected_output": "id|title|status|priority|created_at|is_archived\n1|Review code|todo|3|2024-03-15|false"
  },
  "1250": {
    "title": "Conditional Update",
    "chapter_title": "DML",
    "content": "# üîÑ Conditional Updates: Update Based on Conditions\n\n## Basic Conditional Update\n\n```sql\nUPDATE orders\nSET status = 'cancelled'\nWHERE status = 'pending' AND order_date < '2024-01-01';\n```\n\n## CASE in UPDATE\n\nUpdate to different values based on conditions:\n\n```sql\nUPDATE products\nSET price_tier = CASE\n    WHEN price < 10 THEN 'budget'\n    WHEN price < 50 THEN 'standard'\n    WHEN price < 100 THEN 'premium'\n    ELSE 'luxury'\nEND;\n```\n\n## Update From Another Table\n\n```sql\nUPDATE orders o\nSET customer_name = c.name\nFROM customers c\nWHERE o.customer_id = c.id;\n```\n\n## Increment/Decrement Conditionally\n\n```sql\nUPDATE inventory\nSET stock = CASE\n    WHEN stock > 10 THEN stock - 5\n    ELSE 0\nEND\nWHERE product_id IN (SELECT product_id FROM hot_items);\n```\n\n## Update with Subquery\n\n```sql\nUPDATE orders\nSET discount = (\n    SELECT MAX(discount_rate)\n    FROM promotions\n    WHERE promotions.category = orders.category\n)\nWHERE order_date >= CURRENT_DATE;\n```\n\n## Safety: Always Test First\n\n```sql\n-- Test with SELECT first\nSELECT * FROM orders WHERE status = 'pending' AND total < 10;\n\n-- Then UPDATE if results look right\nUPDATE orders SET status = 'cancelled' WHERE status = 'pending' AND total < 10;\n```\n\n---\n\n## üéØ Your Task\n\nWrite conditional updates to modify data based on business rules.",
    "starter_code": "-- Conditional update: Set customer tier based on total purchases\nUPDATE customers c\nSET tier = CASE\n    WHEN (SELECT SUM(amount) FROM orders WHERE customer_id = c.id) >= 10000 THEN 'platinum'\n    WHEN (SELECT SUM(amount) FROM orders WHERE customer_id = c.id) >= 5000 THEN 'gold'\n    WHEN (SELECT SUM(amount) FROM orders WHERE customer_id = c.id) >= 1000 THEN 'silver'\n    ELSE 'bronze'\nEND;\n\n-- Verify\nSELECT id, name, tier FROM customers ORDER BY tier;",
    "solution_code": "UPDATE products SET status = CASE WHEN stock = 0 THEN 'out_of_stock' ELSE 'in_stock' END;",
    "expected_output": "UPDATE 150"
  },
  "1251": {
    "title": "Batch Update",
    "chapter_title": "DML",
    "content": "# üì¶ Batch Updates: Modify Many Rows Efficiently\n\n## Why Batch Updates?\n\nUpdating millions of rows at once can lock tables and overwhelm the database. Batch in chunks!\n\n## The Pattern\n\n```sql\n-- Update in batches of 1000\nUPDATE orders\nSET processed = true\nWHERE id IN (\n    SELECT id FROM orders\n    WHERE processed = false\n    LIMIT 1000\n);\n-- Repeat until no rows updated\n```\n\n## With CTEs\n\n```sql\nWITH batch AS (\n    SELECT id FROM large_table\n    WHERE needs_update = true\n    LIMIT 5000\n)\nUPDATE large_table\nSET updated = true\nWHERE id IN (SELECT id FROM batch);\n```\n\n## Using RETURNING\n\nTrack what was updated:\n\n```sql\nUPDATE orders\nSET status = 'processed'\nWHERE status = 'pending'\nLIMIT 1000\nRETURNING id, customer_id;\n```\n\n## Batch Delete Pattern\n\n```sql\n-- Delete in batches to avoid locking\nDELETE FROM logs\nWHERE id IN (\n    SELECT id FROM logs\n    WHERE created_at < '2023-01-01'\n    LIMIT 10000\n);\n```\n\n## Progress Tracking\n\n```sql\n-- Run this repeatedly until 0 rows affected\nDO $$\nDECLARE\n    rows_updated INTEGER := 1;\nBEGIN\n    WHILE rows_updated > 0 LOOP\n        UPDATE large_table SET processed = true\n        WHERE processed = false\n        LIMIT 5000;\n        GET DIAGNOSTICS rows_updated = ROW_COUNT;\n        RAISE NOTICE 'Updated % rows', rows_updated;\n        COMMIT;\n    END LOOP;\nEND $$;\n```\n\n---\n\n## üéØ Your Task\n\nImplement a batch update pattern for large table modifications.",
    "starter_code": "-- Batch update for large table\nWITH batch AS (\n    SELECT order_id \n    FROM orders \n    WHERE processed = false \n    AND created_at < '2024-01-01'\n    LIMIT 1000\n)\nUPDATE orders\nSET \n    processed = true,\n    processed_at = NOW()\nWHERE order_id IN (SELECT order_id FROM batch)\nRETURNING order_id;",
    "solution_code": "UPDATE orders SET processed = true WHERE id IN (SELECT id FROM orders WHERE processed = false LIMIT 1000);",
    "expected_output": "order_id\n1001\n1002\n1003\n..."
  },
  "1252": {
    "title": "Regular Views Intro",
    "chapter_title": "Analytics Engineering",
    "content": "# üëÅÔ∏è Regular Views: Saved Queries for Simplicity\n\n## What is a View?\n\nA **view** is a virtual table defined by a SQL query. It doesn't store data itself; it runs the query every time you access it.\n\n## Why Use Views?\n\n1.  **Simplification**: Hide complex joins/logic behind a simple name.\n2.  **Security**: Restrict access to specific columns (e.g., hide salary).\n3.  **Consistency**: Ensure everyone uses the same logic for \"Active Users\".\n\n## Creating a View\n\n```sql\nCREATE VIEW active_users AS\nSELECT id, name, email\nFROM users\nWHERE last_login >= CURRENT_DATE - INTERVAL '30 days';\n```\n\n## Usage\n\nTreat it like a regular table:\n\n```sql\nSELECT * FROM active_users WHERE name LIKE 'A%';\n```\n\n## Updating Views\n\n```sql\nCREATE OR REPLACE VIEW active_users AS ...\nDROP VIEW active_users;\n```\n\n---\n\n## üéØ Your Task\n\nCreate a view to simplify access to high-value orders.",
    "starter_code": "-- Create a view for high value orders (> $500)\nCREATE VIEW high_value_orders AS\nSELECT \n    o.order_id,\n    o.order_date,\n    c.name as customer_name,\n    o.amount\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.amount > 500;\n\n-- Query the view\nSELECT * FROM high_value_orders ORDER BY amount DESC;",
    "solution_code": "CREATE VIEW high_value_orders AS SELECT * FROM orders WHERE amount > 500;",
    "expected_output": "CREATE VIEW"
  },
  "1253": {
    "title": "When to Use Each View Type",
    "chapter_title": "Analytics Engineering",
    "content": "# ‚öñÔ∏è View Types: Regular vs Materialized\n\n## Regular View (Virtual)\n\n*   **Pros**: Always fresh data, zero storage cost.\n*   **Cons**: query runs every time (slow for complex logic).\n*   **Use when**: Real-time data needed, query is fast.\n\n## Materialized View (Physical)\n\n*   **Pros**: Query results are stored (fast reads).\n*   **Cons**: Data can be stale, requires storage, needs refreshing.\n*   **Use when**: Complex aggregations, slow queries, real-time not critical.\n\n## Decision Matrix\n\n| Scenario | Regular View | Materialized View |\n| :--- | :--- | :--- |\n| Simple filter | ‚úÖ | ‚ùå |\n| heavy aggregation | ‚ùå | ‚úÖ |\n| Real-time dashboard | ‚úÖ | ‚ùå (unless refresh is fast) |\n| Daily Report | ‚ùì | ‚úÖ |\n\n## Syntax Comparison\n\n```sql\n-- Regular\nCREATE VIEW my_view AS SELECT ...\n\n-- Materialized\nCREATE MATERIALIZED VIEW my_mat_view AS SELECT ...\n```\n\n---\n\n## üéØ Your Task\n\nChoose the right view type for a daily sales summary and create it.",
    "starter_code": "-- Create a materialized view for daily sales summary\n-- (Since it involves heavy aggregation and doesn't change second-by-second)\nCREATE MATERIALIZED VIEW daily_sales_summary AS\nSELECT \n    DATE_TRUNC('day', order_date) as sales_date,\n    COUNT(*) as total_orders,\n    SUM(amount) as total_revenue,\n    AVG(amount) as avg_order_value\nFROM orders\nGROUP BY 1;\n\n-- Check data\nSELECT * FROM daily_sales_summary ORDER BY sales_date DESC;",
    "solution_code": "CREATE MATERIALIZED VIEW daily_sales_summary AS SELECT DATE_TRUNC('day', order_date), SUM(amount) FROM orders GROUP BY 1;",
    "expected_output": "SELECT 1"
  },
  "1254": {
    "title": "Refreshing Materialized Views",
    "chapter_title": "Analytics Engineering",
    "content": "# üîÑ Refreshing Materialized Views\n\n## The Freshness Problem\n\nMaterialized views are static snapshots. Use `REFRESH` to update them.\n\n## Basic Refresh\n\n```sql\nREFRESH MATERIALIZED VIEW my_mat_view;\n```\n\n*   **Impact**: Locks the view! No one can read while it updates.\n\n## Concurrent Refresh\n\n```sql\nREFRESH MATERIALIZED VIEW CONCURRENTLY my_mat_view;\n```\n\n*   **Impact**: Allows reads during update. Slower, requires a UNIQUE index.\n\n## Automation\n\nIn production, use a scheduler (cron, dbt, Airflow) to run this:\n\n```sql\n-- Every hour\nREFRESH MATERIALIZED VIEW CONCURRENTLY hourly_stats;\n```\n\n---\n\n## üéØ Your Task\n\nRefresh a materialized view to see new data.",
    "starter_code": "-- Insert new data\nINSERT INTO orders (order_date, amount) VALUES (CURRENT_DATE, 999.00);\n\n-- Standard view (sees it immediately)\nSELECT COUNT(*) FROM regular_view_orders;\n\n-- Materialized view (stale)\nSELECT COUNT(*) FROM mat_view_orders;\n\n-- Refresh it\nREFRESH MATERIALIZED VIEW mat_view_orders;\n\n-- Now checks again\nSELECT COUNT(*) FROM mat_view_orders;",
    "solution_code": "REFRESH MATERIALIZED VIEW mat_view_orders;",
    "expected_output": "REFRESH MATERIALIZED VIEW"
  },
  "1255": {
    "title": "View Performance Challenge",
    "chapter_title": "Analytics Engineering",
    "content": "# üèéÔ∏è View Performance Challenge\n\n## The Scenario\n\nYou have a dashboard loading directly from a 10-join query. It takes 15 seconds to load. Users are angry.\n\n## Your Toolkit\n\n1.  **Analyze**: Use `EXPLAIN ANALYZE` to find bottlenecks.\n2.  **Materialize**: Convert to a Materialized View if appropriate.\n3.  **Index**: Add indexes to underlying tables.\n\n## The Query\n\n```sql\nSELECT \n    c.name, \n    SUM(o.amount) as ltv\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\n-- ... 8 more joins ...\nGROUP BY c.name;\n```\n\n---\n\n## üéØ Your Task\n\nOptimize this slow reporting query using a Materialized View.",
    "starter_code": "-- 1. The Slow Query\n-- SELECT r.region, SUM(s.amount) FROM ... (takes 5s)\n\n-- 2. Create optimized structure\nCREATE MATERIALIZED VIEW region_sales_cache AS\nSELECT \n    r.region_name,\n    p.category,\n    SUM(o.amount) as total_sales,\n    COUNT(*) as transaction_count\nFROM regions r\nJOIN stores s ON r.id = s.region_id\nJOIN orders o ON s.id = o.store_id\nJOIN products p ON o.product_id = p.id\nGROUP BY 1, 2;\n\n-- 3. Add index for speed\nCREATE UNIQUE INDEX idx_region_cat ON region_sales_cache(region_name, category);\n\n-- 4. Fast Query\nSELECT * FROM region_sales_cache WHERE region_name = 'East';",
    "solution_code": "CREATE MATERIALIZED VIEW cache AS SELECT * FROM heavy_query;",
    "expected_output": "SELECT 1"
  },
  "1256": {
    "title": "Model Dependencies",
    "chapter_title": "Analytics Engineering",
    "content": "# üï∏Ô∏è Model Dependencies: Building a DAG\n\n## What is a DAG?\n\n**Directed Acyclic Graph**. Only means: data flows in one direction, no loops.\n\n## Layered Architecture (dbt style)\n\n1.  **Raw**: `source_tables`\n2.  **Staging**: `stg_orders` (Clean up column names, types)\n3.  **Intermediate**: `int_customer_orders` (Joins, aggregations)\n4.  **Marts**: `fct_orders`, `dim_customers` (Business ready)\n\n## SQL dependency\n\n```sql\n-- stg_orders depends on raw_orders\nCREATE VIEW stg_orders AS SELECT ... FROM raw_orders;\n\n-- fct_monthly_sales depends on stg_orders\nCREATE VIEW fct_monthly_sales AS \nSELECT ... FROM stg_orders ...;\n```\n\n## Why Layers?\n\n*   **Debuggability**: Fix logic in one place (`stg_orders`) fixes everything downstream.\n*   **Maintainability**: Separate cleaning from business logic.\n\n---\n\n## üéØ Your Task\n\nBuild a Staging view (cleaning) and a downstream Mart view (aggregating).",
    "starter_code": "-- 1. Staging Layer: Clean column names\nCREATE VIEW stg_orders AS\nSELECT \n    id as order_id,\n    user_id as customer_id,\n    order_ts::date as order_date,\n    COALESCE(amt, 0) as amount\nFROM raw_orders_v1;\n\n-- 2. Marts Layer: Business aggregation\nCREATE VIEW fct_daily_revenue AS\nSELECT \n    order_date,\n    COUNT(*) as num_orders,\n    SUM(amount) as total_revenue\nFROM stg_orders  -- References the view above!\nGROUP BY 1;",
    "solution_code": "CREATE VIEW stg AS SELECT * FROM raw; CREATE VIEW mart AS SELECT * FROM stg;",
    "expected_output": "CREATE VIEW"
  },
  "1257": {
    "title": "Build Analytics Pipeline",
    "chapter_title": "Analytics Engineering",
    "content": "# üõ†Ô∏è Build an Analytics Pipeline\n\n## The Goal\n\nRaw App Data ‚û°Ô∏è  Clean Data ‚û°Ô∏è  Business Insights.\n\n## Pipeline Steps\n\n1.  **Ingest**: `raw_events` table.\n2.  **Clean**: `stg_events` (parse JSON, handle nulls).\n3.  **Enrich**: `int_events_users` (join with user info).\n4.  **Aggregate**: `metric_daily_active_users`.\n\n## Common Pitfalls\n\n*   **Fan-out**: Joining 1:N incorrectly explodes row counts.\n*   **Time Travel**: Using future data to predict past events.\n*   **Hardcoding**: `WHERE id = 123` (Don't do this!).\n\n---\n\n## üéØ Your Task\n\nAssemble the pieces to create a functioning data pipeline.",
    "starter_code": "-- Step 1: Clean\nCREATE VIEW stg_page_views AS\nSELECT \n    event_id,\n    user_id,\n    url,\n    timestamp\nFROM raw_events\nWHERE event_type = 'page_view';\n\n-- Step 2: Enrich\nCREATE VIEW int_views_geo AS\nSELECT \n    v.*,\n    u.country\nFROM stg_page_views v\nLEFT JOIN users u ON v.user_id = u.id;\n\n-- Step 3: Metric\nSELECT \n    country,\n    COUNT(*) as views\nFROM int_views_geo\nGROUP BY 1\nORDER BY 2 DESC;",
    "solution_code": "SELECT country, COUNT(*) FROM int_views_geo GROUP BY 1;",
    "expected_output": "country|views\nUS|1500\nUK|800"
  },
  "1258": {
    "title": "Documenting Your Models",
    "chapter_title": "Style & Conventions",
    "content": "# üìù Documenting Code: Be Kind to Future You\n\n## Comments in SQL\n\n```sql\n-- Single line comment explaining WHY, not WHAT\n/* Multi-line comment for complex logic blocks */\n```\n\n## Database Metadata (The \"Real\" Docs)\n\nMost DBs support attaching comments to objects:\n\n```sql\nCOMMENT ON TABLE fct_orders IS 'Core fact table for approved orders';\nCOMMENT ON COLUMN fct_orders.amount IS 'Total value in USD, incl. tax';\n```\n\n## Why Document?\n\n1.  **Ambiguity**: Is `amount` in cents or dollars?\n2.  **Logic**: Why did we filter `status != 5`?\n3.  **Owner**: Who maintains this table?\n\n---\n\n## üéØ Your Task\n\nAdd documentation comments to your table and columns.",
    "starter_code": "-- Create table\nCREATE TABLE core_metrics (\n    date DATE,\n    dau INT\n);\n\n-- Document it\nCOMMENT ON TABLE core_metrics IS 'Daily Active Users tracked by login events';\nCOMMENT ON COLUMN core_metrics.dau IS 'Count of unique user_ids with at least one session';\n\n-- Verify docs (Postgres specific)\nSELECT obj_description('core_metrics'::regclass);",
    "solution_code": "COMMENT ON TABLE core_metrics IS 'docs';",
    "expected_output": "COMMENT"
  },
  "1259": {
    "title": "Testing Data Quality",
    "chapter_title": "Style & Conventions",
    "content": "# üß™ Testing Data Quality: Trust but Verify\n\n## Assertions in SQL\n\nA data test is a query that **should return 0 rows**.\n\n## Examples\n\n### 1. Uniqueness Test\n```sql\n-- Should return empty if order_id is unique\nSELECT order_id, COUNT(*)\nFROM orders\nGROUP BY 1\nHAVING COUNT(*) > 1;\n```\n\n### 2. Referential Integrity\n```sql\n-- Should return empty if all customers exist\nSELECT o.id\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL;\n```\n\n### 3. Logic/Business Rules\n```sql\n-- Discount shouldn't be negative\nSELECT * FROM orders WHERE discount < 0;\n```\n\n## Implementing Tests\n\nRun these queries. If `rows > 0`, **FAIL** the pipeline!\n\n---\n\n## üéØ Your Task\n\nWrite a test query to find invalid negative order amounts.",
    "starter_code": "-- Data Quality Test: Find invalid amounts\nSELECT \n    order_id,\n    amount\nFROM orders\nWHERE amount < 0\n   OR tax < 0\n   OR total_amount != (amount + tax);\n\n-- If this returns rows, we have a problem!",
    "solution_code": "SELECT * FROM orders WHERE amount < 0;",
    "expected_output": "order_id|amount\n(0 rows)"
  },
  "1260": {
    "title": "Code Review Challenge",
    "chapter_title": "Style & Conventions",
    "content": "# üïµÔ∏è Code Review Challenge: Spot the Badger\n\n## The Scenario\n\nJunior Data Analyst \"Badger\" wrote this query. It runs... but it's dangerous. Review it!\n\n## The Code\n\n```sql\nSELECT * \nFROM users, orders \nWHERE users.id = orders.user_id\n```\n\n## Issues to Spot\n\n1.  **Implicit Join** (`FROM a, b`): Use `JOIN .. ON` syntax! Implicit joins are archaic and prone to errors.\n2.  **SELECT ***: Select explicit columns. `*` breaks if schema changes and fetches unused data.\n3.  **No Aliases**: Hard to read `users.id`. Use `FROM users u`.\n\n## Corrected Version\n\n```sql\nSELECT \n    u.id,\n    u.email,\n    o.id as order_id\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n```\n\n---\n\n## üéØ Your Task\n\nRefactor a messy legacy query into clean, modern SQL.",
    "starter_code": "-- Legacy Query:\n-- SELECT * FROM products p, categories c WHERE p.cat_id = c.id AND p.price > 100;\n\n-- Refactored Modern SQL:\nSELECT \n    p.product_name,\n    p.price,\n    c.category_name\nFROM products p\nJOIN categories c ON p.cat_id = c.id\nWHERE p.price > 100;",
    "solution_code": "SELECT p.name FROM products p JOIN categories c ON p.cat_id = c.id;",
    "expected_output": "product_name|price|category_name\nPro Widget|150.00|Gadgets"
  },
  "1261": {
    "title": "Compute vs Storage Separation",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# ‚òÅÔ∏è Cloud Architecture: Compute vs Storage\n\n## Traditional DB (Postgres/MySQL)\n\n*   Compute (CPU) and Storage (Disk) are tied to one machine.\n*   To get more storage, you buy a bigger server (and waste CPU).\n\n## Cloud Warehouse (Snowflake/BigQuery)\n\n*   **Storage**: S3/GCS blob storage. Infinite, cheap. (~$23/TB/mo)\n*   **Compute**: 'Warehouses' or 'Slots'. Ephemeral, expensive. (~$2-3/hour)\n\n## The Benefit\n\n*   **Scale Independently**: Store Petabytes of data (cheap) but only pay for CPU when you run a query.\n*   **Concurrency**: 10 users can use 10 different compute clusters accessing the *same data* simultaneously.\n\n---\n\n## üéØ Your Task\n\nSimulate scaling compute up without moving data.",
    "starter_code": "-- Snowflake Syntax Simulation\n-- 1. Resize warehouse (Compute) instantly\nALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'LARGE';\n\n-- 2. Query heavy data (uses new power)\nSELECT COUNT(*) FROM massive_log_table;\n\n-- 3. suspend to save money\nALTER WAREHOUSE my_wh SUSPEND;",
    "solution_code": "ALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'LARGE';",
    "expected_output": "Statement executed successfully."
  },
  "1262": {
    "title": "Auto-Scaling & Elasticity",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# üìà Auto-Scaling: Handling Usage Spikes\n\n## What is Elasticity?\n\nThe ability to expand and contract resources based on demand.\n\n## Vertical Scaling (Scale Up)\n\n*   **Bigger Machine**: XS -> 2XL.\n*   **Use case**: Single huge query (Complex ML model training).\n\n## Horizontal Scaling (Scale Out)\n\n*   **More Machines**: 1 cluster -> 10 clusters.\n*   **Use case**: Monday morning rush (100 analysts running reports at 9 AM).\n\n## Auto-Suspending\n\n*   **Stop paying**: If no queries run for 5 mins, turn off compute.\n*   **Resume**: Instantly wake up when a new query arrives.\n\n---\n\n## üéØ Your Task\n\nConfigure a warehouse to auto-suspend after inactivity.",
    "starter_code": "-- Configure \"Elastic\" Warehouse\nCREATE WAREHOUSE analytics_wh\nWITH \n    WAREHOUSE_SIZE = 'MEDIUM'\n    AUTO_SUSPEND = 300  -- Seconds (5 mins)\n    AUTO_RESUME = TRUE\n    MIN_CLUSTER_COUNT = 1\n    MAX_CLUSTER_COUNT = 5; -- Auto-scale out for concurrency",
    "solution_code": "CREATE WAREHOUSE w WITH AUTO_SUSPEND = 300;",
    "expected_output": "Warehouse created."
  },
  "1263": {
    "title": "Cloud Warehouse Comparison",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# ‚öîÔ∏è Cloud Wars: Snowflake vs BigQuery vs Redshift\n\n## The Big Three\n\n| Feature | Snowflake | BigQuery | Redshift (Classic) |\n| :--- | :--- | :--- | :--- |\n| **Architecture** | Decoupled | Serverless | Coupled (mostly) |\n| **Pricing** | Per-second compute | Per-query (bytes scanned) | Hourly instance |\n| **Maintenance** | Near Zero | Zero | Moderate |\n\n## Selecting for Your Use Case\n\n*   **BigQuery**: Ad-hoc analysis. \"I don't know when I'll query, but I want it fast.\"\n*   **Snowflake**: Steady workloads. Data Engineering pipelines. Fine-grained control.\n*   **Redshift**: Low cost for predictable, always-on massive workloads.\n\n## Columnar Storage (All of them)\n\nThey store data by *column*, not row. \n*   `SELECT avg(age) FROM users` only reads the 'age' column blocks.\n*   **Tip**: `SELECT *` is evil in cloud warehouses! It scans everything.\n\n---\n\n## üéØ Your Task\n\nWrite a query optimized for columnar storage (select only what you need).",
    "starter_code": "-- ‚ùå Bad (Scans all columns)\n-- SELECT * FROM enormous_logs WHERE status = 'error';\n\n-- ‚úÖ Good (Scans only 2 columns)\nSELECT \n    timestamp, \n    error_message \nFROM enormous_logs \nWHERE status = 'error';",
    "solution_code": "SELECT col1 FROM table;",
    "expected_output": "timestamp|error_message"
  },
  "1264": {
    "title": "Choose the Right Warehouse",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# üìè Sizing Your Warehouse: XS, L, or 4XL?\n\n## T-Shirt Sizing\n\nSnowflake uses t-shirt sizes. Each size doubles the power (and cost).\n\n*   **XS**: 1 credit/hr. Good for basic loading, simple dev.\n*   **S**: 2 credits. Standard reporting.\n*   **L**: 8 credits. heavy aggregations on TBs of data.\n*   **4XL**: 128 credits. \"We need this done in 5 minutes or we lose money.\"\n\n## Linear Performance Scaling\n\nIdeally, a query taking 1 hour on XS takes 30 mins on S. \n*   If it doesn't, you aren't CPU bound (maybe data skew or network).\n\n## Strategy\n\n1.  Start on **XS**.\n2.  Check query duration.\n3.  If too slow, try **S**.\n4.  Stop increasing when speed gains diminish.\n\n---\n\n## üéØ Your Task\n\nExperiment with resizing a warehouse for a complex query.",
    "starter_code": "-- Start small\nALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'X-SMALL';\n-- Run Query A (Takes 10s)\n\n-- Scale up\nALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'MEDIUM';\n-- Run Query A (Takes 2s?) -> Worth it if time is money!",
    "solution_code": "ALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'MEDIUM';",
    "expected_output": "Statement executed successfully."
  },
  "1265": {
    "title": "Partition Design Challenge",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# üß© Partition Design Challenge\n\n## The Concept\n\n**Partitioning** divides your massive table into efficient chunks (usually by date).\n\n## Why Pruning Matters\n\nQuery: `WHERE date = '2024-01-01'`\n\n*   **Unpartitioned**: Scans 10 years of data (Slow, Expensive).\n*   **Partitioned**: Scans only the '2024-01-01' folder (Fast, Cheap).\n\n## Choosing a Key\n\n*   **Good**: Date (`order_date`), Low cardinality geo (`region`).\n*   **Bad**: Unique ID (`user_id`), High timestamp granularity (`timestamp_ms`).\n\n**Rule of Thumb**: Partition by how users *filter*.\n\n---\n\n## üéØ Your Task\n\nChoose the best partition key for a log table queried mainly by date and app_id.",
    "starter_code": "-- Create table with partitioning (BigQuery style)\nCREATE TABLE app_logs (\n    log_id INT64,\n    event_timestamp TIMESTAMP,\n    app_id STRING,\n    message STRING\n)\nPARTITION BY DATE(event_timestamp) -- Best for date filtering\nCLUSTER BY app_id;                 -- Secondary sort for app filtering",
    "solution_code": "CREATE TABLE logs (ts TIMESTAMP) PARTITION BY DATE(ts);",
    "expected_output": "Table created."
  },
  "1266": {
    "title": "Cost Reduction Challenge",
    "chapter_title": "Cloud Warehouse Features",
    "content": "# üí∏ Cost Reduction Challenge: Save the Budget!\n\n## The Emergency\n\nYour cloud bill just hit $10,000/month. The CTO is asking why. Fix it!\n\n## Top Cost Killers\n\n1.  **SELECT ***: Scanning unused columns.\n2.  **Runaway Queries**: Cartesian joins running for 24 hours.\n3.  **Oversized Warehouses**: Running a 4XL for a 10-row insert.\n4.  **Zombie Warehouses**: Auto-suspend disabled.\n\n## Your Audit Queries\n\n```sql\n-- Find most expensive queries\nSELECT query_text, cost \nFROM query_history \nORDER BY cost DESC LIMIT 10;\n\n-- Find unused tables (storage cost)\nSELECT table_name, last_accessed \nFROM table_stats \nWHERE last_accessed < CURRENT_DATE - 365;\n```\n\n---\n\n## üéØ Your Task\n\nIdentify and terminate a runaway query consuming resources.",
    "starter_code": "-- 1. Find long running queries (> 1 hour)\nSELECT \n    query_id,\n    user_name,\n    execution_time_seconds,\n    query_text\nFROM query_history\nWHERE state = 'RUNNING'\n  AND execution_time_seconds > 3600;\n\n-- 2. Kill it!\nSELECT SYSTEM$ABORT_QUERY('query_id_from_above');",
    "solution_code": "SELECT SYSTEM$ABORT_QUERY('12345');",
    "expected_output": "Query 12345 aborted."
  },
  "1267": {
    "title": "The Data Warehouse Migration",
    "chapter_title": "Final Boss",
    "content": "# üèÜ Final Boss: The Data Warehouse Migration\n\n## The Scenario\n\nYou are the Lead Data Engineer at *ScaleUp Inc*. The CTO has ordered a migration from a messy legacy database to a modern warehouse. \n\nYour final challenge is to generate the critical **\"Golden Table\"** that will power the CEO's dashboard.\n\n## The Data Models\n\n### 1. `legacy_users`\n| id | name | email | joined_at (Text!) | status |\n|----|------|-------|-------------------|--------|\n| 1 | Alice | a@x.com | \"2023-01-01\" | active |\n| 2 | Bob | | NULL | banned |\n\n### 2. `legacy_orders`\n| order_id | user_id | amount | paid_at |\n|----------|---------|--------|---------|\n| 101 | 1 | 99.99 | 2023-02-15 |\n\n## The Requirements\n\nCreate a report with the following columns:\n1.  **`user_id`**: Clean integer ID.\n2.  **`user_name`**: Title cased name.\n3.  **`cohort_month`**: The month they joined (e.g., '2023-01'). Handle string dates!\n4.  **`total_revenue`**: Sum of all their paid orders. Replace NULL with 0.\n5.  **`pct_rank`**: Their percentile rank by revenue (0-1). Top spender = 1.\n\n## Constraints\n\n*   Filter out **banned** users.\n*   Filter out users with **missing emails**.\n*   Revenue must include only **paid** orders (`paid_at` is not null).\n*   Sort by `total_revenue` DESC.\n\n---\n\n## üéØ Your Task\n\nWrite the query to generate this Golden Table.",
    "starter_code": "-- Build the Golden Table\n-- HINT: Use CTEs to clean data step-by-step\n\nWITH cleaned_users AS (\n    -- 1. Clean users (Filter banned/no-email, fix dates)\n    SELECT \n        id,\n        name,\n        TO_DATE(joined_at, 'YYYY-MM-DD') as join_date\n    FROM legacy_users\n    WHERE status != 'banned'\n),\n\nuser_revenue AS (\n    -- 2. Aggregate revenue for these users\n    SELECT \n        u.id,\n        COUNT(o.order_id) as order_count\n    FROM cleaned_users u\n    LEFT JOIN legacy_orders o ON u.id = o.user_id\n    GROUP BY 1\n)\n\n-- 3. Final Select with Window Function\nSELECT * FROM user_revenue;",
    "solution_code": "WITH clean_users AS (\n    SELECT \n        id, \n        INITCAP(name) as user_name, \n        DATE_TRUNC('month', CAST(joined_at AS DATE))::DATE as cohort_month\n    FROM legacy_users \n    WHERE status != 'banned' AND email IS NOT NULL\n),\nrevenue AS (\n    SELECT \n        user_id, \n        SUM(amount) as total_revenue \n    FROM legacy_orders \n    WHERE paid_at IS NOT NULL \n    GROUP BY 1\n)\nSELECT \n    u.id as user_id,\n    u.user_name,\n    u.cohort_month,\n    COALESCE(r.total_revenue, 0) as total_revenue,\n    PERCENT_RANK() OVER (ORDER BY COALESCE(r.total_revenue, 0) ASC) as pct_rank\nFROM clean_users u\nLEFT JOIN revenue r ON u.id = r.user_id\nORDER BY 4 DESC;",
    "expected_output": "user_id|user_name|cohort_month|total_revenue|pct_rank\n1|Alice|2023-01-01|99.99|1.0"
  },
  "218": {
    "title": "Imputation Strategy",
    "chapter_title": "Data Cleaning",
    "content": "# üîß Imputation: Filling Missing Values Smartly\n\n## Why Not Just Delete Missing Data?\n\nDeleting rows with missing values seems easy, but:\n- You might lose 30%+ of your data\n- Remaining data may be biased\n- Some ML models can't handle NaN anyway\n\n**Solution**: Impute (fill in) missing values intelligently!\n\n## Simple Imputation Strategies\n\n### 1. Mean Imputation\nReplace missing with the column average. Best for normally distributed numeric data.\n\n```python\ndf['age'].fillna(df['age'].mean())\n```\n\n### 2. Median Imputation\nReplace with the middle value. Better when data has outliers!\n\n```python\ndf['income'].fillna(df['income'].median())\n```\n\n### 3. Mode Imputation\nReplace with most common value. Best for categorical data.\n\n```python\ndf['city'].fillna(df['city'].mode()[0])\n```\n\n## Advanced Strategies\n\n### Forward/Backward Fill\nUse previous/next value. Great for time series!\n\n```python\ndf['price'].fillna(method='ffill')  # Use previous value\ndf['price'].fillna(method='bfill')  # Use next value\n```\n\n### Group-Based\nImpute based on similar groups:\n\n```python\ndf['salary'] = df.groupby('department')['salary'].transform(\n    lambda x: x.fillna(x.median())\n)\n```\n\n## When to Use Which\n\n| Strategy | Use When |\n|----------|----------|\n| Mean | Normal distribution, no outliers |\n| Median | Outliers present, skewed data |\n| Mode | Categorical data |\n| Forward fill | Time series |\n| Group-based | Clear grouping variable exists |\n\n---\n\n## üéØ Your Task\n\nImpute missing values in a dataset using appropriate strategies.",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Sample data with missing values\ndf = pd.DataFrame({\n    'age': [25, np.nan, 35, 40, np.nan, 55],\n    'salary': [50000, 60000, np.nan, 80000, 70000, np.nan],\n    'department': ['Sales', 'HR', 'Sales', np.nan, 'HR', 'Sales']\n})\n\nprint(\"Before imputation:\")\nprint(df)\n\n# Impute age with median (robust to outliers)\ndf['age'] = df['age'].fillna(df['age'].median())\n\n# Impute salary with mean\ndf['salary'] = df['salary'].fillna(df['salary'].mean())\n\n# Impute department with mode\ndf['department'] = df['department'].fillna(df['department'].mode()[0])\n\nprint(\"\\nAfter imputation:\")\nprint(df)",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'age': [25, np.nan, 35, 40, np.nan, 55],\n    'salary': [50000, 60000, np.nan, 80000, 70000, np.nan],\n    'department': ['Sales', 'HR', 'Sales', np.nan, 'HR', 'Sales']\n})\n\ndf['age'] = df['age'].fillna(df['age'].median())\ndf['salary'] = df['salary'].fillna(df['salary'].mean())\ndf['department'] = df['department'].fillna(df['department'].mode()[0])\nprint(df)",
    "expected_output": "After imputation:"
  },
  "219": {
    "title": "Missing Data Report",
    "chapter_title": "Data Cleaning",
    "content": "# üìã Missing Data Report: Assess Before You Clean\n\n## Why Report First?\n\nBefore cleaning, you need to understand your data quality:\n- Which columns have missing values?\n- How much is missing?\n- Is it random or systematic?\n\n## The Essential Report\n\n```python\n# Count missing per column\nmissing_count = df.isnull().sum()\n\n# Percentage missing\nmissing_pct = (df.isnull().sum() / len(df)) * 100\n\n# Create a report\nreport = pd.DataFrame({\n    'Missing': missing_count,\n    'Percent': missing_pct.round(1)\n}).sort_values('Missing', ascending=False)\n```\n\n## Key Metrics to Check\n\n1. **Total missing values**: `df.isnull().sum().sum()`\n2. **Rows with any missing**: `df.isnull().any(axis=1).sum()`\n3. **Columns with > X% missing**: Candidate for dropping\n\n## Deciding Strategy\n\n| Missing % | Strategy |\n|-----------|----------|\n| < 5% | Impute (fill in) |\n| 5-30% | Impute carefully, consider impact |\n| > 30% | Consider dropping column |\n| Random | Safe to impute |\n| Systematic | Investigate why! |\n\n## Real-World Example\n\n```python\n# Quick data quality check\ndef missing_report(df):\n    missing = df.isnull().sum()\n    pct = (missing / len(df)) * 100\n    report = pd.DataFrame({'Missing': missing, 'Pct': pct})\n    return report[report['Missing'] > 0].sort_values('Pct', ascending=False)\n```\n\n---\n\n## üéØ Your Task\n\nGenerate a missing data report for a dataset.",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Sample data with missing values\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', None, 'Diana', 'Eve'],\n    'age': [25, None, 35, None, 45],\n    'salary': [50000, 60000, None, 80000, None],\n    'department': ['Sales', 'HR', 'IT', None, 'Sales']\n})\n\nprint(\"Original Data:\")\nprint(df)\n\n# Missing data report\nmissing_count = df.isnull().sum()\nmissing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n\nreport = pd.DataFrame({\n    'Missing Count': missing_count,\n    'Missing %': missing_pct\n}).sort_values('Missing Count', ascending=False)\n\nprint(\"\\nMissing Data Report:\")\nprint(report[report['Missing Count'] > 0])\nprint(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', None, 'Diana'],\n    'age': [25, None, 35, None],\n    'salary': [50000, 60000, None, 80000]\n})\n\nreport = pd.DataFrame({\n    'Missing': df.isnull().sum(),\n    'Pct': (df.isnull().sum() / len(df) * 100).round(1)\n})\nprint(report)",
    "expected_output": "Missing Data Report:\n            Missing Count  Missing %\nage                     2       40.0\nsalary                  2       40.0\ndepartment              1       20.0\nname                    1       20.0"
  },
  "220": {
    "title": "Forward Fill",
    "chapter_title": "Data Cleaning",
    "content": "# ‚û°Ô∏è Forward Fill: Propagate Last Valid Value\n\n## What is Forward Fill?\n\nForward fill (ffill) replaces missing values with the **last known value**. It's like saying \"keep using whatever we had before.\"\n\n## Visual Example\n\n```\nBefore:  [1, NaN, NaN, 4, NaN, 6]\nAfter:   [1,  1,   1,  4,  4,  6]\n              ‚Üë    ‚Üë      ‚Üë\n        Filled from previous value\n```\n\n## When to Use Forward Fill\n\n‚úÖ **Time series data**: Stock prices, sensor readings\n‚úÖ **Status data**: Customer status, subscription level\n‚úÖ **Sampling gaps**: Missing hourly readings\n\n‚ùå **Don't use when**: Missing data is structural, not temporal\n\n## Implementation\n\n```python\n# Pandas forward fill\ndf['column'].fillna(method='ffill')\n\n# Or the newer way (pandas 2.0+)\ndf['column'].ffill()\n\n# Limit how far to fill\ndf['column'].ffill(limit=2)  # Only fill 2 consecutive NaNs\n```\n\n## Backward Fill Too!\n\n```python\n# Fill from the next value instead\ndf['column'].fillna(method='bfill')\ndf['column'].bfill()\n```\n\n## Real-World Example\n\n```python\n# Sensor data with gaps\nsensor_data = pd.DataFrame({\n    'timestamp': pd.date_range('2024-01-01', periods=10, freq='H'),\n    'temperature': [20, 21, None, None, 23, 24, None, 25, 26, None]\n})\n\n# Forward fill missing readings\nsensor_data['temperature'] = sensor_data['temperature'].ffill()\n```\n\n---\n\n## üéØ Your Task\n\nUse forward fill to handle missing values in time series data.",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Time series with gaps\ndf = pd.DataFrame({\n    'date': pd.date_range('2024-01-01', periods=7, freq='D'),\n    'temperature': [20.0, np.nan, np.nan, 23.0, np.nan, 25.0, np.nan],\n    'status': ['active', np.nan, np.nan, 'inactive', np.nan, 'active', np.nan]\n})\n\nprint(\"Before forward fill:\")\nprint(df)\n\n# Apply forward fill\ndf['temperature'] = df['temperature'].ffill()\ndf['status'] = df['status'].ffill()\n\nprint(\"\\nAfter forward fill:\")\nprint(df)",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'date': pd.date_range('2024-01-01', periods=5),\n    'temp': [20.0, np.nan, np.nan, 23.0, np.nan]\n})\n\ndf['temp'] = df['temp'].ffill()\nprint(df)",
    "expected_output": "After forward fill:\n        date  temperature    status\n0 2024-01-01         20.0    active\n1 2024-01-02         20.0    active\n2 2024-01-03         20.0    active\n3 2024-01-04         23.0  inactive\n4 2024-01-05         23.0  inactive\n5 2024-01-06         25.0    active\n6 2024-01-07         25.0    active"
  },
  "221": {
    "title": "Group-based Fill",
    "chapter_title": "Data Cleaning",
    "content": "# üè∑Ô∏è Group-Based Imputation: Smarter Missing Value Handling\n\n## Why Group-Based Fill?\n\nUsing the global mean ignores important patterns:\n\n```\nDepartment | Salary\nEngineering | 120K\nEngineering | NaN     ‚Üê Fill with engineering avg, not company avg!\nSales | 60K\nSales | NaN           ‚Üê Fill with sales avg!\n```\n\n## The Pattern\n\n```python\ndf['salary'] = df.groupby('department')['salary'].transform(\n    lambda x: x.fillna(x.mean())\n)\n```\n\n## Why transform()?\n\n`transform()` returns a Series the same size as the original, so it can be assigned back:\n\n```python\n# This returns group means (3 values for 3 departments)\ndf.groupby('department')['salary'].mean()\n\n# This returns filled values (same length as original)\ndf.groupby('department')['salary'].transform(lambda x: x.fillna(x.mean()))\n```\n\n## Common Fill Strategies\n\n```python\n# Fill with group mean\n.transform(lambda x: x.fillna(x.mean()))\n\n# Fill with group median (better for skewed data)\n.transform(lambda x: x.fillna(x.median()))\n\n# Fill with group mode (for categorical)\n.transform(lambda x: x.fillna(x.mode()[0] if len(x.mode()) > 0 else None))\n\n# Forward fill within group (time series)\n.transform(lambda x: x.ffill())\n```\n\n## Real-World Example\n\n```python\n# Fill missing product ratings with category average\ndf['rating'] = df.groupby('category')['rating'].transform(\n    lambda x: x.fillna(x.mean())\n)\n```\n\n---\n\n## üéØ Your Task\n\nFill missing salaries with department-specific averages.",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Employee data with missing salaries\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n    'department': ['Engineering', 'Engineering', 'Sales', 'Sales', 'Engineering', 'Sales'],\n    'salary': [100000, np.nan, 60000, np.nan, 110000, 55000]\n})\n\nprint(\"Before (with NaN):\")\nprint(df)\n\n# Fill missing salaries with department average\ndf['salary'] = df.groupby('department')['salary'].transform(\n    lambda x: x.fillna(x.mean())\n)\n\nprint(\"\\nAfter (group-based fill):\")\nprint(df)",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'dept': ['Eng', 'Eng', 'Sales', 'Sales'],\n    'salary': [100000, np.nan, 60000, np.nan]\n})\n\ndf['salary'] = df.groupby('dept')['salary'].transform(lambda x: x.fillna(x.mean()))\nprint(df)",
    "expected_output": "After (group-based fill):\n      name   department    salary\n0    Alice  Engineering  100000.0\n1      Bob  Engineering  105000.0\n2  Charlie        Sales   60000.0\n3    Diana        Sales   57500.0\n4      Eve  Engineering  110000.0\n5    Frank        Sales   55000.0"
  },
  "222": {
    "title": "Duplicate Report",
    "chapter_title": "Data Cleaning",
    "content": "# üîç Finding Duplicates: Quality Control for Data\n\n## Why Duplicates Are Problems\n\nDuplicates corrupt your analysis:\n- **Inflated counts**: \"We have 1000 orders!\" (Actually 800)\n- **Wrong averages**: Some records count twice\n- **Wasted resources**: Processing same data multiple times\n\n## Finding Duplicates\n\n```python\n# Check if any duplicates exist\ndf.duplicated().any()  # True/False\n\n# Count total duplicates\ndf.duplicated().sum()  # e.g., 23\n\n# Show the duplicate rows\ndf[df.duplicated()]\n\n# Show ALL occurrences (including first)\ndf[df.duplicated(keep=False)]\n```\n\n## Duplicates on Specific Columns\n\n```python\n# Same customer_id and order_date = duplicate order\ndf[df.duplicated(subset=['customer_id', 'order_date'])]\n```\n\n## Creating a Duplicate Report\n\n```python\n# Complete analysis\ndef duplicate_report(df, subset=None):\n    dupes = df.duplicated(subset=subset, keep=False)\n    return {\n        'total_rows': len(df),\n        'duplicate_rows': dupes.sum(),\n        'unique_rows': len(df) - df.duplicated(subset=subset).sum(),\n        'duplicate_pct': (dupes.sum() / len(df) * 100).round(2)\n    }\n```\n\n## Removing Duplicates\n\n```python\n# Keep first occurrence\ndf.drop_duplicates()\n\n# Keep last occurrence\ndf.drop_duplicates(keep='last')\n\n# Based on specific columns\ndf.drop_duplicates(subset=['customer_id', 'product_id'])\n```\n\n---\n\n## üéØ Your Task\n\nGenerate a duplicate report and clean a dataset.",
    "starter_code": "import pandas as pd\n\n# Data with duplicates\ndf = pd.DataFrame({\n    'order_id': [1, 2, 3, 3, 4, 5, 5, 5],\n    'customer': ['Alice', 'Bob', 'Charlie', 'Charlie', 'Diana', 'Eve', 'Eve', 'Eve'],\n    'amount': [100, 200, 150, 150, 300, 250, 250, 250]\n})\n\nprint(\"Original data:\")\nprint(df)\n\n# Duplicate analysis\ntotal_dupes = df.duplicated().sum()\ndupe_pct = (total_dupes / len(df) * 100)\n\nprint(f\"\\nDuplicate Report:\")\nprint(f\"  Total rows: {len(df)}\")\nprint(f\"  Duplicate rows: {total_dupes}\")\nprint(f\"  Duplicate %: {dupe_pct:.1f}%\")\n\n# Remove duplicates\nclean_df = df.drop_duplicates()\nprint(f\"\\nAfter cleaning: {len(clean_df)} rows\")",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'order_id': [1, 2, 3, 3, 4, 5, 5],\n    'customer': ['A', 'B', 'C', 'C', 'D', 'E', 'E']\n})\n\nprint(f\"Duplicates: {df.duplicated().sum()}\")\nclean = df.drop_duplicates()\nprint(f\"After: {len(clean)} rows\")",
    "expected_output": "Duplicate Report:\n  Total rows: 8\n  Duplicate rows: 3\n  Duplicate %: 37.5%"
  },
  "223": {
    "title": "Smart Dedup",
    "chapter_title": "Data Cleaning",
    "content": "# üß† Smart Deduplication: Beyond Simple Removal\n\n## When Simple Dedup Isn't Enough\n\nSometimes you need to:\n- Keep the row with the MOST data (fewest NaNs)\n- Keep the LATEST record\n- Merge information from duplicate rows\n\n## Keep Row with Most Data\n\n```python\n# Count non-null values per row\ndf['non_null_count'] = df.notna().sum(axis=1)\n\n# Sort by non-null count, then drop duplicates keeping first\ndf = df.sort_values('non_null_count', ascending=False)\ndf = df.drop_duplicates(subset='customer_id', keep='first')\ndf = df.drop('non_null_count', axis=1)\n```\n\n## Keep Most Recent Record\n\n```python\n# Sort by date descending, keep first\ndf = df.sort_values('last_updated', ascending=False)\ndf = df.drop_duplicates(subset='customer_id', keep='first')\n```\n\n## Aggregate Duplicates\n\nInstead of picking one, combine them:\n\n```python\n# Sum amounts for duplicate customers\ndf = df.groupby('customer_id').agg({\n    'name': 'first',\n    'amount': 'sum',\n    'last_login': 'max'\n}).reset_index()\n```\n\n## Real-World Example\n\n```python\n# E-commerce: merge duplicate customer profiles\ndf = df.sort_values(['customer_id', 'last_updated'], ascending=[True, False])\nmerged = df.groupby('customer_id').agg({\n    'email': 'first',          # Keep most recent email\n    'orders': 'sum',           # Sum all orders\n    'lifetime_value': 'sum',   # Sum revenue\n    'created_at': 'min'        # Keep earliest signup\n}).reset_index()\n```\n\n---\n\n## üéØ Your Task\n\nImplement smart deduplication keeping the most complete record.",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Duplicate records with varying completeness\ndf = pd.DataFrame({\n    'customer_id': [1, 1, 2, 2, 3],\n    'name': ['Alice', 'Alice', 'Bob', None, 'Charlie'],\n    'email': [None, 'alice@x.com', 'bob@x.com', 'bob@x.com', 'charlie@x.com'],\n    'phone': ['555-1234', None, None, '555-5678', '555-9999']\n})\n\nprint(\"Before (with duplicates):\")\nprint(df)\n\n# Count non-null values per row\ndf['completeness'] = df.notna().sum(axis=1)\n\n# Sort by completeness descending, keep most complete\ndf = df.sort_values('completeness', ascending=False)\nclean = df.drop_duplicates(subset='customer_id', keep='first')\nclean = clean.drop('completeness', axis=1)\n\nprint(\"\\nAfter (keeping most complete):\")\nprint(clean)",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'id': [1, 1, 2, 2],\n    'name': ['A', 'A', 'B', None],\n    'email': [None, 'a@x.com', 'b@x.com', 'b@x.com']\n})\n\ndf['completeness'] = df.notna().sum(axis=1)\ndf = df.sort_values('completeness', ascending=False)\nclean = df.drop_duplicates(subset='id', keep='first').drop('completeness', axis=1)\nprint(clean)",
    "expected_output": "After (keeping most complete):\n   customer_id     name          email     phone\n1            1    Alice    alice@x.com      None\n3            2      Bob      bob@x.com  555-5678\n4            3  Charlie  charlie@x.com  555-9999"
  },
  "224": {
    "title": "Regex Cleanup",
    "chapter_title": "Data Cleaning",
    "content": "# üßπ Regex for Data Cleaning: Pattern-Based Fixes\n\n## Why Regex for Cleaning?\n\nSome cleaning tasks need patterns, not exact matches:\n- Remove all numbers from text\n- Extract data from mixed-format strings\n- Standardize inconsistent formats\n\n## Common Cleaning Patterns\n\n### Remove Non-Numeric Characters\n```python\n# Keep only digits\ndf['phone'] = df['phone'].str.replace(r'[^0-9]', '', regex=True)\n# \"555-123-4567\" ‚Üí \"5551234567\"\n```\n\n### Remove Extra Whitespace\n```python\n# Multiple spaces to single space\ndf['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n```\n\n### Extract Parts of Text\n```python\n# Extract price from text\ndf['price'] = df['description'].str.extract(r'\\$(\\d+\\.\\d{2})')\n```\n\n### Standardize Formats\n```python\n# Standardize dates\ndf['date'] = df['date'].str.replace(r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', r'\\3-\\1-\\2', regex=True)\n# \"1/15/2024\" ‚Üí \"2024-1-15\"\n```\n\n## Pandas + Regex Methods\n\n```python\n# Check if pattern matches\ndf['is_email'] = df['contact'].str.contains(r'@.*\\.', regex=True)\n\n# Replace matches\ndf['clean'] = df['text'].str.replace(r'pattern', 'replacement', regex=True)\n\n# Extract groups\ndf[['area', 'number']] = df['phone'].str.extract(r'(\\d{3})-(\\d{7})')\n```\n\n---\n\n## üéØ Your Task\n\nClean phone numbers by removing all non-numeric characters.",
    "starter_code": "import pandas as pd\n\n# Messy phone number formats\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n    'phone': ['(555) 123-4567', '555.987.6543', '555-111-2222', '+1 555 999 8888']\n})\n\nprint(\"Before cleaning:\")\nprint(df)\n\n# Remove all non-numeric characters using regex\ndf['phone_clean'] = df['phone'].str.replace(r'[^0-9]', '', regex=True)\n\nprint(\"\\nAfter cleaning:\")\nprint(df[['name', 'phone_clean']])",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob'],\n    'phone': ['(555) 123-4567', '555.987.6543']\n})\n\ndf['phone_clean'] = df['phone'].str.replace(r'[^0-9]', '', regex=True)\nprint(df[['name', 'phone_clean']])",
    "expected_output": "After cleaning:\n      name  phone_clean\n0    Alice   5551234567\n1      Bob   5559876543\n2  Charlie   5551112222\n3    Diana  15559998888"
  },
  "225": {
    "title": "Standardize Format",
    "chapter_title": "Data Cleaning",
    "content": "# üìè Format Standardization: Consistency is Key\n\n## The Inconsistency Problem\n\nReal data has inconsistent formats:\n```\n\"New York\", \"NEW YORK\", \"new york\", \"N.Y.\"\n\"01/15/2024\", \"2024-01-15\", \"Jan 15, 2024\"\n\"$1,000.00\", \"1000\", \"USD 1000\"\n```\n\nThese are the SAME data but won't match in analysis!\n\n## Text Standardization\n\n```python\n# Case normalization\ndf['city'] = df['city'].str.lower()         # new york\ndf['city'] = df['city'].str.upper()         # NEW YORK\ndf['city'] = df['city'].str.title()         # New York\n\n# Strip whitespace\ndf['name'] = df['name'].str.strip()         # Remove leading/trailing\ndf['name'] = df['name'].str.replace('  ', ' ')  # Fix double spaces\n\n# Abbreviation mapping\ncity_map = {'NYC': 'New York', 'LA': 'Los Angeles', 'SF': 'San Francisco'}\ndf['city'] = df['city'].replace(city_map)\n```\n\n## Date Standardization\n\n```python\n# Convert multiple formats to one\ndf['date'] = pd.to_datetime(df['date'])  # Pandas auto-detects format\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')  # Output: 2024-01-15\n```\n\n## Number Standardization\n\n```python\n# Remove currency symbols and commas\ndf['amount'] = df['amount'].str.replace('[$,]', '', regex=True)\ndf['amount'] = df['amount'].astype(float)\n```\n\n## Complete Pipeline\n\n```python\ndef standardize_text(series):\n    return series.str.strip().str.lower().str.replace('\\s+', ' ', regex=True)\n```\n\n---\n\n## ÔøΩÔøΩ Your Task\n\nStandardize a dataset with inconsistent text formats.",
    "starter_code": "import pandas as pd\n\n# Inconsistent data formats\ndf = pd.DataFrame({\n    'name': ['  Alice Smith  ', 'BOB JONES', 'charlie brown'],\n    'city': ['new york', 'NEW YORK', 'NYC'],\n    'amount': ['$1,500.00', '2000', '$750.50']\n})\n\nprint(\"Before standardization:\")\nprint(df)\n\n# Standardize names: strip + title case\ndf['name'] = df['name'].str.strip().str.title()\n\n# Standardize cities: title case + replace abbreviations\ncity_map = {'Nyc': 'New York'}\ndf['city'] = df['city'].str.title().replace(city_map)\n\n# Standardize amounts: remove $ and commas\ndf['amount'] = df['amount'].str.replace('[$,]', '', regex=True).astype(float)\n\nprint(\"\\nAfter standardization:\")\nprint(df)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['  Alice  ', 'BOB'],\n    'city': ['new york', 'NYC'],\n    'amount': ['$1,500', '2000']\n})\n\ndf['name'] = df['name'].str.strip().str.title()\ndf['city'] = df['city'].str.title().replace({'Nyc': 'New York'})\ndf['amount'] = df['amount'].str.replace('[$,]', '', regex=True).astype(float)\nprint(df)",
    "expected_output": "After standardization:\n          name      city   amount\n0  Alice Smith  New York  1500.00\n1    Bob Jones  New York  2000.00\n2 Charlie Brown  New York   750.50"
  },
  "226": {
    "title": "Batch Conversion",
    "chapter_title": "Data Cleaning",
    "content": "# üîÑ Batch Type Conversion: Fix Data Types at Scale\n\n## Why Type Conversion Matters\n\nWrong data types cause problems:\n- Can't do math on strings: `\"100\" + \"50\" = \"10050\"` üò±\n- Dates stored as strings won't sort correctly\n- Memory waste: strings use more space than numbers\n\n## Identifying Type Issues\n\n```python\n# Check current types\ndf.dtypes\n\n# Check for mixed types\ndf['column'].apply(type).value_counts()\n```\n\n## Batch Conversion Strategies\n\n### Convert Specific Columns\n```python\n# Multiple columns at once\ndf[['col1', 'col2', 'col3']] = df[['col1', 'col2', 'col3']].astype(float)\n```\n\n### Convert by Pattern\n```python\n# All columns ending in '_id' to int\nid_cols = [c for c in df.columns if c.endswith('_id')]\ndf[id_cols] = df[id_cols].astype(int)\n```\n\n### Convert to Optimal Types\n```python\n# Automatically downcast to smallest type\ndf = df.convert_dtypes()\n\n# Reduce memory: int64 ‚Üí int32 or smaller\ndf['count'] = pd.to_numeric(df['count'], downcast='integer')\n```\n\n## Safe Conversion with Errors\n\n```python\n# Don't crash on bad data\ndf['price'] = pd.to_numeric(df['price'], errors='coerce')  # Bad ‚Üí NaN\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\n```\n\n## Memory Savings\n\n```\nOriginal: \n  customer_id (int64): 8 bytes per row\n  status (object): ~50 bytes per row\n\nAfter optimization:\n  customer_id (int32): 4 bytes per row  \n  status (category): ~2 bytes per row\n```\n\n---\n\n## üéØ Your Task\n\nConvert columns to appropriate data types in batch.",
    "starter_code": "import pandas as pd\n\n# Data with wrong types\ndf = pd.DataFrame({\n    'customer_id': ['1', '2', '3', '4'],  # Should be int\n    'amount': ['100.50', '200.75', '150.00', 'N/A'],  # Should be float\n    'date': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],  # Should be datetime\n    'is_active': ['True', 'False', 'True', 'True']  # Should be bool\n})\n\nprint(\"Before - Data types:\")\nprint(df.dtypes)\n\n# Batch conversions\ndf['customer_id'] = df['customer_id'].astype(int)\ndf['amount'] = pd.to_numeric(df['amount'], errors='coerce')  # N/A ‚Üí NaN\ndf['date'] = pd.to_datetime(df['date'])\ndf['is_active'] = df['is_active'].map({'True': True, 'False': False})\n\nprint(\"\\nAfter - Data types:\")\nprint(df.dtypes)\nprint(\"\\nConverted data:\")\nprint(df)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'id': ['1', '2', '3'],\n    'amount': ['100.5', '200', 'N/A'],\n    'date': ['2024-01-15', '2024-02-20', '2024-03-10']\n})\n\ndf['id'] = df['id'].astype(int)\ndf['amount'] = pd.to_numeric(df['amount'], errors='coerce')\ndf['date'] = pd.to_datetime(df['date'])\nprint(df.dtypes)",
    "expected_output": "After - Data types:\ncustomer_id            int64\namount               float64\ndate          datetime64[ns]\nis_active               bool"
  },
  "227": {
    "title": "Error Handling",
    "chapter_title": "Data Cleaning",
    "content": "# ‚ö†Ô∏è Error Handling in Data Cleaning: Graceful Failures\n\n## Why Error Handling Matters\n\nReal data WILL have problems:\n- Unexpected formats\n- Missing files\n- Invalid values\n- Encoding issues\n\nYour pipeline should handle these gracefully, not crash!\n\n## Python Try/Except Pattern\n\n```python\ntry:\n    # Risky operation\n    value = int(user_input)\nexcept ValueError:\n    # Handle the error\n    value = 0  # default\n    print(\"Invalid input, using default\")\n```\n\n## Pandas Error Handling\n\n```python\n# Convert with error handling\ndf['amount'] = pd.to_numeric(df['amount'], errors='coerce')  # Bad ‚Üí NaN\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\n\n# Check errors='coerce' options:\n# 'raise' - throw error (default)\n# 'coerce' - set to NaN\n# 'ignore' - leave as-is\n```\n\n## Logging Errors for Review\n\n```python\n# Track what went wrong\nerrors = []\n\nfor idx, row in df.iterrows():\n    try:\n        process_row(row)\n    except Exception as e:\n        errors.append({'row': idx, 'error': str(e)})\n\n# Review errors after\nerror_df = pd.DataFrame(errors)\n```\n\n## Safe File Reading\n\n```python\nimport os\n\ndef safe_read_csv(filepath):\n    if not os.path.exists(filepath):\n        print(f\"File not found: {filepath}\")\n        return None\n    \n    try:\n        return pd.read_csv(filepath)\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return None\n```\n\n---\n\n## üéØ Your Task\n\nImplement error handling for data type conversion.",
    "starter_code": "import pandas as pd\n\n# Messy data that will cause errors\ndf = pd.DataFrame({\n    'amount': ['100', '200', 'INVALID', '400', 'N/A'],\n    'date': ['2024-01-15', 'bad_date', '2024-03-10', '', '2024-05-20']\n})\n\nprint(\"Original data:\")\nprint(df)\n\n# Safe conversion with error handling\ndf['amount_clean'] = pd.to_numeric(df['amount'], errors='coerce')\ndf['date_clean'] = pd.to_datetime(df['date'], errors='coerce')\n\n# Report what couldn't be converted\namount_errors = df['amount_clean'].isna().sum()\ndate_errors = df['date_clean'].isna().sum()\n\nprint(f\"\\nConversion errors:\")\nprint(f\"  Amount: {amount_errors} errors\")\nprint(f\"  Date: {date_errors} errors\")\n\nprint(\"\\nCleaned data:\")\nprint(df)",
    "solution_code": "import pandas as pd\n\ndf = pd.DataFrame({\n    'amount': ['100', 'INVALID', '300'],\n    'date': ['2024-01-15', 'bad', '2024-03-10']\n})\n\ndf['amount'] = pd.to_numeric(df['amount'], errors='coerce')\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\n\nprint(f\"Errors: {df.isna().sum().sum()}\")",
    "expected_output": "Conversion errors:\n  Amount: 2 errors\n  Date: 2 errors"
  },
  "228": {
    "title": "Weighted Mean",
    "chapter_title": "Statistics",
    "content": "# ‚öñÔ∏è Weighted Mean: When Not All Values Are Equal\n\n## What is a Weighted Mean?\n\nIn a regular mean, every value counts equally. But what if some values are MORE important? That's where **weighted mean** comes in!\n\n## Real-World Analogy\n\nYour final grade isn't a simple average‚Äîexams (worth 60%) matter more than homework (worth 40%):\n\n| Component | Score | Weight |\n|-----------|-------|--------|\n| Homework | 95 | 40% |\n| Exam | 80 | 60% |\n\nSimple average: (95 + 80) / 2 = 87.5\nWeighted average: (95 √ó 0.4) + (80 √ó 0.6) = 38 + 48 = **86**\n\nThe exam pulls your grade down more because it's weighted higher!\n\n## The Formula\n\n```python\nweighted_mean = sum(value √ó weight) / sum(weights)\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\n\nscores = np.array([95, 80])\nweights = np.array([0.4, 0.6])\n\nweighted_avg = np.average(scores, weights=weights)\nprint(weighted_avg)  # 86.0\n```\n\n## When to Use\n\n‚úÖ Grading systems with different component weights\n‚úÖ Financial portfolio returns\n‚úÖ Survey data where some responses are more reliable\n‚úÖ Moving averages in time series\n\n---\n\n## üéØ Your Task\n\nYou're analyzing customer satisfaction scores from different regions. Each region has a different number of customers (the weight). Calculate the weighted average satisfaction score!\n\n**Data:**\n- Region A: Score 4.2, 1000 customers\n- Region B: Score 4.8, 500 customers\n- Region C: Score 3.9, 2000 customers",
    "starter_code": "import numpy as np\n\n# Satisfaction scores by region\nscores = np.array([4.2, 4.8, 3.9])\n\n# Number of customers in each region (weights)\ncustomers = np.array([1000, 500, 2000])\n\n# Calculate weighted average satisfaction\n# Hint: Use np.average() with the weights parameter\nweighted_satisfaction = None  # Your code here\n\nprint(f\"Weighted Average Satisfaction: {weighted_satisfaction:.2f} / 5.0\")",
    "solution_code": "import numpy as np\n\n# Satisfaction scores by region\nscores = np.array([4.2, 4.8, 3.9])\n\n# Number of customers in each region (weights)\ncustomers = np.array([1000, 500, 2000])\n\n# Calculate weighted average satisfaction\nweighted_satisfaction = np.average(scores, weights=customers)\n\nprint(f\"Weighted Average Satisfaction: {weighted_satisfaction:.2f} / 5.0\")",
    "expected_output": "Weighted Average Satisfaction: 4.09 / 5.0"
  },
  "229": {
    "title": "Central Tendency Challenge",
    "chapter_title": "Statistics",
    "content": "# üèÜ Central Tendency Challenge: Salary Analysis\n\n## The Scenario\n\nYou're a data analyst at a tech company. HR wants to understand the salary distribution to make fair hiring decisions. But here's the twist‚Äî**the CEO makes $10 million!**\n\n## Why This Matters\n\nThis challenge demonstrates a critical real-world problem: **extreme outliers** can dramatically affect some statistics but not others.\n\n## What You'll Calculate\n\n1. **Mean**: The arithmetic average\n2. **Median**: The middle value\n3. **Mode**: The most common value\n4. **Impact Analysis**: How the CEO's salary affects each measure\n\n## The Dataset\n\n```python\n# Regular employees (in thousands)\nsalaries = [65, 72, 68, 75, 70, 72, 80, 72, 85, 78]\n\n# With CEO included\nsalaries_with_ceo = [65, 72, 68, 75, 70, 72, 80, 72, 85, 78, 10000]\n```\n\n## Key Insight Preview\n\nYou'll discover why **median** is often called \"resistant\" or \"robust\"‚Äîit doesn't get pulled by outliers!\n\n---\n\n## üéØ Your Task\n\n1. Calculate mean, median, and mode for the regular salaries\n2. Calculate the same for salaries WITH the CEO\n3. Calculate the percentage change in each statistic\n4. Determine which measure is most \"robust\" to outliers",
    "starter_code": "import numpy as np\nfrom collections import Counter\n\n# Salaries in thousands\nregular_salaries = np.array([65, 72, 68, 75, 70, 72, 80, 72, 85, 78])\nwith_ceo = np.array([65, 72, 68, 75, 70, 72, 80, 72, 85, 78, 10000])\n\n# Calculate for regular salaries\nmean_regular = np.mean(regular_salaries)\nmedian_regular = np.median(regular_salaries)\nmode_regular = Counter(regular_salaries).most_common(1)[0][0]\n\n# Calculate for salaries with CEO\nmean_ceo = np.mean(with_ceo)\nmedian_ceo = np.median(with_ceo)\nmode_ceo = Counter(with_ceo).most_common(1)[0][0]\n\n# Calculate percentage change\nmean_change = ((mean_ceo - mean_regular) / mean_regular) * 100\nmedian_change = ((median_ceo - median_regular) / median_regular) * 100\n\nprint(\"=== Regular Salaries ===\")\nprint(f\"Mean: ${mean_regular}K\")\nprint(f\"Median: ${median_regular}K\")\nprint(f\"Mode: ${mode_regular}K\")\n\nprint(\"\\n=== With CEO ($10M) ===\")\nprint(f\"Mean: ${mean_ceo:.1f}K\")\nprint(f\"Median: ${median_ceo}K\")\nprint(f\"Mode: ${mode_ceo}K\")\n\nprint(\"\\n=== Outlier Impact ===\")\nprint(f\"Mean changed by: {mean_change:.1f}%\")\nprint(f\"Median changed by: {median_change:.1f}%\")\nprint(f\"\\nMost robust measure: {'Median' if median_change < mean_change else 'Mean'}\")",
    "solution_code": "import numpy as np\nfrom collections import Counter\n\nregular_salaries = np.array([65, 72, 68, 75, 70, 72, 80, 72, 85, 78])\nwith_ceo = np.array([65, 72, 68, 75, 70, 72, 80, 72, 85, 78, 10000])\n\nmean_regular = np.mean(regular_salaries)\nmedian_regular = np.median(regular_salaries)\nmode_regular = Counter(regular_salaries).most_common(1)[0][0]\n\nmean_ceo = np.mean(with_ceo)\nmedian_ceo = np.median(with_ceo)\nmode_ceo = Counter(with_ceo).most_common(1)[0][0]\n\nmean_change = ((mean_ceo - mean_regular) / mean_regular) * 100\nmedian_change = ((median_ceo - median_regular) / median_regular) * 100\n\nprint(\"=== Regular Salaries ===\")\nprint(f\"Mean: ${mean_regular}K\")\nprint(f\"Median: ${median_regular}K\")\nprint(f\"Mode: ${mode_regular}K\")\n\nprint(\"\\n=== With CEO ($10M) ===\")\nprint(f\"Mean: ${mean_ceo:.1f}K\")\nprint(f\"Median: ${median_ceo}K\")\nprint(f\"Mode: ${mode_ceo}K\")\n\nprint(\"\\n=== Outlier Impact ===\")\nprint(f\"Mean changed by: {mean_change:.1f}%\")\nprint(f\"Median changed by: {median_change:.1f}%\")\nprint(f\"\\nMost robust measure: Median\")",
    "expected_output": "=== Regular Salaries ===\nMean: $73.7K\nMedian: $72.0K\nMode: $72K\n\n=== With CEO ($10M) ===\nMean: $983.4K\nMedian: $72.0K\nMode: $72K\n\n=== Outlier Impact ===\nMean changed by: 1234.0%\nMedian changed by: 0.0%\n\nMost robust measure: Median"
  },
  "230": {
    "title": "Interquartile Range (IQR)",
    "chapter_title": "Statistics",
    "content": "# üìä IQR: The Outlier-Resistant Spread\n\n## What is the IQR?\n\nThe **Interquartile Range (IQR)** measures the spread of the middle 50% of your data. It's calculated as:\n\n```\nIQR = Q3 - Q1\n```\n\nWhere Q1 is the 25th percentile and Q3 is the 75th percentile.\n\n## Why IQR is Powerful\n\nUnlike range (max - min), IQR ignores extreme values! It only looks at the middle portion of your data.\n\n## Visual Representation\n\n```\nData: [1, 2, 3, 4, 5, 6, 7, 8, 9, 100]\n       ‚Üë           ‚Üë           ‚Üë\n      Q1=2.5      Q2=5.5     Q3=7.5\n      \nRange = 100 - 1 = 99 (affected by outlier!)\nIQR = 7.5 - 2.5 = 5 (ignores outlier!)\n```\n\n## Using IQR for Outlier Detection\n\nA value is considered an outlier if:\n- Below: Q1 - 1.5 √ó IQR\n- Above: Q3 + 1.5 √ó IQR\n\n## Using NumPy\n\n```python\nimport numpy as np\n\ndata = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])\nq1 = np.percentile(data, 25)\nq3 = np.percentile(data, 75)\niqr = q3 - q1\n\n# Outlier boundaries\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze house prices and identify any outliers using the IQR method.",
    "starter_code": "import numpy as np\n\n# House prices (in thousands)\nprices = np.array([250, 275, 300, 285, 295, 310, 280, 290, 305, 750, 270, 295])\n\n# Calculate Q1, Q3, and IQR\nq1 = np.percentile(prices, 25)\nq3 = np.percentile(prices, 75)\niqr = q3 - q1\n\n# Calculate outlier boundaries\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# Find outliers\noutliers = prices[(prices < lower_bound) | (prices > upper_bound)]\n\nprint(f\"Q1 (25th percentile): ${q1}K\")\nprint(f\"Q3 (75th percentile): ${q3}K\")\nprint(f\"IQR: ${iqr}K\")\nprint(f\"\\nOutlier Boundaries: ${lower_bound:.1f}K to ${upper_bound:.1f}K\")\nprint(f\"Outliers found: {outliers}\")",
    "solution_code": "import numpy as np\n\nprices = np.array([250, 275, 300, 285, 295, 310, 280, 290, 305, 750, 270, 295])\n\nq1 = np.percentile(prices, 25)\nq3 = np.percentile(prices, 75)\niqr = q3 - q1\n\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\noutliers = prices[(prices < lower_bound) | (prices > upper_bound)]\n\nprint(f\"Q1 (25th percentile): ${q1}K\")\nprint(f\"Q3 (75th percentile): ${q3}K\")\nprint(f\"IQR: ${iqr}K\")\nprint(f\"\\nOutlier Boundaries: ${lower_bound:.1f}K to ${upper_bound:.1f}K\")\nprint(f\"Outliers found: {outliers}\")",
    "expected_output": "Q1 (25th percentile): $278.75K\nQ3 (75th percentile): $301.25K\nIQR: $22.5K\n\nOutlier Boundaries: $245.0K to $335.0K\nOutliers found: [750]"
  },
  "231": {
    "title": "Outlier Detection Challenge",
    "chapter_title": "Statistics",
    "content": "# üîç Outlier Detection Challenge: Quality Control\n\n## The Scenario\n\nYou're a quality control analyst at a manufacturing plant. Products coming off the assembly line should be between 9.5cm and 10.5cm. But some defective products slip through!\n\n## Your Mission\n\nUse statistical outlier detection to automatically flag potentially defective products. You'll use BOTH methods:\n\n1. **IQR Method**: Flag values outside Q1 - 1.5√óIQR or Q3 + 1.5√óIQR\n2. **Z-Score Method**: Flag values where |Z| > 2\n\n## Why Two Methods?\n\n| Method | Best For | Limitation |\n|--------|----------|------------|\n| IQR | Non-normal data, resistant to extreme outliers | May miss outliers in small datasets |\n| Z-Score | Normal distributions, quantifies \"how unusual\" | Assumes normal distribution |\n\n## The Manufacturing Data\n\n100 product measurements with some defects mixed in:\n\n```python\n# Most are around 10cm, but some are clearly wrong!\nmeasurements = [9.8, 10.1, 9.9, 10.2, 8.5, 10.0, ...]\n```\n\n---\n\n## üéØ Your Task\n\n1. Detect outliers using the IQR method\n2. Detect outliers using the Z-score method\n3. Compare which products each method flags\n4. Determine which products should be rejected from the assembly line",
    "starter_code": "import numpy as np\n\n# 50 product measurements (cm)\nnp.random.seed(42)\nnormal_products = np.random.normal(10, 0.2, 45)  # Most around 10cm\ndefects = np.array([8.5, 11.8, 7.9, 12.1, 8.2])  # Known defects\n\nmeasurements = np.concatenate([normal_products, defects])\nnp.random.shuffle(measurements)\n\n# IQR Method\nq1 = np.percentile(measurements, 25)\nq3 = np.percentile(measurements, 75)\niqr = q3 - q1\niqr_lower = q1 - 1.5 * iqr\niqr_upper = q3 + 1.5 * iqr\niqr_outliers = measurements[(measurements < iqr_lower) | (measurements > iqr_upper)]\n\n# Z-Score Method\nmean = np.mean(measurements)\nstd = np.std(measurements)\nz_scores = (measurements - mean) / std\nzscore_outliers = measurements[np.abs(z_scores) > 2]\n\nprint(f\"=== IQR Method ===\")\nprint(f\"Bounds: {iqr_lower:.2f} to {iqr_upper:.2f}\")\nprint(f\"Outliers found: {len(iqr_outliers)}\")\nprint(f\"Values: {np.round(iqr_outliers, 2)}\")\n\nprint(f\"\\n=== Z-Score Method ===\")\nprint(f\"Mean: {mean:.2f}, Std: {std:.2f}\")\nprint(f\"Outliers found: {len(zscore_outliers)}\")\nprint(f\"Values: {np.round(zscore_outliers, 2)}\")",
    "solution_code": "import numpy as np\n\nnp.random.seed(42)\nnormal_products = np.random.normal(10, 0.2, 45)\ndefects = np.array([8.5, 11.8, 7.9, 12.1, 8.2])\nmeasurements = np.concatenate([normal_products, defects])\nnp.random.shuffle(measurements)\n\nq1 = np.percentile(measurements, 25)\nq3 = np.percentile(measurements, 75)\niqr = q3 - q1\niqr_lower = q1 - 1.5 * iqr\niqr_upper = q3 + 1.5 * iqr\niqr_outliers = measurements[(measurements < iqr_lower) | (measurements > iqr_upper)]\n\nmean = np.mean(measurements)\nstd = np.std(measurements)\nz_scores = (measurements - mean) / std\nzscore_outliers = measurements[np.abs(z_scores) > 2]\n\nprint(f\"=== IQR Method ===\")\nprint(f\"Bounds: {iqr_lower:.2f} to {iqr_upper:.2f}\")\nprint(f\"Outliers found: {len(iqr_outliers)}\")\nprint(f\"Values: {np.round(iqr_outliers, 2)}\")\n\nprint(f\"\\n=== Z-Score Method ===\")\nprint(f\"Mean: {mean:.2f}, Std: {std:.2f}\")\nprint(f\"Outliers found: {len(zscore_outliers)}\")\nprint(f\"Values: {np.round(zscore_outliers, 2)}\")",
    "expected_output": "=== IQR Method ===\nBounds: 9.47 to 10.49\nOutliers found: 5\nValues: [ 8.5  11.8  7.9  12.1  8.2 ]"
  },
  "232": {
    "title": "Covariance",
    "chapter_title": "Statistics",
    "content": "# üìâ Covariance: Do They Move Together?\n\n## What is Covariance?\n\n**Covariance** measures whether two variables tend to increase or decrease together. Unlike correlation (which is standardized), covariance is in the original units!\n\n## Positive vs Negative Covariance\n\n| Sign | Relationship | Example |\n|------|-------------|---------|\n| Positive | Both increase together | Height & Weight |\n| Negative | One increases, other decreases | Price & Demand |\n| Near Zero | No relationship | Shoe size & IQ |\n\n## The Intuition\n\nCovariance asks: \"When X is above its mean, is Y also above its mean?\"\n\n```python\n# If both are above OR both below their means: positive contribution\n# If one is above and one is below: negative contribution\n```\n\n## Using NumPy\n\n```python\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 5, 4, 5])\n\n# Covariance matrix\ncov_matrix = np.cov(x, y)\ncovariance = cov_matrix[0, 1]  # The off-diagonal element\n```\n\n## Covariance vs Correlation\n\n| Measure | Range | Interpretation |\n|---------|-------|----------------|\n| Covariance | -‚àû to +‚àû | Hard to interpret |\n| Correlation | -1 to +1 | Easy: 0.9 = strong! |\n\nCorrelation = Covariance / (std_x √ó std_y)\n\n---\n\n## üéØ Your Task\n\nCalculate the covariance between advertising spend and sales revenue. Then verify using correlation!",
    "starter_code": "import numpy as np\n\n# Monthly data\nad_spend = np.array([1000, 1500, 2000, 2500, 3000, 3500, 4000])\nsales = np.array([10000, 12000, 15000, 18000, 20000, 22000, 25000])\n\n# Calculate covariance\ncov_matrix = np.cov(ad_spend, sales)\ncovariance = cov_matrix[0, 1]\n\n# Also calculate correlation for reference\ncorrelation = np.corrcoef(ad_spend, sales)[0, 1]\n\nprint(f\"Covariance: {covariance:,.0f}\")\nprint(f\"Correlation: {correlation:.3f}\")\n\n# Interpret\nif covariance > 0:\n    print(\"\\nüìà Positive covariance: Ad spend and sales move together!\")\nelif covariance < 0:\n    print(\"\\nüìâ Negative covariance: They move in opposite directions!\")",
    "solution_code": "import numpy as np\n\nad_spend = np.array([1000, 1500, 2000, 2500, 3000, 3500, 4000])\nsales = np.array([10000, 12000, 15000, 18000, 20000, 22000, 25000])\n\ncov_matrix = np.cov(ad_spend, sales)\ncovariance = cov_matrix[0, 1]\n\ncorrelation = np.corrcoef(ad_spend, sales)[0, 1]\n\nprint(f\"Covariance: {covariance:,.0f}\")\nprint(f\"Correlation: {correlation:.3f}\")\n\nif covariance > 0:\n    print(\"\\nüìà Positive covariance: Ad spend and sales move together!\")",
    "expected_output": "Covariance: 5,833,333\nCorrelation: 0.998\n\nüìà Positive covariance: Ad spend and sales move together!"
  },
  "233": {
    "title": "Statistical Analysis Project",
    "chapter_title": "Statistics",
    "content": "# üèÜ Statistical Analysis Project: Sales Performance\n\n## The Grand Challenge\n\nYou're the head of analytics at a retail company. The CEO wants a COMPLETE statistical analysis of this quarter's sales data to present to the board. This is your moment to shine!\n\n## What the Board Needs\n\n1. **Central Tendency**: What's our \"typical\" daily sales?\n2. **Spread**: How consistent are we day-to-day?\n3. **Outliers**: Any unusually good or bad days?\n4. **Trends**: Are weekends different from weekdays?\n\n## The Dataset\n\n90 days of sales data with:\n- Regular weekday sales\n- Weekend boosts\n- Some holiday spikes\n- One Black Friday blockbuster\n\n## Your Deliverables\n\nGenerate a comprehensive report with:\n- Mean, Median, Mode\n- Standard Deviation, IQR\n- Outlier identification\n- Summary insights\n\n---\n\n## üéØ Your Task\n\nComplete the analysis and generate an executive summary that even non-technical board members can understand!",
    "starter_code": "import numpy as np\nfrom collections import Counter\n\nnp.random.seed(42)\n\n# Generate realistic sales data\nweekday_sales = np.random.normal(5000, 800, 65)  # Weekdays\nweekend_sales = np.random.normal(8000, 1200, 20)  # Weekends (higher)\nblack_friday = np.array([25000])  # One massive day\nholidays = np.random.normal(3000, 500, 4)  # Slow holiday days\n\nall_sales = np.concatenate([weekday_sales, weekend_sales, black_friday, holidays])\nnp.random.shuffle(all_sales)\n\n# === YOUR ANALYSIS ===\n\n# Central Tendency\nmean_sales = np.mean(all_sales)\nmedian_sales = np.median(all_sales)\n\n# Spread\nstd_sales = np.std(all_sales)\nq1 = np.percentile(all_sales, 25)\nq3 = np.percentile(all_sales, 75)\niqr = q3 - q1\n\n# Outliers (IQR method)\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\noutliers = all_sales[(all_sales < lower) | (all_sales > upper)]\n\n# Generate Executive Summary\nprint(\"=\" * 50)\nprint(\"üìä QUARTERLY SALES ANALYSIS - EXECUTIVE SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"\\nüìà Central Tendency:\")\nprint(f\"   ‚Ä¢ Average Daily Sales: ${mean_sales:,.0f}\")\nprint(f\"   ‚Ä¢ Median Daily Sales: ${median_sales:,.0f}\")\nprint(f\"\\nüìè Variability:\")\nprint(f\"   ‚Ä¢ Standard Deviation: ${std_sales:,.0f}\")\nprint(f\"   ‚Ä¢ Interquartile Range: ${iqr:,.0f}\")\nprint(f\"\\nüîç Outlier Analysis:\")\nprint(f\"   ‚Ä¢ {len(outliers)} unusual days detected\")\nprint(f\"   ‚Ä¢ Notable outliers: {np.round(outliers, 0)}\")",
    "solution_code": "import numpy as np\n\nnp.random.seed(42)\n\nweekday_sales = np.random.normal(5000, 800, 65)\nweekend_sales = np.random.normal(8000, 1200, 20)\nblack_friday = np.array([25000])\nholidays = np.random.normal(3000, 500, 4)\n\nall_sales = np.concatenate([weekday_sales, weekend_sales, black_friday, holidays])\n\nmean_sales = np.mean(all_sales)\nmedian_sales = np.median(all_sales)\nstd_sales = np.std(all_sales)\nq1 = np.percentile(all_sales, 25)\nq3 = np.percentile(all_sales, 75)\niqr = q3 - q1\n\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\noutliers = all_sales[(all_sales < lower) | (all_sales > upper)]\n\nprint(\"=\" * 50)\nprint(\"üìä QUARTERLY SALES ANALYSIS - EXECUTIVE SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"\\nüìà Central Tendency:\")\nprint(f\"   ‚Ä¢ Average Daily Sales: ${mean_sales:,.0f}\")\nprint(f\"   ‚Ä¢ Median Daily Sales: ${median_sales:,.0f}\")\nprint(f\"\\nüìè Variability:\")\nprint(f\"   ‚Ä¢ Standard Deviation: ${std_sales:,.0f}\")\nprint(f\"   ‚Ä¢ Interquartile Range: ${iqr:,.0f}\")\nprint(f\"\\nüîç Outlier Analysis:\")\nprint(f\"   ‚Ä¢ {len(outliers)} unusual days detected\")",
    "expected_output": "QUARTERLY SALES ANALYSIS - EXECUTIVE SUMMARY"
  },
  "234": {
    "title": "Feature Engineering Basics",
    "chapter_title": "Machine Learning Intro",
    "content": "# üîß Feature Engineering: Creating Better Inputs\n\n## What is Feature Engineering?\n\nRaw data rarely comes in the perfect format for ML. **Feature engineering** transforms raw data into features that better represent the underlying patterns!\n\n## Real-World Analogy\n\nImagine predicting house prices. The raw data might just have \"address\". But you can engineer features like:\n- Distance to downtown\n- School district rating\n- Crime rate of neighborhood\n\nThese engineered features are MORE useful than the raw address!\n\n## Common Techniques\n\n### 1. Mathematical Transformations\n```python\n# Log transform for skewed data\ndf['log_income'] = np.log(df['income'])\n\n# Square for polynomial relationships\ndf['age_squared'] = df['age'] ** 2\n```\n\n### 2. Combining Features\n```python\n# Ratios\ndf['price_per_sqft'] = df['price'] / df['sqft']\n\n# Interactions\ndf['income_x_age'] = df['income'] * df['age']\n```\n\n### 3. Extracting from Dates\n```python\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['day_of_week'] >= 5\n```\n\n### 4. Binning/Bucketing\n```python\ndf['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 55, 100])\n```\n\n---\n\n## üéØ Your Task\n\nEngineer useful features from customer data to improve churn prediction!",
    "starter_code": "import pandas as pd\nimport numpy as np\n\n# Customer data\ndf = pd.DataFrame({\n    'customer_id': range(1, 11),\n    'tenure_days': [30, 365, 730, 60, 180, 90, 450, 1095, 15, 240],\n    'monthly_charges': [50, 75, 60, 45, 85, 55, 95, 70, 40, 65],\n    'total_charges': [50, 2737, 4380, 90, 1530, 495, 4275, 7665, 20, 1560]\n})\n\n# Feature Engineering\n\n# 1. Tenure in months (more interpretable)\ndf['tenure_months'] = df['tenure_days'] / 30\n\n# 2. Average monthly spend (charges / tenure)\ndf['avg_monthly'] = df['total_charges'] / (df['tenure_days'] / 30)\n\n# 3. Customer \"value\" indicator\ndf['is_high_value'] = (df['monthly_charges'] > 70).astype(int)\n\n# 4. New customer flag\ndf['is_new'] = (df['tenure_days'] < 90).astype(int)\n\nprint(\"Original columns:\", ['customer_id', 'tenure_days', 'monthly_charges', 'total_charges'])\nprint(\"\\nNew engineered features:\")\nprint(df[['customer_id', 'tenure_months', 'avg_monthly', 'is_high_value', 'is_new']].to_string())",
    "solution_code": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'customer_id': range(1, 11),\n    'tenure_days': [30, 365, 730, 60, 180, 90, 450, 1095, 15, 240],\n    'monthly_charges': [50, 75, 60, 45, 85, 55, 95, 70, 40, 65],\n    'total_charges': [50, 2737, 4380, 90, 1530, 495, 4275, 7665, 20, 1560]\n})\n\ndf['tenure_months'] = df['tenure_days'] / 30\ndf['avg_monthly'] = df['total_charges'] / (df['tenure_days'] / 30)\ndf['is_high_value'] = (df['monthly_charges'] > 70).astype(int)\ndf['is_new'] = (df['tenure_days'] < 90).astype(int)\n\nprint(\"Original columns:\", ['customer_id', 'tenure_days', 'monthly_charges', 'total_charges'])\nprint(\"\\nNew engineered features:\")\nprint(df[['customer_id', 'tenure_months', 'avg_monthly', 'is_high_value', 'is_new']].to_string())",
    "expected_output": "New engineered features:"
  },
  "235": {
    "title": "Data Preprocessing Pipeline",
    "chapter_title": "Machine Learning Intro",
    "content": "# üîÑ Data Preprocessing Pipeline: Organized ML Prep\n\n## The Problem with Ad-Hoc Preprocessing\n\nWhen you preprocess training data one way and test data another way, your model fails! A **pipeline** ensures consistent transformations.\n\n## What is a Pipeline?\n\nA Pipeline chains multiple preprocessing steps into a single object that:\n1. Fits on training data\n2. Transforms both train and test data identically\n3. Prevents data leakage\n\n## Visual Flow\n\n```\nRaw Data ‚Üí [Scale] ‚Üí [Encode] ‚Üí [Select Features] ‚Üí ML Model\n           \\_____Pipeline chains these together____/\n```\n\n## Using sklearn Pipeline\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', SVC())\n])\n\n# Now one line does everything!\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n```\n\n## Why Pipelines Matter\n\n‚úÖ No data leakage (test data never influences scaling)\n‚úÖ Reproducible (same preprocessing every time)\n‚úÖ Cleaner code (one object instead of many steps)\n‚úÖ Easy deployment (save one pipeline object)\n\n---\n\n## üéØ Your Task\n\nBuild a preprocessing pipeline that scales features and trains a KNN classifier!",
    "starter_code": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(\n    wine.data, wine.target, test_size=0.2, random_state=42\n)\n\n# Create pipeline: Scale then classify\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=5))\n])\n\n# One line does it all!\npipeline.fit(X_train, y_train)\n\n# Evaluate\naccuracy = pipeline.score(X_test, y_test)\nprint(f\"Pipeline Accuracy: {accuracy:.1%}\")",
    "solution_code": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(\n    wine.data, wine.target, test_size=0.2, random_state=42\n)\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=5))\n])\n\npipeline.fit(X_train, y_train)\naccuracy = pipeline.score(X_test, y_test)\nprint(f\"Pipeline Accuracy: {accuracy:.1%}\")",
    "expected_output": "Pipeline Accuracy: 97.2%"
  },
  "236": {
    "title": "Training Your First Model",
    "chapter_title": "Machine Learning Intro",
    "content": "# üöÄ Training Your First Model: Hands-On Linear Regression\n\n## The Moment of Truth!\n\nIt's time to train your first real machine learning model. We'll predict a continuous value (regression) using linear regression.\n\n## The 4-Step ML Workflow\n\n```python\n# 1. Prepare Data\nX_train, X_test, y_train, y_test = train_test_split(...)\n\n# 2. Create Model\nmodel = LinearRegression()\n\n# 3. Train (Fit)\nmodel.fit(X_train, y_train)\n\n# 4. Predict & Evaluate\npredictions = model.predict(X_test)\n```\n\n## What Happens During Training?\n\nThe model finds the best line through your data by minimizing error:\n\n```\nError = Actual - Predicted\nGoal: Make total error as small as possible!\n```\n\n## After Training\n\nThe model learns:\n- **Coefficients (slopes)**: How much output changes per unit of input\n- **Intercept**: The baseline value when inputs are zero\n\n```python\nprint(model.coef_)      # [2.5] - Output increases 2.5 for each unit of input\nprint(model.intercept_)  # 10 - Starting point\n```\n\n---\n\n## üéØ Your Task\n\nTrain a linear regression model to predict study hours ‚Üí exam scores, then interpret the coefficients!",
    "starter_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Study hours and corresponding exam scores\nnp.random.seed(42)\nhours = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\nscores = 10 * hours.flatten() + 40 + np.random.randn(10) * 3\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    hours, scores, test_size=0.3, random_state=42\n)\n\n# Create and train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate\nscore = model.score(X_test, y_test)\n\nprint(\"üéì Study Hours ‚Üí Exam Score Model\")\nprint(f\"\\nModel Equation: Score = {model.coef_[0]:.1f} √ó Hours + {model.intercept_:.1f}\")\nprint(f\"\\nInterpretation: Each hour of study = {model.coef_[0]:.1f} more points!\")\nprint(f\"R¬≤ Score: {score:.3f}\")",
    "solution_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nnp.random.seed(42)\nhours = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\nscores = 10 * hours.flatten() + 40 + np.random.randn(10) * 3\n\nX_train, X_test, y_train, y_test = train_test_split(hours, scores, test_size=0.3, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nscore = model.score(X_test, y_test)\n\nprint(\"üéì Study Hours ‚Üí Exam Score Model\")\nprint(f\"\\nModel Equation: Score = {model.coef_[0]:.1f} √ó Hours + {model.intercept_:.1f}\")\nprint(f\"\\nInterpretation: Each hour of study = {model.coef_[0]:.1f} more points!\")\nprint(f\"R¬≤ Score: {score:.3f}\")",
    "expected_output": "üéì Study Hours ‚Üí Exam Score Model\n\nModel Equation: Score = 10.2 √ó Hours + 39.2\n\nInterpretation: Each hour of study = 10.2 more points!"
  },
  "237": {
    "title": "Evaluating Regression Models",
    "chapter_title": "Machine Learning Intro",
    "content": "# üìè Evaluating Regression Models: Beyond R¬≤\n\n## R¬≤ Isn't Everything\n\nR¬≤ (coefficient of determination) is popular but has limitations. Let's explore multiple evaluation metrics!\n\n## Common Metrics\n\n### 1. R¬≤ Score (Coefficient of Determination)\n- How much variance is explained?\n- Range: 0 to 1 (higher is better)\n- Limitation: Can be misleading with non-linear data\n\n### 2. Mean Absolute Error (MAE)\n```python\nMAE = mean(|actual - predicted|)\n```\n- Average absolute difference\n- Same units as your target\n- Easy to interpret!\n\n### 3. Mean Squared Error (MSE)\n```python\nMSE = mean((actual - predicted)¬≤)\n```\n- Penalizes large errors more\n- In squared units (harder to interpret)\n\n### 4. Root Mean Squared Error (RMSE)\n```python\nRMSE = ‚àöMSE\n```\n- Back to original units\n- Standard measure in competitions\n\n## Which to Use?\n\n| Metric | When to Use |\n|--------|------------|\n| R¬≤ | Quick sanity check |\n| MAE | When all errors matter equally |\n| RMSE | When large errors are really bad |\n\n---\n\n## üéØ Your Task\n\nCompare multiple evaluation metrics for a house price prediction model!",
    "starter_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# House data: sqft ‚Üí price\nnp.random.seed(42)\nsqft = np.random.randint(800, 3000, 100).reshape(-1, 1)\nprice = 150 * sqft.flatten() + 50000 + np.random.randn(100) * 20000\n\nX_train, X_test, y_train, y_test = train_test_split(sqft, price, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n# Calculate all metrics\nr2 = r2_score(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nmse = mean_squared_error(y_test, predictions)\nrmse = np.sqrt(mse)\n\nprint(\"üè† House Price Prediction Metrics\")\nprint(\"=\" * 40)\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"MAE: ${mae:,.0f}\")\nprint(f\"MSE: {mse:,.0f}\")\nprint(f\"RMSE: ${rmse:,.0f}\")\nprint(\"\\nüí° Interpretation:\")\nprint(f\"   On average, predictions are off by ${mae:,.0f}\")",
    "solution_code": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport numpy as np\n\nnp.random.seed(42)\nsqft = np.random.randint(800, 3000, 100).reshape(-1, 1)\nprice = 150 * sqft.flatten() + 50000 + np.random.randn(100) * 20000\n\nX_train, X_test, y_train, y_test = train_test_split(sqft, price, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\nr2 = r2_score(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nmse = mean_squared_error(y_test, predictions)\nrmse = np.sqrt(mse)\n\nprint(\"üè† House Price Prediction Metrics\")\nprint(\"=\" * 40)\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"MAE: ${mae:,.0f}\")\nprint(f\"MSE: {mse:,.0f}\")\nprint(f\"RMSE: ${rmse:,.0f}\")",
    "expected_output": "üè† House Price Prediction Metrics"
  },
  "240": {
    "title": "Precision vs Recall",
    "chapter_title": "Machine Learning Intro",
    "content": "# ‚öñÔ∏è Precision vs Recall: The Tradeoff\n\n## Why Accuracy Isn't Enough\n\nImagine a disease affects 1% of people. A model that ALWAYS predicts \"healthy\" gets 99% accuracy‚Äîbut misses every sick person!\n\n## Precision: \"When I predict positive, am I right?\"\n\n```\nPrecision = True Positives / All Predicted Positives\n```\n\nHigh precision = Few false alarms\n\n## Recall: \"Of all positives, how many did I find?\"\n\n```\nRecall = True Positives / All Actual Positives\n```\n\nHigh recall = Find most positive cases\n\n## The Tradeoff\n\n| Situation | Prioritize | Why |\n|-----------|-----------|-----|\n| Spam filter | Precision | Don't want real emails in spam! |\n| Cancer screening | Recall | Don't want to miss any cases! |\n| Fraud detection | Balance | Both matter |\n\n## The F1 Score\n\nBalances precision and recall:\n```python\nF1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n```\n\n---\n\n## üéØ Your Task\n\nAnalyze a fraud detection model and understand the precision-recall tradeoff!",
    "starter_code": "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nimport numpy as np\n\n# Simulated fraud detection results\n# 1 = Fraud, 0 = Legitimate\nnp.random.seed(42)\ny_true = np.array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\ny_pred = np.array([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n\n# Calculate metrics\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\n# Confusion matrix breakdown\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\nprint(\"üîç Fraud Detection Model Analysis\")\nprint(\"=\" * 45)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"   True Negatives (correct non-fraud): {tn}\")\nprint(f\"   False Positives (false alarms): {fp}\")\nprint(f\"   False Negatives (missed fraud): {fn}\")\nprint(f\"   True Positives (caught fraud): {tp}\")\nprint(f\"\\nüìä Performance Metrics:\")\nprint(f\"   Precision: {precision:.1%} (When we flag fraud, we're right this often)\")\nprint(f\"   Recall: {recall:.1%} (We catch this much of actual fraud)\")\nprint(f\"   F1 Score: {f1:.1%} (Balanced measure)\")",
    "solution_code": "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nimport numpy as np\n\nnp.random.seed(42)\ny_true = np.array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\ny_pred = np.array([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\nprint(\"üîç Fraud Detection Model Analysis\")\nprint(\"=\" * 45)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"   True Negatives: {tn}\")\nprint(f\"   False Positives: {fp}\")\nprint(f\"   False Negatives: {fn}\")\nprint(f\"   True Positives: {tp}\")\nprint(f\"\\nüìä Performance Metrics:\")\nprint(f\"   Precision: {precision:.1%}\")\nprint(f\"   Recall: {recall:.1%}\")\nprint(f\"   F1 Score: {f1:.1%}\")",
    "expected_output": "üîç Fraud Detection Model Analysis"
  },
  "242": {
    "title": "Precision vs Recall",
    "chapter_title": "Machine Learning Intro",
    "content": "# ‚öñÔ∏è Precision vs Recall: The ML Tradeoff\n\n## The Critical Tradeoff\n\nIn classification, you can't maximize both precision and recall:\n- **High precision** = fewer false positives\n- **High recall** = fewer false negatives\n\n## Definitions\n\n```\nPrecision = TP / (TP + FP)\n\"Of those I predicted positive, how many were actually positive?\"\n\nRecall = TP / (TP + FN)\n\"Of all actual positives, how many did I find?\"\n```\n\n## Real-World Example: Spam Filter\n\n| Scenario | Precision Focus | Recall Focus |\n|----------|----------------|--------------|\n| Goal | Don't mark real emails as spam | Catch ALL spam |\n| Risk | Some spam gets through | Important emails in spam folder |\n| FP vs FN | Fewer FP, more FN | Fewer FN, more FP |\n\n## Choosing Your Priority\n\n| Problem | Prioritize | Why |\n|---------|-----------|-----|\n| Medical screening | Recall | Don't miss sick patients |\n| Spam filter | Precision | Don't lose real emails |\n| Fraud detection | Recall | Don't miss fraud cases |\n| Search results | Precision | Show relevant results |\n\n## The F1 Score: Balancing Both\n\n```python\nF1 = 2 * (precision * recall) / (precision + recall)\n```\n\nF1 is the harmonic mean‚Äîgood when you need both!\n\n## Sklearn Implementation\n\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate precision, recall, and F1 score for a classifier.",
    "starter_code": "# Simulated classification results\ny_true = [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\ny_pred = [1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n\n# Calculate manually\nTP = sum(t == 1 and p == 1 for t, p in zip(y_true, y_pred))\nFP = sum(t == 0 and p == 1 for t, p in zip(y_true, y_pred))\nFN = sum(t == 1 and p == 0 for t, p in zip(y_true, y_pred))\n\nprecision = TP / (TP + FP) if (TP + FP) > 0 else 0\nrecall = TP / (TP + FN) if (TP + FN) > 0 else 0\nf1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\nprint(\"Classification Metrics:\")\nprint(f\"  True Positives: {TP}\")\nprint(f\"  False Positives: {FP}\")\nprint(f\"  False Negatives: {FN}\")\nprint(f\"  Precision: {precision:.2f}\")\nprint(f\"  Recall: {recall:.2f}\")\nprint(f\"  F1 Score: {f1:.2f}\")",
    "solution_code": "y_true = [1, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 1, 1]\n\nTP = sum(t == 1 and p == 1 for t, p in zip(y_true, y_pred))\nFP = sum(t == 0 and p == 1 for t, p in zip(y_true, y_pred))\nFN = sum(t == 1 and p == 0 for t, p in zip(y_true, y_pred))\n\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nprint(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")",
    "expected_output": "Classification Metrics:\n  Precision: 0.80\n  Recall: 0.67\n  F1 Score: 0.73"
  },
  "241": {
    "title": "Evaluating Regression Models",
    "chapter_title": "Machine Learning Intro",
    "content": "# üìê Regression Metrics: Measuring Prediction Quality\n\n## Why Metrics Matter\n\nKnowing your model is \"good\" isn't enough. You need to quantify HOW good:\n- Compare different models objectively\n- Know if improvements are significant\n- Communicate results to stakeholders\n\n## Key Regression Metrics\n\n### 1. Mean Absolute Error (MAE)\nAverage of absolute differences. Easy to interpret!\n\n```python\nMAE = mean(|y_true - y_pred|)\n# \"On average, predictions are off by $X\"\n```\n\n### 2. Mean Squared Error (MSE)\nSquares errors, so big mistakes hurt more.\n\n```python\nMSE = mean((y_true - y_pred)¬≤)\n```\n\n### 3. Root Mean Squared Error (RMSE)\nSquare root of MSE‚Äîsame units as target.\n\n```python\nRMSE = sqrt(MSE)\n# \"On average, predictions are within $X\"\n```\n\n### 4. R¬≤ Score\nHow much variance your model explains (0-1).\n\n```python\nR¬≤ = 1 - (SS_residual / SS_total)\n# 0.85 = model explains 85% of variance\n```\n\n## When to Use Which\n\n| Metric | Use When |\n|--------|----------|\n| MAE | All errors equally bad |\n| MSE/RMSE | Big errors are much worse |\n| R¬≤ | Explaining to non-technical stakeholders |\n\n## Sklearn Implementation\n\n```python\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nmae = mean_absolute_error(y_true, y_pred)\nmse = mean_squared_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate MAE, RMSE, and R¬≤ for house price predictions.",
    "starter_code": "import numpy as np\n\n# Actual vs predicted house prices (in $1000s)\ny_true = np.array([200, 300, 400, 500, 600])\ny_pred = np.array([210, 280, 420, 480, 590])\n\n# Calculate metrics\nmae = np.mean(np.abs(y_true - y_pred))\nmse = np.mean((y_true - y_pred) ** 2)\nrmse = np.sqrt(mse)\n\nss_res = np.sum((y_true - y_pred) ** 2)\nss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\nr2 = 1 - (ss_res / ss_tot)\n\nprint(\"Regression Metrics:\")\nprint(f\"  MAE: ${mae:.2f}K\")\nprint(f\"  RMSE: ${rmse:.2f}K\")\nprint(f\"  R¬≤ Score: {r2:.3f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  Predictions are off by ${mae:.0f}K on average\")\nprint(f\"  Model explains {r2*100:.1f}% of price variance\")",
    "solution_code": "import numpy as np\n\ny_true = np.array([200, 300, 400, 500])\ny_pred = np.array([210, 280, 420, 480])\n\nmae = np.mean(np.abs(y_true - y_pred))\nrmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\nprint(f\"MAE: ${mae:.2f}K, RMSE: ${rmse:.2f}K\")",
    "expected_output": "Regression Metrics:\n  MAE: $18.00K\n  RMSE: $20.00K\n  R¬≤ Score: 0.992"
  },
  "2001": {
    "id": 2001,
    "title": "Meet the Penguins",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# üêß Meet the Penguins!\n\n## Why Visualize Data?\n\n> \"A picture is worth a thousand numbers.\"\n\nImagine you have a list of 344 penguin measurements. Reading 344 rows is boring and confusing! But a **graph** can show you the story instantly.\n\n## Our Data: The `penguins` Dataset\n\nWe'll use data about penguins from Antarctica. Think of it like a giant spreadsheet:\n\n| species | island | flipper_length_mm | body_mass_g |\n| ------- | ------ | ----------------- | ----------- |\n| Adelie | Torgersen | 181 | 3750 |\n| Gentoo | Biscoe | 217 | 5000 |\n| ... | ... | ... | ... |\n\n- **344 rows**: Each row is one penguin.\n- **8 columns**: Each column is a measurement (like flipper length or body mass).\n\n## Our Big Question\n\n> Do penguins with longer flippers also weigh more?\n\nTo answer this, we'll draw a graph. Let's learn how!\n\n![Goal: Scatterplot](/assets/r-plots/penguin_scatter_trend.png)\n\n---\n\n## üéØ Your Task\n\nFirst, let's just *look* at our data. Type `penguins` and press Run.\n\nLook for the columns `flipper_length_mm` and `body_mass_g`‚Äîwe'll use them next!\n",
    "starter_code": "# Type the name of the dataset to see it\npenguins\n",
    "solution_code": "penguins",
    "expected_output": "[Tibble Preview: 344 x 8]",
    "chapter_id": 1
  },
  "2002": {
    "id": 2002,
    "title": "The Empty Canvas",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# üé® The Empty Canvas\n\n## How ggplot2 Works: Like Painting!\n\nMaking a graph in R is like painting a picture:\n\n1.  **Canvas**: You start with a blank canvas (`ggplot()`).\n2.  **Brushstrokes**: You add layers of paint (dots, lines, bars).\n\n## Step 1: Create the Canvas\n\nThe `ggplot()` function creates your empty canvas. You tell it *which data* to use:\n\n```r\nggplot(data = your_dataset)\n```\n\nThis creates... a gray box! That's our blank canvas.\n\n![Empty Canvas](/assets/r-plots/ggplot_empty.png)\n\nIt's empty because we haven't painted anything on it yet.\n\n## The `data` Argument\n\n`data = penguins` tells R: \"Hey, I want to paint a picture using the `penguins` spreadsheet.\"\n\n---\n\n## üéØ Your Task\n\nCreate an empty canvas using the `penguins` dataset.\n\n- Use the `ggplot()` function\n- Set `data = penguins`\n\n> **Hint**: Replace the `...` with `penguins`.\n",
    "starter_code": "# Create your canvas\nggplot(data = ...)\n",
    "solution_code": "ggplot(data = penguins)",
    "expected_output": "[Graph: Empty Gray Canvas]",
    "chapter_id": 1
  },
  "2003": {
    "id": 2003,
    "title": "Mapping Data to Axes",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# üìç Mapping Data to Axes\n\n## Step 2: Tell R *Where* to Put Things\n\nWe have a canvas. Now we need to tell R two things:\n- What goes on the **x-axis** (left-to-right)?\n- What goes on the **y-axis** (up-and-down)?\n\nWe use `mapping = aes(...)` for this. `aes` stands for **aesthetics** (visual properties).\n\n## The Pattern\n\n```r\nggplot(\n  data = penguins,\n  mapping = aes(x = COLUMN_NAME, y = COLUMN_NAME)\n)\n```\n\nThis says:\n- Canvas uses `penguins` data.\n- The **x-axis** shows the column you pick.\n- The **y-axis** shows another column.\n\nNow our canvas has labels! But still no dots...\n\n![Canvas with Axes](/assets/r-plots/ggplot_axes.png)\n\nbecause we haven't added any 'paint' yet.\n\n---\n\n## üéØ Your Task\n\nMap the axes using the `penguins` dataset:\n- Set **x** to `flipper_length_mm` (flipper length in millimeters)\n- Set **y** to `body_mass_g` (body mass in grams)\n\n> **Hint**: Replace the `...` placeholders with the column names above.\n",
    "starter_code": "ggplot(\n  data = penguins,\n  mapping = aes(x = ..., y = ...)\n)\n",
    "solution_code": "ggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)",
    "expected_output": "[Graph: Canvas with Axes Labels]",
    "chapter_id": 1
  },
  "2004": {
    "id": 2004,
    "title": "Adding Dots with geom_point()",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# ‚≠ï Adding Dots with `geom_point()`\n\n## Step 3: Paint the Data!\n\nWe have a canvas with axes. Now let's paint some dots on it!\n\n![Scatterplot](/assets/r-plots/penguin_scatter.png)\n\nIn ggplot2, different shapes are called **geoms** (short for \"geometries\"):\n- `geom_point()` = dots\n- `geom_bar()` = bars\n- `geom_line()` = lines\n\n## The `+` Operator\n\nTo add a layer, you use a `+`. Think of it as \"and then add...\"\n\n```r\nggplot(...) +\n  geom_point()\n```\n\n**Important**: The `+` **MUST** go at the *end* of a line, not the beginning!\n\n---\n\n## üéØ Your Task\n\nAdd a layer of dots to your plot by replacing `...` with `geom_point()`.\n\nThis will draw one dot for each penguin, using the x and y axes we already set up.\n\n> **Hint**: The geom function that draws dots is called `geom_point()`.\n",
    "starter_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  ...\n",
    "solution_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()",
    "expected_output": "[Graph: Scatterplot of Penguins!]",
    "chapter_id": 1
  },
  "2005": {
    "id": 2005,
    "title": "Coloring by Species",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# üåà Coloring by Species\n\n## Adding More Information\n\nWe can see a pattern: bigger flippers = heavier penguin. But does this hold for ALL species?\n\nLet's color each dot by the penguin's species. The `penguins` dataset has 3 species:\n- Adelie\n- Chinstrap\n- Gentoo\n\n## The `color` Aesthetic\n\nTo color dots by a variable, add `color = column_name` *inside* `aes()`.\n\n```r\naes(x = ..., y = ..., color = species)\n```\n\nNow each species gets its own color, and R automatically adds a legend!\n\n![Colored Scatterplot](/assets/r-plots/penguin_scatter_colored.png)\n\n---\n\n## üéØ Your Task\n\nAdd color to distinguish the three penguin species.\n\n- Inside `aes()`, set `color` equal to the `species` column.\n\n> **Hint**: Replace `...` with `species`.\n",
    "starter_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = ...)) +\n  geom_point()\n",
    "solution_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point()",
    "expected_output": "[Graph: Colored Scatterplot]",
    "chapter_id": 1
  },
  "2006": {
    "id": 2006,
    "title": "Adding a Trend Line",
    "chapter_title": "Ch 1: Data Visualization",
    "content": "# üìà Adding a Trend Line\n\n## Seeing the Pattern Clearly\n\nThe dots show a pattern, but a **trend line** makes it crystal clear.\n\n![Trend Line](/assets/r-plots/penguin_scatter_trend.png)\n\nWe use `geom_smooth()` to add a smooth line.\n\n## Layering Geoms\n\nJust like adding more brushstrokes to a painting, you can add multiple layers with `+`:\n\n```r\nggplot(...) +\n  geom_point() +\n  geom_smooth()\n```\n\n## The `method` Argument\n\n`geom_smooth()` can draw different types of lines. Use `method = \"lm\"` for a straight \"best fit\" line (Linear Model).\n\n---\n\n## üéØ Your Task\n\nAdd a linear trend line to your scatterplot.\n\n- Replace `...` with `lm` (short for \"linear model\").\n\n> **Hint**: `lm` should be in quotes: `\"lm\"`.\n",
    "starter_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"...\")\n",
    "solution_code": "ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")",
    "expected_output": "[Graph: Scatterplot with Trend Line]",
    "chapter_id": 1
  },
  "2010": {
    "id": 2010,
    "title": "Coding Basics",
    "chapter_title": "Ch 2: Workflow: Basics",
    "content": "# üßÆ Coding Basics\n\n## Your First Steps in R\n\nYou've already made some plots! But let's slow down and build a solid foundation. Don't worry if R feels frustrating at first‚Äîit's picky about punctuation, and everyone gets errors. The only way through is practice!\n\n---\n\n## R is a Fancy Calculator\n\nAt its heart, R is just a calculator. Try typing math directly:\n\n```r\n1 / 200 * 30      #> 0.15\n(59 + 73 + 2) / 3 #> 44.66667\nsin(pi / 2)       #> 1\n```\n\nR follows normal math rules (PEMDAS: Parentheses, Exponents, Multiplication/Division, Addition/Subtraction).\n\n---\n\n## Saving Values: The Assignment Operator `<-`\n\nWhat if you want to *save* a result for later? You use the **assignment operator** `<-`.\n\nThink of it like putting something in a labeled box:\n\n```\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ  x <- 3 * 4 ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚Üì\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ  BOX 'x'    ‚îÇ\n   ‚îÇ  Contains:  ‚îÇ\n   ‚îÇ     12      ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n```r\nx <- 3 * 4   # Create a box named 'x', put 12 inside\nx            # Open the box to see what's inside ‚Üí 12\n```\n\nWhen you read `x <- 3 * 4`, say in your head: **\"x gets the value 12\"**.\n\n> **RStudio Shortcut**: `Alt + -` (Windows) or `Option + -` (Mac) types `<-` for you!\n\n---\n\n## Vectors: Storing Multiple Values\n\nA **vector** is like a row of boxes glued together. Create one with `c()` (combine):\n\n```r\nprimes <- c(2, 3, 5, 7, 11, 13)\n```\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 2 ‚îÇ 3 ‚îÇ 5 ‚îÇ 7 ‚îÇ11 ‚îÇ13 ‚îÇ  ‚Üê 'primes' vector\n‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n```\n\nMath on vectors applies to **every element**:\n\n```r\nprimes * 2   #> 4  6 10 14 22 26\nprimes - 1   #> 1  2  4  6 10 12\n```\n\n---\n\n## üéØ Your Task\n\n1. Create a variable `my_numbers` containing the values `5, 10, 15, 20`.\n2. Multiply `my_numbers` by `3`.\n",
    "starter_code": "# Step 1: Create a vector with c()\nmy_numbers <- c(...)\n\n# Step 2: Multiply by 3\nmy_numbers * ...\n",
    "solution_code": "my_numbers <- c(5, 10, 15, 20)\nmy_numbers * 3",
    "expected_output": "15 30 45 60",
    "chapter_id": 2
  },
  "2011": {
    "id": 2011,
    "title": "Names & Comments",
    "chapter_title": "Ch 2: Workflow: Basics",
    "content": "# üè∑Ô∏è Names & Comments\n\n## Naming Your Variables\n\nProgramming is hard. Naming things is *harder*. Here are R's rules:\n\n| Rule | Example |\n|------|---------|\n| Must start with a letter | ‚úÖ `my_var`, ‚ùå `1var` |\n| Can contain letters, numbers, `_`, `.` | ‚úÖ `my_var2`, ‚úÖ `my.var` |\n| **Case matters!** | `r_rocks` ‚â† `R_rocks` |\n\n---\n\n## Convention: Use `snake_case`\n\nWe use **snake_case**: all lowercase, words separated by underscores.\n\n```\n‚úÖ i_use_snake_case       ‚Üê We use this!\n‚ùå iUseCamelCase          ‚Üê Some languages use this\n‚ùå some.people.use.periods‚Üê Confusing in R\n‚ùå AND_aFew.RENOUNCE_convention ‚Üê Chaos\n```\n\nGood names are **descriptive**: `penguin_weight` is better than `x`.\n\n---\n\n## The Most Common Error\n\nYou *will* see this error message. Everyone does:\n\n```\nError: object 'my_varƒ±able' not found\n```\n\nThis almost always means **typo** or **wrong case**. R can't read your mind!\n\n```r\nr_rocks <- 2^3\nr_rock   # Error! You typed 'r_rock' not 'r_rocks'\nR_rocks  # Error! R is case-sensitive\n```\n\n---\n\n## Comments: Explain *Why*, Not *What*\n\nR ignores anything after `#`. Use comments to explain your **reasoning**, not what the code literally does (that's obvious from reading it).\n\n```r\n# BAD comment (states the obvious)\nx <- 5  # assigns 5 to x\n\n# GOOD comment (explains why)\nspan <- 0.9  # Increased from 0.75 for smoother curve\n```\n\n---\n\n## üéØ Your Task\n\nThe code below has **two typos**. Find and fix them in the editor so the calculation works!\n\n```r\nthis_is_a_really_long_name <- 2.5\nr_rocks <- 2^3\n\n# Fix both typos below:\nthis_is_a_really_long_name + r_rock\n```\n\n> **Hint**: Look very carefully at the spelling and case.\n",
    "starter_code": "this_is_a_really_long_name <- 2.5\nr_rocks <- 2^3\n\n# Fix both typos below:\nthis_is_a_really_long_name + r_rock\n",
    "solution_code": "this_is_a_really_long_name <- 2.5\nr_rocks <- 2^3\nthis_is_a_really_long_name + r_rocks",
    "expected_output": "10.5",
    "chapter_id": 2
  },
  "2012": {
    "id": 2012,
    "title": "Calling Functions",
    "chapter_title": "Ch 2: Workflow: Basics",
    "content": "# üìû Calling Functions\n\n## What is a Function?\n\nA **function** is like a kitchen appliance. You put ingredients in, it does something, and you get a result out.\n\n```\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ      BLENDER          ‚îÇ\n   ‚îÇ   (the function)      ‚îÇ\n   ‚îÇ                       ‚îÇ\n   ‚îÇ  Fruit, Ice, Milk     ‚îÇ  ‚Üê Inputs (arguments)\n   ‚îÇ        ‚Üì              ‚îÇ\n   ‚îÇ   üîÑ BLEND üîÑ         ‚îÇ  ‚Üê The function does work\n   ‚îÇ        ‚Üì              ‚îÇ\n   ‚îÇ    Smoothie!          ‚îÇ  ‚Üê Output (return value)\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\nIn R, the pattern is:\n\n```r\nfunction_name(argument1 = value1, argument2 = value2)\n```\n\n---\n\n## Example: `seq()`\n\n`seq()` creates a **sequence** of numbers:\n\n```r\nseq(from = 1, to = 10)\n#> [1]  1  2  3  4  5  6  7  8  9 10\n```\n\nThe `from` and `to` are **arguments** (the ingredients). You can skip the names if you use the correct order:\n\n```r\nseq(1, 10)   # Same result!\n```\n\n---\n\n## Paired Symbols: `()` `\"\"` `''`\n\nParentheses and quotes **must come in pairs**. If you forget one, R shows a `+` meaning \"I'm waiting for more\":\n\n```r\n> x <- \"hello\n+\n```\n\nThis means you forgot the closing `\"`. Press **Esc** to cancel and try again.\n\n---\n\n## Getting Help\n\nNot sure what a function does? Use `?function_name`:\n\n```r\n?seq   # Opens help page for seq()\n```\n\nOr just Google \"R seq function\"!\n\n---\n\n## üéØ Your Task\n\n1. Use `seq()` to create a sequence from `1` to `20`.\n2. Store it in a variable called `my_sequence`.\n3. Use `mean()` to calculate the average of `my_sequence`.\n",
    "starter_code": "# Step 1 & 2: Create a sequence from 1 to 20\nmy_sequence <- seq(from = ..., to = ...)\n\n# Step 3: Calculate the mean\nmean(...)\n",
    "solution_code": "my_sequence <- seq(from = 1, to = 20)\nmean(my_sequence)",
    "expected_output": "10.5",
    "chapter_id": 2
  },
  "2020": {
    "id": 2020,
    "title": "Filter Rows",
    "chapter_title": "Ch 3: Data Transformation",
    "content": "# üåä Filter Rows with `filter()`\n\n## Introducing dplyr\n\n`dplyr` is the grammar of data manipulation. We'll use the `flights` dataset‚Äî336,776 flights from NYC in 2013!\n\n```\n| year | month | day | dep_delay | arr_delay | carrier | dest |\n|------|-------|-----|-----------|-----------|---------|------|\n| 2013 |     1 |   1 |         2 |        11 | UA      | IAH  |\n| 2013 |     1 |   1 |         4 |        20 | UA      | IAH  |\n| ...  |   ... | ... |       ... |       ... | ...     | ...  |\n```\n\n## What `filter()` Does\n\n`filter()` keeps rows that match a condition. Think of it as \"keep only the rows where...\".\n\n```r\nflights |> filter(dep_delay > 120)  # Flights delayed 2+ hours\n```\n\n## The Pipe `|>`\n\nRead `|>` as **\"then\"**: \"Take flights, *then* filter.\".\n\n## Comparison Operators\n\n| Operator | Meaning |\n|----------|---------|\n| `==`     | Equal (NOT `=`!) |\n| `!=`     | Not equal |\n| `>`, `>=`| Greater than (or equal) |\n| `<`, `<=`| Less than (or equal) |\n\n## Combining Conditions\n\n- **AND**: Use `,` or `&` ‚Üí `filter(column1 == value, column2 == value)`\n- **OR**: Use `|` ‚Üí `filter(column == value1 | column == value2)`\n- **Shortcut**: Use `%in%` ‚Üí `filter(column %in% c(value1, value2))`\n\n---\n\n## üéØ Your Task\n\nFind all flights that departed on **January 1st**.\n\n- Filter where `month` equals `1` AND `day` equals `1`.\n\n> **Hint**: Use `==` for equality (not `=`). Separate conditions with a comma.\n",
    "starter_code": "# Filter for flights on January 1st\nflights |> filter(month == ..., day == ...)\n",
    "solution_code": "flights |> filter(month == 1, day == 1)",
    "expected_output": "[Tibble: 842 x 19]",
    "chapter_id": 3
  },
  "2021": {
    "id": 2021,
    "title": "Arrange Rows",
    "chapter_title": "Ch 3: Data Transformation",
    "content": "# üóÇÔ∏è Arrange Rows with `arrange()`\n\n## Sorting Data\n\n`arrange()` reorders rows. It's like \"Sort\" in Excel, but more powerful.\n\n```r\nflights |> arrange(year, month, day, dep_time)\n```\n\nThis sorts by year first, then month, then day, then departure time.\n\n## Descending Order\n\nWrap a column in `desc()` to sort big ‚Üí small:\n\n```r\nflights |> arrange(desc(column_name))\n```\n\nExample output when sorting by `dep_delay` descending:\n```\n| year | month | day | dep_delay |\n|------|-------|-----|-----------|\n| 2013 |     1 |   9 |      1301 |  ‚Üê 21+ hours late!\n| 2013 |     6 |  15 |      1137 |\n| ...  |   ... | ... |       ... |\n```\n\n## Key Point\n\n`arrange()` changes **order**, not **content**. You still have all 336,776 rows!\n\n---\n\n## üéØ Your Task\n\nSort `flights` to find the **most delayed** flights.\n\n- Sort by `dep_delay` in **descending** order (highest first).\n\n> **Hint**: Use `desc(dep_delay)` to sort from highest to lowest delay.\n",
    "starter_code": "# Sort by departure delay, descending\nflights |> arrange(...)\n",
    "solution_code": "flights |> arrange(desc(dep_delay))",
    "expected_output": "[Tibble: Most delayed flights first]",
    "chapter_id": 3
  },
  "2022": {
    "id": 2022,
    "title": "Select Columns",
    "chapter_title": "Ch 3: Data Transformation",
    "content": "# ‚úÇÔ∏è Select Columns with `select()`\n\n## Focusing on What Matters\n\nThe `flights` dataset has 19 columns! `select()` lets you zoom in on just the ones you need.\n\n```r\nflights |> select(year, month, day)\n```\n\n## Selection Tricks\n\n| Syntax | What It Does |\n|--------|--------------|\n| `select(year:day)` | Columns from `year` to `day` |\n| `select(!year:day)` | All columns EXCEPT `year` to `day` |\n| `select(starts_with(\"dep\"))` | Columns starting with \"dep\" |\n| `select(ends_with(\"delay\"))` | Columns ending with \"delay\" |\n| `select(contains(\"time\"))` | Columns containing \"time\" |\n\n## Renaming While Selecting\n\nYou can rename columns on the fly:\n\n```r\nflights |> select(new_name = old_name)\n```\n\n---\n\n## üéØ Your Task\n\nSelect only the two columns related to **delays**:\n- `dep_delay` (departure delay)\n- `arr_delay` (arrival delay)\n\n> **Hint**: List both column names separated by a comma.\n",
    "starter_code": "# Select the delay columns\nflights |> select(..., ...)\n",
    "solution_code": "flights |> select(dep_delay, arr_delay)",
    "expected_output": "[Tibble: 336776 x 2]",
    "chapter_id": 3
  },
  "2023": {
    "id": 2023,
    "title": "Add Columns (Mutate)",
    "chapter_title": "Ch 3: Data Transformation",
    "content": "# üß¨ Add Columns with `mutate()`\n\n## Creating New Variables\n\n`mutate()` adds new columns calculated from existing ones.\n\nExample: Calculate how much time a delayed flight made up in the air:\n\n```r\nflights |> mutate(gain = dep_delay - arr_delay)\n```\n\nThis creates a new `gain` column from existing columns.\n\n## Controlling Column Position\n\nBy default, new columns appear at the end. Use `.before` or `.after` to place them:\n\n```r\nflights |> mutate(\n  new_column = calculation,\n  .before = 1  # Put at the beginning\n)\n```\n\n## Keep Only Calculated Columns\n\nUse `.keep = \"used\"` to keep only the columns involved in the calculation.\n\n---\n\n## üéØ Your Task\n\nCreate a new column called `speed` that calculates miles per hour.\n\n**Formula**: `distance / air_time * 60`\n\n(The `air_time` column is in minutes, so we multiply by 60 to get mph.)\n\n> **Hint**: Replace `...` with `distance / air_time * 60`.\n",
    "starter_code": "# Calculate speed in mph\nflights |> mutate(speed = ...)\n",
    "solution_code": "flights |> mutate(speed = distance / air_time * 60)",
    "expected_output": "[Tibble: with speed column]",
    "chapter_id": 3
  },
  "2024": {
    "id": 2024,
    "title": "Groups & Summaries",
    "chapter_title": "Ch 3: Data Transformation",
    "content": "# üìä Groups & Summaries\n\n## The Power of `group_by()` + `summarize()`\n\n`summarize()` collapses a data frame to summary statistics. But it's magical when combined with `group_by()`.\n\nExample: Average delay for each month:\n\n```r\nflights |>\n  group_by(month) |>\n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE))\n```\n\nThis computes the average delay **for each month**!\n\n```\n| month | avg_delay |\n|-------|----------:|\n|     1 |      10.0 |\n|     2 |      10.8 |\n|   ... |       ... |\n```\n\n## Why `na.rm = TRUE`?\n\nSome flights have missing delay data. Without `na.rm = TRUE`, you get `NA` results.\n\n## `n()` Counts Rows\n\n`n()` is a special function that counts rows in each group. Always include it!\n\n---\n\n## üéØ Your Task\n\nFind the average **arrival delay** (`arr_delay`) for each **destination** (`dest`).\n\n1. Group by `dest`\n2. Summarize with `mean(arr_delay, na.rm = TRUE)`\n\n> **Hint**: Replace the `...` placeholders with `dest` and `arr_delay`.\n",
    "starter_code": "# Average arrival delay by destination\nflights |>\n  group_by(...) |>\n  summarize(avg_arr_delay = mean(..., na.rm = TRUE))\n",
    "solution_code": "flights |>\n  group_by(dest) |>\n  summarize(avg_arr_delay = mean(arr_delay, na.rm = TRUE))",
    "expected_output": "[Tibble: 105 x 2]",
    "chapter_id": 3
  },
  "2030": {
    "id": 2030,
    "title": "Names & Spaces",
    "chapter_title": "Ch 4: Workflow: Code Style",
    "content": "# üíÖ Names & Spaces\n\n## Why Style Matters\n\n> Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.\n\nUsing a consistent style makes it easier for others (and future-you!) to read your work.\n\n---\n\n## Naming Rules\n\nVariable names should use only **lowercase letters**, **numbers**, and **underscores** (`_`).\n\n```\n‚úÖ GOOD                    ‚ùå BAD\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nshort_flights             SHORTFLIGHTS\nflight_data               FlightData\ncalculate_mean            calcMean\n```\n\n**Tip**: Prefer long, descriptive names over short, cryptic ones. Autocomplete will help you type them!\n\n---\n\n## Spacing Rules\n\n### 1. Spaces around operators\n\nPut spaces around `+`, `-`, `*`, `/`, `<-`, `==`, etc.\n\n```r\n# ‚úÖ Good\nz <- (a + b)^2 / d\n\n# ‚ùå Bad\nz<-( a + b ) ^ 2/d\n```\n\n### 2. Spaces in function calls\n\nNo space before `(`, always space after `,`.\n\n```r\n# ‚úÖ Good\nmean(x, na.rm = TRUE)\n\n# ‚ùå Bad\nmean (x ,na.rm=TRUE)\n```\n\n---\n\n## üéØ Your Task\n\nThe code below has **bad style**. Fix it by:\n1. Renaming `MyVariable` to use `snake_case`\n2. Adding proper spacing around `<-` and `+`\n3. Adding spaces after commas in `c()`\n\n> **Hint**: The fixed variable name should be `my_variable`.\n",
    "starter_code": "# Fix the naming and spacing\nMyVariable<-c(1,2,3)+4\n",
    "solution_code": "my_variable <- c(1, 2, 3) + 4",
    "expected_output": "[Vector: 5 6 7]",
    "chapter_id": 4
  },
  "2031": {
    "id": 2031,
    "title": "Pipes & Formatting",
    "chapter_title": "Ch 4: Workflow: Code Style",
    "content": "# üîß Pipes & Formatting\n\n## Pipe Style Rules\n\nThe pipe `|>` should:\n1. Have a **space before** it\n2. Be the **last thing on a line**\n3. Each new step should be **indented**\n\n```r\n# ‚úÖ Good: Easy to read and extend\nflights |>\n  filter(!is.na(arr_delay)) |>\n  group_by(dest) |>\n  summarize(delay = mean(arr_delay))\n\n# ‚ùå Bad: Hard to follow\nflights|>filter(!is.na(arr_delay))|>group_by(dest)|>summarize(delay=mean(arr_delay))\n```\n\n---\n\n## When to Break Lines\n\n**Named arguments** (like in `summarize()`) ‚Üí each on its own line:\n\n```r\nflights |>\n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n```\n\n**Simple functions** (like `filter()`) ‚Üí can stay on one line if short.\n\n---\n\n## ggplot2 Uses `+`\n\nThe same rules apply to `ggplot2`, but use `+` instead of `|>`.\n\n```r\nggplot(data, aes(x = month, y = delay)) +\n  geom_point() +\n  geom_line()\n```\n\n---\n\n## üéØ Your Task\n\nRestyle this messy pipeline to follow proper formatting:\n\n```r\nflights|>filter(dest==\"IAH\")|>group_by(year,month,day)|>summarize(n=n(),delay=mean(arr_delay,na.rm=TRUE))\n```\n\nRules to follow:\n1. Space before each `|>`\n2. Each verb on its own line\n3. Indent after the first line\n4. Space after commas\n\n> **Hint**: Start with `flights |>` then add each step on a new indented line.\n",
    "starter_code": "# Restyle this pipeline with proper formatting\nflights|>filter(dest==\"IAH\")|>group_by(year,month,day)|>summarize(n=n(),delay=mean(arr_delay,na.rm=TRUE))\n",
    "solution_code": "flights |>\n  filter(dest == \"IAH\") |>\n  group_by(year, month, day) |>\n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  )",
    "expected_output": "[Tibble: Formatted result]",
    "chapter_id": 4
  },
  "2040": {
    "id": 2040,
    "title": "Tidy Data",
    "chapter_title": "Ch 5: Data Tidying",
    "content": "# üßπ Tidy Data\n\n> \"Tidy datasets are all alike, but every messy dataset is messy in its own way.\"\n> ‚Äî Hadley Wickham\n\n## The Three Rules of Tidy Data\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  1. Each VARIABLE is a COLUMN     ‚îÇ\n‚îÇ  2. Each OBSERVATION is a ROW     ‚îÇ\n‚îÇ  3. Each VALUE is a CELL          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## Example: Tidy vs Messy\n\n**‚úÖ TIDY (`table1`)**:\n```\n| country     | year | cases  | population |\n|-------------|------|--------|------------|\n| Afghanistan | 1999 |    745 |   19987071 |\n| Afghanistan | 2000 |   2666 |   20595360 |\n| Brazil      | 1999 |  37737 |  172006362 |\n```\n\n**‚ùå MESSY (values in column names)**:\n```\n| country     | 1999 | 2000 |\n|-------------|------|------|\n| Afghanistan |  745 | 2666 |\n| Brazil      |37737 |80488 |\n```\n\nThe messy version hides \"year\" in the column headers instead of having it as a column!\n\n---\n\n## Why Tidy Data?\n\nWith tidy data, `dplyr` and `ggplot2` work seamlessly:\n\n```r\n# Compute rate per 10,000\ntable1 |> mutate(rate = cases / population * 10000)\n\n# Total cases per year\ntable1 |> group_by(year) |> summarize(total = sum(cases))\n```\n\n---\n\n## üéØ Your Task\n\nInspect `table1` by typing its name. Notice how:\n- Each variable (`country`, `year`, `cases`, `population`) has its own column\n- Each observation (country-year combination) is a row\n- Each cell contains a single value\n",
    "starter_code": "# View the tidy dataset\ntable1\n",
    "solution_code": "table1",
    "expected_output": "[Tibble: 6 x 4]",
    "chapter_id": 5
  },
  "2041": {
    "id": 2041,
    "title": "Lengthening Data",
    "chapter_title": "Ch 5: Data Tidying",
    "content": "# üìè Lengthening Data with `pivot_longer()`\n\n## The Problem\n\nOften, column names are actually **values** of a variable, not variable names themselves.\n\n**`table4a` (messy)**:\n```\n| country     | `1999` | `2000` |\n|-------------|--------|--------|\n| Afghanistan |    745 |   2666 |\n| Brazil      |  37737 |  80488 |\n| China       | 212258 | 213766 |\n```\n\nHere, `1999` and `2000` are **values** of a \"year\" variable, not separate variables!\n\n---\n\n## The Solution: `pivot_longer()`\n\nTransform columns into rows:\n\n```\nBEFORE                          AFTER\n+-------+------+------+         +-------+------+--------+\n|country| 1999 | 2000 |         |country| year | cases  |\n+-------+------+------+   ‚Üí     +-------+------+--------+\n| Afgh  | 745  | 2666 |         | Afgh  | 1999 |    745 |\n| Brazil|37737 |80488 |         | Afgh  | 2000 |   2666 |\n+-------+------+------+         | Brazil| 1999 |  37737 |\n                                | Brazil| 2000 |  80488 |\n                                +-------+------+--------+\n```\n\n## The Key Arguments\n\n| Argument | Purpose |\n|----------|---------|\n| `cols` | Which columns to pivot (the messy ones) |\n| `names_to` | Name for the new column that gets the old column **names** |\n| `values_to` | Name for the new column that gets the cell **values** |\n\n---\n\n## üéØ Your Task\n\nPivot `table4a` to create a tidy dataset with `year` and `cases` columns.\n\n- The columns to pivot are `` `1999` `` and `` `2000` `` (use backticks because they start with numbers)\n- The column names should become values in a new `year` column\n- The cell values should go into a new `cases` column\n\n> **Hint**: Replace the `...` with `\"year\"` and `\"cases\"` (in quotes because they're new column names).\n",
    "starter_code": "table4a |>\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"...\",\n    values_to = \"...\"\n  )\n",
    "solution_code": "table4a |>\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"cases\"\n  )",
    "expected_output": "[Tibble: 6 x 3]",
    "chapter_id": 5
  },
  "2042": {
    "id": 2042,
    "title": "Widening Data",
    "chapter_title": "Ch 5: Data Tidying",
    "content": "# ‚ÜîÔ∏è Widening Data with `pivot_wider()`\n\n## The Problem\n\nSometimes one observation is scattered across **multiple rows**.\n\n**`table2` (messy)**:\n```\n| country     | year | type       | count      |\n|-------------|------|------------|------------|\n| Afghanistan | 1999 | cases      |        745 |\n| Afghanistan | 1999 | population |   19987071 |\n| Afghanistan | 2000 | cases      |       2666 |\n| Afghanistan | 2000 | population |   20595360 |\n```\n\nOne observation (Afghanistan in 1999) is spread across **two rows**!\n\n---\n\n## The Solution: `pivot_wider()`\n\nTransform rows into columns:\n\n```\nBEFORE                              AFTER\n+-------+------+------+----------+  +-------+------+-------+------------+\n|country| year | type |   count  |  |country| year | cases | population |\n+-------+------+------+----------+  +-------+------+-------+------------+\n| Afgh  | 1999 |cases |      745 |  | Afgh  | 1999 |   745 |   19987071 |\n| Afgh  | 1999 |pop   | 19987071 |  | Afgh  | 2000 |  2666 |   20595360 |\n| Afgh  | 2000 |cases |     2666 |  +-------+------+-------+------------+\n| Afgh  | 2000 |pop   | 20595360 |\n+-------+------+------+----------+\n```\n\n## The Key Arguments\n\n| Argument | Purpose |\n|----------|---------|\n| `names_from` | Column whose **values** become new column **names** |\n| `values_from` | Column whose values fill the new columns |\n\n---\n\n## üéØ Your Task\n\nWiden `table2` so that `cases` and `population` each get their own column.\n\n- The `type` column has values `\"cases\"` and `\"population\"` ‚Üí these become new column names\n- The `count` column has the values ‚Üí these fill the new columns\n\n> **Hint**: `names_from` should be `type` (the column with the future column names), `values_from` should be `count`.\n",
    "starter_code": "table2 |>\n  pivot_wider(\n    names_from = ...,\n    values_from = ...\n  )\n",
    "solution_code": "table2 |>\n  pivot_wider(\n    names_from = type,\n    values_from = count\n  )",
    "expected_output": "[Tibble: 6 x 4]",
    "chapter_id": 5
  },
  "2050": {
    "id": 2050,
    "title": "Scripts vs Console",
    "chapter_title": "Ch 6: Workflow: Scripts",
    "content": "# üìú Scripts\n\n## Console vs Script Editor\n\nThe **console** is for quick tests. **Scripts** are for your real work.\n\n```\nCONSOLE              SCRIPT (.R)\n-----------          -----------\n‚ùå Ephemeral          ‚úÖ Saved\n‚ùå Gone on restart    ‚úÖ Reproducible\n‚ùå Hard to share      ‚úÖ Shareable\n```\n\n---\n\n## Essential Keyboard Shortcuts\n\n| Shortcut | Action |\n|----------|--------|\n| `Cmd/Ctrl + Shift + N` | Create new script |\n| `Cmd/Ctrl + Enter` | Run current line/selection |\n| `Cmd/Ctrl + Shift + S` | Run entire script |\n| `Cmd/Ctrl + S` | Save script |\n\n---\n\n## Best Practices\n\n1. **Always start with packages**: Put `library()` calls at the top\n2. **Never include `install.packages()`** in scripts you share\n3. **Restart R frequently**: `Cmd/Ctrl + Shift + F10` to ensure your script captures everything\n\n---\n\n## RStudio Diagnostics\n\nRStudio highlights errors with:\n- üî¥ **Red squiggly line**: Syntax error\n- üü° **Yellow warning**: Potential problem\n\nHover over the icon to see what's wrong!\n\n---\n\n## üéØ Your Task\n\nRun this multi-line script. Notice how you can run it all at once or line-by-line.\n\n1. First, read through the code to understand what it does\n2. Run all lines together\n",
    "starter_code": "# Load packages first\nlibrary(dplyr)\n\n# Create some data\nx <- 5\ny <- 10\n\n# Calculate and print result\nx * y\n",
    "solution_code": "library(dplyr)\nx <- 5\ny <- 10\nx * y",
    "expected_output": "50",
    "chapter_id": 6
  },
  "2051": {
    "id": 2051,
    "title": "Projects & Paths",
    "chapter_title": "Ch 6: Workflow: Scripts",
    "content": "# üìÅ Projects & Paths\n\n## The Working Directory\n\nR has a **working directory**‚Äîthe folder where it looks for files. Check yours:\n\n```r\ngetwd()  # e.g., \"/Users/hadley/Documents/r4ds\"\n```\n\n---\n\n## RStudio Projects\n\nKeep all files for an analysis in **one folder** with a `.Rproj` file.\n\n```\nmy_project/\n‚îú‚îÄ‚îÄ my_project.Rproj\n‚îú‚îÄ‚îÄ 01-load-data.R\n‚îú‚îÄ‚îÄ 02-analysis.R\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ raw_data.csv\n‚îî‚îÄ‚îÄ figures/\n    ‚îî‚îÄ‚îÄ plot.png\n```\n\n---\n\n## Relative vs Absolute Paths\n\n| Type | Example | Problem |\n|------|---------|---------|\n| **Absolute** | `/Users/hadley/data.csv` | ‚ùå Breaks on other computers |\n| **Relative** | `data/data.csv` | ‚úÖ Works everywhere |\n\n**Always use relative paths** in your scripts!\n\n---\n\n## File Naming Rules\n\n```\n‚ùå BAD                       ‚úÖ GOOD\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncode for analysis.R        01-load-data.R\nFinalReport.qmd            02-analysis.R\nfig 1.png                  fig-01.png\n```\n\n**Rules**:\n1. No spaces or special characters\n2. Descriptive names\n3. Number files to show order\n\n---\n\n## üéØ Your Task\n\nCheck where R is currently \"looking\" for files by running `getwd()`.\n\nThis tells you the current working directory‚Äîthe starting point for all relative paths.\n",
    "starter_code": "# Check the current working directory\ngetwd()\n",
    "solution_code": "getwd()",
    "expected_output": "/cloud/project",
    "chapter_id": 6
  },
  "2060": {
    "id": 2060,
    "title": "Reading CSV Files",
    "chapter_title": "Ch 7: Data Import",
    "content": "# üì• Reading CSV Files\n\n## What is a CSV?\n\nCSV = **C**omma-**S**eparated **V**alues. It's the most common data file format.\n\n```\nStudent ID,Full Name,favourite.food,AGE\n1,Sunil Huffmann,Strawberry yoghurt,4\n2,Barclay Lynn,French fries,5\n3,Jayendra Lyne,N/A,7\n```\n\n---\n\n## Reading with `read_csv()`\n\nUse `read_csv()` from the `readr` package:\n\n```r\nstudents <- read_csv(\"data/students.csv\")\n```\n\nIt automatically:\n- Detects column types (number, text, date)\n- Prints a summary of what it found\n\n---\n\n## Inline CSV (for testing)\n\nYou can read CSV data directly from a string:\n\n```r\nread_csv(\"a,b,c\n1,2,3\n4,5,6\")\n```\n\nResult:\n```\n| a | b | c |\n|---|---|---|\n| 1 | 2 | 3 |\n| 4 | 5 | 6 |\n```\n\n---\n\n## üéØ Your Task\n\nRead this inline CSV string into a data frame. The string contains columns `x`, `y`, `z` with two rows of data.\n\n1. Uncomment the `read_csv()` line\n2. Run the code to see the resulting tibble\n",
    "starter_code": "# An inline CSV string\ncsv_data <- \"x,y,z\n1,2,3\n4,5,6\"\n\n# Uncomment to read it:\n# read_csv(csv_data)\n",
    "solution_code": "csv_data <- \"x,y,z\n1,2,3\n4,5,6\"\nread_csv(csv_data)",
    "expected_output": "[Tibble: 2 x 3]",
    "chapter_id": 7
  },
  "2061": {
    "id": 2061,
    "title": "Handling Messy Data",
    "chapter_title": "Ch 7: Data Import",
    "content": "# üßπ Handling Messy Data\n\n## Common Problems\n\nReal data is messy! Common issues include:\n\n| Problem | Example | Solution |\n|---------|---------|----------|\n| Custom NA values | `N/A`, `.`, `missing` | `na = c(\"N/A\", \".\")` |\n| Spaces in column names | `Student ID` | `janitor::clean_names()` |\n| Wrong column type | Age stored as text | `col_types` argument |\n\n---\n\n## Handling Custom NA Values\n\nBy default, `read_csv()` only treats empty cells as NA. Tell it about other values:\n\n```r\nread_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n```\n\n---\n\n## Fixing Column Names\n\nColumn names with spaces need backticks. Use `janitor::clean_names()` to fix them:\n\n```r\nstudents |> janitor::clean_names()\n# student_id, full_name, favourite_food...\n```\n\n---\n\n## Parsing Functions\n\nWhen types are wrong, use parse functions:\n\n```r\nparse_number(\"$1,000\")    # 1000\nparse_date(\"2023-01-15\")  # Date object\nparse_integer(\"42\")       # 42L\n```\n\n---\n\n## üéØ Your Task\n\nThe `parse_number()` function extracts numbers from messy strings.\n\nParse the string `\"$1,234.56\"` to get the numeric value `1234.56`.\n\n> **Hint**: Just call `parse_number()` with the string as its argument.\n",
    "starter_code": "# Extract the number from this messy string\nmessy_price <- \"$1,234.56\"\n\n# Use parse_number() to clean it\n",
    "solution_code": "messy_price <- \"$1,234.56\"\nparse_number(messy_price)",
    "expected_output": "1234.56",
    "chapter_id": 7
  },
  "2070": {
    "id": 2070,
    "title": "Google is Your Friend",
    "chapter_title": "Ch 8: Workflow: Getting Help",
    "content": "# üÜò Getting Help\n\n## Googling Error Messages\n\nIf (when) you see an error message, **copy and paste it into Google**. Someone else has had the same problem.\n\nExample: `Error: object 'x' not found`\n\n## Stack Overflow\n\nLook for answers on Stack Overflow. Good answers often provide a **reprex** (reproducible example) that you can run yourself.\n\n---\n\n## üéØ Your Task\n\nSimulate an error by printing a string that LOOKS like an error message (just for fun/demo).\n",
    "starter_code": "\"Error: object 'x' not found\"",
    "solution_code": "\"Error: object 'x' not found\"",
    "expected_output": "\"Error: object 'x' not found\"",
    "chapter_id": 8
  },
  "2071": {
    "id": 2071,
    "title": "Making Reprexes",
    "chapter_title": "Ch 8: Workflow: Getting Help",
    "content": "# ‚ôªÔ∏è Reprexes\n\n## Help Me Help You\n\nIf you ask for help, you must provide a **Reprex** (Minimal Reproducible Example).\n\n1.  **Minimal**: Use the smallest dataset possible (like built-in `mtcars` or a tiny `tibble`).\n2.  **Reproducible**: Include all `library()` calls.\n\n## Example\n\n```r\nlibrary(dplyr)\n\ndf <- tibble(x = c(1, 2, NA))\nmean(df$x)\n#> [1] NA\n```\n\nThis code snippet tells the whole story. \n\n---\n\n## üéØ Your Task\n\nCreate a minimal example calculating the mean of numbers 1 to 5.\n",
    "starter_code": "mean(1:5)",
    "solution_code": "mean(1:5)",
    "expected_output": "3",
    "chapter_id": 8
  },
  "2100": {
    "id": 2100,
    "title": "Aesthetic Mappings",
    "chapter_title": "Ch 9: Layers",
    "content": "# üé® Aesthetic Mappings\n\n## What are Aesthetics?\n\nAesthetics map data variables to visual properties:\n\n| Aesthetic | What it controls | Example |\n|-----------|------------------|---------|\n| `x`, `y` | Position | `aes(x = displ, y = hwy)` |\n| `color` | Outline/line color | `aes(color = class)` |\n| `fill` | Fill color | `aes(fill = drv)` |\n| `shape` | Point shape | `aes(shape = drv)` |\n| `size` | Point size | `aes(size = cyl)` |\n| `alpha` | Transparency | `aes(alpha = year)` |\n\n---\n\n## Mapping vs Setting\n\n**Mapping** (inside `aes()`): Varies based on data\n```r\ngeom_point(aes(color = class))  # Different colors per class\n```\n\n**Setting** (outside `aes()`): Fixed value\n```r\ngeom_point(color = \"blue\")  # All points blue\n```\n\n---\n\n## The `mpg` Dataset\n\n```\n| manufacturer | model | displ | hwy | class   |\n|--------------|-------|-------|-----|---------|\n| audi         | a4    | 1.8   | 29  | compact |\n| audi         | a4    | 2.0   | 31  | compact |\n| ...          | ...   | ...   | ... | ...     |\n```\n\n- `displ`: Engine size (liters)\n- `hwy`: Highway MPG\n- `class`: Type of car\n\n---\n\n## üéØ Your Task\n\nCreate a scatterplot of `displ` vs `hwy` where points are colored by `class`.\n\n1. Map `displ` to x-axis\n2. Map `hwy` to y-axis\n3. Map `class` to color\n\n![Aesthetic Mapping](/assets/r-plots/mpg_mapping_color.png)\n\n> **Hint**: All three mappings go inside `aes()`.\n",
    "starter_code": "# Scatterplot with color by class\nggplot(mpg, aes(x = ..., y = ..., color = ...)) +\n  geom_point()\n",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()",
    "expected_output": "[Scatterplot colored by class]",
    "chapter_id": 9
  },
  "2101": {
    "id": 2101,
    "title": "Geoms & Layers",
    "chapter_title": "Ch 9: Layers",
    "content": "# üìê Geoms & Layers\n\n## Common Geoms\n\n| Geom | Purpose | Example |\n|------|---------|---------|\n| `geom_point()` | Scatterplot | Points |\n| `geom_smooth()` | Trend line | Fitted curve |\n| `geom_bar()` | Bar chart | Counts categories |\n| `geom_histogram()` | Distribution | Bins continuous data |\n| `geom_boxplot()` | Box plot | Shows quartiles |\n| `geom_line()` | Line chart | Connects points |\n\n---\n\n## Layering Geoms\n\nYou can add multiple geoms to the same plot!\n\n```r\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +         # Layer 1: points\n  geom_smooth()          # Layer 2: trend line\n```\n\nEach `+` adds a new layer on top of the previous one.\n\n---\n\n## Local vs Global Aesthetics\n\n**Global** (in `ggplot()`): Applies to all layers\n**Local** (in `geom_*()`): Only applies to that layer\n\n```r\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +  # Color only for points\n  geom_smooth()                      # No color\n```\n\n---\n\n## üéØ Your Task\n\nCreate a plot with **two layers**:\n1. A scatterplot of `displ` vs `hwy`\n2. A smooth trend line on top\n\n![Geoms and Layers](/assets/r-plots/mpg_geoms_layers.png)\n\n> **Hint**: Add `geom_point()` first, then `+ geom_smooth()`.\n",
    "starter_code": "# Two layers: points + smooth line\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  ...\n",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()",
    "expected_output": "[Scatterplot with trend line]",
    "chapter_id": 9
  },
  "2102": {
    "id": 2102,
    "title": "Facets",
    "chapter_title": "Ch 9: Layers",
    "content": "# üî≥ Facets (Small Multiples)\n\n## Why Facet?\n\nSometimes color isn't enough. **Faceting** splits your plot into subplots based on a categorical variable.\n\n---\n\n## `facet_wrap()`\n\nWraps a 1D ribbon of panels into 2D:\n\n```r\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~cyl)\n```\n\nThis creates one panel for each value of `cyl` (4, 5, 6, 8).\n\n---\n\n## `facet_grid()`\n\nFor two variables, use a grid layout:\n\n```r\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n```\n\n- Rows = `drv` (4, f, r)\n- Columns = `cyl` (4, 5, 6, 8)\n\n---\n\n## The Formula\n\n| Formula | Result |\n|---------|--------|\n| `~var` | Facet by `var` (wrap) |\n| `row ~ col` | Grid with rows and columns |\n| `drv ~ .` | Rows only |\n| `. ~ cyl` | Columns only |\n\n---\n\n## üéØ Your Task\n\nCreate a scatterplot of `displ` vs `hwy`, faceted by number of cylinders (`cyl`).\n\n1. Use `geom_point()` for the scatterplot\n2. Add `facet_wrap(~cyl)` to create subplots\n\n![Facets](/assets/r-plots/mpg_facets.png)\n\n> **Hint**: The tilde `~` before `cyl` is required.\n",
    "starter_code": "# Faceted scatterplot by cylinders\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(...)\n",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~cyl)",
    "expected_output": "[Faceted scatterplot by cyl]",
    "chapter_id": 9
  },
  "2110": {
    "id": 2110,
    "title": "Variation",
    "chapter_title": "Ch 10: EDA",
    "content": "# üîç Variation\n\n## What is EDA?\n\n**Exploratory Data Analysis** is an iterative cycle:\n\n1. Generate questions about your data\n2. Search for answers by visualizing & transforming\n3. Use what you learn to refine questions\n\n---\n\n## Two Key Questions\n\n1. What **variation** occurs within my variables?\n2. What **covariation** occurs between my variables?\n\n---\n\n## Visualizing Variation\n\n| Variable Type | Visualization |\n|---------------|---------------|\n| **Categorical** | `geom_bar()` |\n| **Continuous** | `geom_histogram()` |\n\n## Example: Diamond Carats\n\n```r\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n```\n\nLook for:\n- **Typical values**: Which are most common?\n- **Unusual values**: Outliers or gaps?\n- **Patterns**: Clusters or peaks?\n\n---\n\n## The `diamonds` Dataset\n\n```\n| carat | cut       | color | clarity | price |\n|-------|-----------|-------|---------|-------|\n| 0.23  | Ideal     | E     | SI2     |   326 |\n| 0.21  | Premium   | E     | SI1     |   326 |\n| 0.29  | Premium   | I     | VS2     |   334 |\n```\n\n~54,000 diamonds with carat, cut, color, clarity, price\n\n---\n\n## üéØ Your Task\n\nVisualize the **distribution of carat** in the `diamonds` dataset using a histogram.\n\n1. Map `carat` to x\n2. Use `geom_histogram()`\n3. Set `binwidth = 0.1` to see the pattern\n\n![Diamond Histogram](/assets/r-plots/diamond_histogram.png)\n\n> **Hint**: Notice the peaks at whole numbers and common fractions (0.5, 1.0, 1.5, 2.0).\n",
    "starter_code": "# Distribution of diamond carats\nggplot(diamonds, aes(x = ...)) +\n  geom_histogram(binwidth = 0.1)\n",
    "solution_code": "ggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)",
    "expected_output": "[Histogram of carat]",
    "chapter_id": 10
  },
  "2111": {
    "id": 2111,
    "title": "Covariation",
    "chapter_title": "Ch 10: EDA",
    "content": "# üîó Covariation\n\n## What is Covariation?\n\nCovariation is how variables vary **together**. It reveals relationships!\n\n---\n\n## Visualization by Variable Types\n\n| X Variable | Y Variable | Best Plot |\n|------------|------------|-----------|\n| Categorical | Continuous | `geom_boxplot()` |\n| Continuous | Continuous | `geom_point()` |\n| Categorical | Categorical | `geom_count()` or `geom_tile()` |\n\n---\n\n## Boxplots: Categorical vs Continuous\n\nBoxplots show:\n- **Median** (middle line)\n- **IQR** (box = 25th to 75th percentile)\n- **Outliers** (dots beyond whiskers)\n\n```r\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot()\n```\n\n---\n\n## A Surprising Finding!\n\n```\n| cut       | median_price |\n|-----------|--------------|\n| Fair      |     3282     | ‚Üê Lowest quality, highest price?!\n| Good      |     3050     |\n| Very Good |     2648     |\n| Premium   |     3185     |\n| Ideal     |     1810     | ‚Üê Highest quality, lowest price?!\n```\n\nWhy? Because **carat** (size) is a confounding variable. Larger diamonds tend to be lower quality.\n\n![Diamond Boxplot](/assets/r-plots/diamond_boxplot.png)\n\n---\n\n## üéØ Your Task\n\nExplore the relationship between `cut` (quality) and `price` using a boxplot.\n\n1. Map `cut` to x-axis\n2. Map `price` to y-axis\n3. Use `geom_boxplot()`\n\n> **Question to ponder**: Why might \"Fair\" diamonds have a higher median price than \"Ideal\" diamonds?\n",
    "starter_code": "# Are better cuts more expensive?\nggplot(diamonds, aes(x = ..., y = ...)) +\n  geom_boxplot()\n",
    "solution_code": "ggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot()",
    "expected_output": "[Boxplot of price by cut]",
    "chapter_id": 10
  },
  "2120": {
    "id": 2120,
    "title": "Labels & Titles",
    "chapter_title": "Ch 11: Communication",
    "content": "# üè∑Ô∏è Labels & Titles\n\n## From Exploration to Communication\n\nExploratory plots are for you. **Communication** plots are for others!\n\nMake your message clear with `labs()`.\n\n![Labeled Plot](/assets/r-plots/mpg_scatter_labeled.png)\n\n---\n\n## The `labs()` Function\n\n| Argument | Purpose |\n|----------|---------|\n| `title` | Main finding (not just \"A plot of X vs Y\") |\n| `subtitle` | Additional context in smaller font |\n| `caption` | Data source, often bottom-right |\n| `x`, `y` | Axis labels with units |\n| `color`, `fill` | Legend title |\n\n---\n\n## Example: A Complete Label\n\n```r\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  labs(\n    title = \"Fuel efficiency decreases with engine size\",\n    subtitle = \"Sports cars are an exception\",\n    caption = \"Data from fueleconomy.gov\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway MPG\",\n    color = \"Car type\"\n  )\n```\n\n---\n\n## Pro Tip: Title = Main Finding\n\n‚ùå **Bad**: \"Scatterplot of hwy vs displ\"\n‚úÖ **Good**: \"Fuel efficiency decreases with engine size\"\n\n---\n\n## üéØ Your Task\n\nAdd informative labels to make this plot publication-ready:\n\n1. `title`: \"Fuel efficiency decreases with engine size\"\n2. `x`: \"Engine displacement (L)\"\n3. `y`: \"Highway MPG\"\n\n> **Hint**: Use the `labs()` function with named arguments.\n",
    "starter_code": "# Add informative labels\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  labs(\n    title = \"...\",\n    x = \"...\",\n    y = \"...\"\n  )\n",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  labs(\n    title = \"Fuel efficiency decreases with engine size\",\n    x = \"Engine displacement (L)\",\n    y = \"Highway MPG\"\n  )",
    "expected_output": "[Labeled scatterplot]",
    "chapter_id": 11
  },
  "2121": {
    "id": 2121,
    "title": "Themes & Scales",
    "chapter_title": "Ch 11: Communication",
    "content": "# üé® Themes & Scales\n\n## Themes: Controlling the Look\n\nThemes control **non-data** elements of your plot.\n\n| Theme | Description |\n|-------|-------------|\n| `theme_gray()` | Default gray background |\n| `theme_bw()` | White background, black grid |\n| `theme_minimal()` | Minimal, no background |\n| `theme_classic()` | Classic scientific style |\n| `theme_void()` | Nothing but the geoms |\n\n---\n\n## Scales: Controlling Data Mapping\n\nScales control how data maps to visual properties.\n\n```r\n# Change axis breaks\nscale_y_continuous(breaks = seq(10, 40, by = 5))\n\n# Log scale\nscale_x_log10()\n\n# Better color palettes\nscale_color_brewer(palette = \"Set1\")\n```\n\n---\n\n## Color Palettes for Accessibility\n\n| Function | Use Case |\n|----------|----------|\n| `scale_color_brewer()` | Categorical, colorblind-friendly |\n| `scale_color_viridis_c()` | Continuous, perceptually uniform |\n| `scale_color_manual()` | Custom colors |\n\n---\n\n## Legend Position\n\n```r\ntheme(legend.position = \"bottom\")  # or \"top\", \"left\", \"none\"\n```\n\n---\n\n## üéØ Your Task\n\nApply a clean, professional look to this plot:\n\n![Themed Plot](/assets/r-plots/mpg_scatter_themed.png)\n\n1. Use `theme_minimal()` for a clean background\n2. Use `scale_color_brewer(palette = \"Set1\")` for better colors\n\n> **Hint**: Add each as a separate layer with `+`.\n",
    "starter_code": "# Make it professional\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  theme_...() +\n  scale_color_brewer(palette = \"...\")\n",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")",
    "expected_output": "[Professional styled plot]",
    "chapter_id": 11
  },
  "2200": {
    "id": 2200,
    "title": "Comparisons & Boolean",
    "chapter_title": "Ch 12: Logical Vectors",
    "content": "# üß† Comparisons & Boolean Logic\n\n## Logical Vectors\n\nLogical vectors have only 3 values: `TRUE`, `FALSE`, `NA`\n\n---\n\n## Comparison Operators\n\n- `<` Less than\n- `>` Greater than\n- `<=` Less than or equal\n- `>=` Greater than or equal\n- `==` Equal to\n- `!=` Not equal to\n\n---\n\n## Boolean Operators\n\n- `&` AND (both must be true)\n- OR operator (at least one true)\n- `!` NOT (invert the condition)\n\n---\n\n## The `%in%` Operator\n\nInstead of chaining OR conditions, use `%in%` for cleaner code:\n\n```r\n# Check if month is Nov or Dec\nflights %>% filter(month %in% c(11, 12))\n```\n\n---\n\n## Finding Missing Values: `is.na()`\n\n```r\nis.na(c(1, NA, 3))  # FALSE TRUE FALSE\n\n# Find cancelled flights\nflights %>% filter(is.na(dep_time))\n```\n\n---\n\n## üéØ Your Task\n\nFind all flights that departed in **November or December**.\n\n1. Use `filter()` with `%in%`\n2. Check if `month` is in `c(11, 12)`\n\n> **Hint**: `filter(data, column %in% c(val1, val2))`\n",
    "starter_code": "# Find Nov/Dec flights\nflights %>%\n  filter(month %in% c(..., ...))\n",
    "solution_code": "flights %>% filter(month %in% c(11, 12))",
    "expected_output": "[Tibble: Nov/Dec flights]",
    "chapter_id": 12
  },
  "2201": {
    "id": 2201,
    "title": "Conditional Logic",
    "chapter_title": "Ch 12: Logical Vectors",
    "content": "# üîÄ Conditional Transformations\n\n## `if_else()`: Two Outcomes\n\nUse `if_else()` for **two** possible outcomes:\n\n```r\nif_else(condition, true_value, false_value)\n```\n\n### Example: Label Delays\n\n```r\nflights %>% mutate(\n  status = if_else(arr_delay > 0, \"Late\", \"On time\")\n)\n```\n\n---\n\n## `case_when()`: Multiple Outcomes\n\nFor **many** conditions, use `case_when()`:\n\n```r\ncase_when(\n  condition1 ~ result1,\n  condition2 ~ result2,\n  .default = default_result\n)\n```\n\n### Example: Flight Status\n\n```r\nflights %>% mutate(\n  status = case_when(\n    is.na(arr_delay)     ~ \"Cancelled\",\n    arr_delay < -15      ~ \"Early\",\n    abs(arr_delay) <= 15 ~ \"On time\",\n    arr_delay < 60       ~ \"Late\",\n    .default = \"Very late\"\n  )\n)\n```\n\n---\n\n## Key Differences\n\n- `if_else()`: Exactly 2 outcomes\n- `case_when()`: 3 or more outcomes\n\n---\n\n## üéØ Your Task\n\nCreate a new column `delay_type` in the `flights` dataset:\n\n- If `arr_delay > 30`, label as \"Significant\"\n- Otherwise, label as \"Minor\"\n\n> **Hint**: `mutate(data, new_col = if_else(cond, \"yes\", \"no\"))`\n",
    "starter_code": "# Create delay_type column\nflights %>%\n  mutate(\n    delay_type = if_else(arr_delay > ..., \"...\", \"...\")\n  )\n",
    "solution_code": "flights %>%\n  mutate(delay_type = if_else(arr_delay > 30, \"Significant\", \"Minor\"))",
    "expected_output": "[Tibble with delay_type]",
    "chapter_id": 12
  },
  "2210": {
    "id": 2210,
    "title": "Counts & Summaries",
    "chapter_title": "Ch 13: Numbers",
    "content": "# üî¢ Counts & Summaries\n\n## The `count()` Function\n\n`count()` is one of the most useful functions in dplyr!\n\n```r\n# Count flights per destination\nflights %>% count(dest)\n\n# Sort by most common\nflights %>% count(dest, sort = TRUE)\n```\n\n---\n\n## Counting Variants\n\n- `n()` - Count rows in current group\n- `n_distinct(x)` - Count unique values\n- `sum(is.na(x))` - Count missing values\n\n### Example: Carriers per Destination\n\n```r\nflights %>%\n  group_by(dest) %>%\n  summarize(\n    n_flights = n(),\n    n_carriers = n_distinct(carrier)\n  )\n```\n\n---\n\n## Ranking Functions\n\n- `min_rank(x)` - Rank with ties (1, 2, 2, 4)\n- `row_number(x)` - Unique rank (1, 2, 3, 4)\n- `dense_rank(x)` - No gaps (1, 2, 2, 3)\n\n---\n\n## Numeric Summaries\n\n- `mean(x, na.rm = TRUE)` - Average\n- `median(x, na.rm = TRUE)` - Middle value\n- `sd(x, na.rm = TRUE)` - Standard deviation\n\n---\n\n## üéØ Your Task\n\nCount the number of flights per destination (`dest`) and sort by most common:\n\n1. Use `count()` on the `flights` dataset\n2. Set `sort = TRUE`\n\n> **Hint**: `count(data, column, sort = TRUE)`\n",
    "starter_code": "# Count flights per destination\nflights %>%\n  count(..., sort = ...)\n",
    "solution_code": "flights %>% count(dest, sort = TRUE)",
    "expected_output": "[Tibble: Destinations by flight count]",
    "chapter_id": 13
  },
  "2220": {
    "id": 2220,
    "title": "String Basics",
    "chapter_title": "Ch 14: Strings",
    "content": "# üßµ String Basics\n\n## The stringr Package\n\nAll stringr functions start with `str_` for easy autocomplete!\n\n---\n\n## Creating Strings\n\n```r\n# Single or double quotes\nx <- \"Hello, World!\"\ny <- 'It's a string'\n\n# Escapes\nnewline <- \"Line1\\nLine2\"\ntab <- \"Col1\\tCol2\"\n```\n\n---\n\n## Combining Strings: `str_c()`\n\n```r\nstr_c(\"x\", \"y\", \"z\")           # \"xyz\"\nstr_c(\"Hello \", name, \"!\")     # \"Hello John!\"\nstr_c(x, y, sep = \"-\")         # \"x-y\"\n```\n\nWorks great with `mutate()`:\n```r\ndf %>% mutate(greeting = str_c(\"Hi \", name, \"!\"))\n```\n\n---\n\n## Template Strings: `str_glue()`\n\nEmbed variables directly with `{}`:\n\n```r\nname <- \"Alice\"\nstr_glue(\"Hello {name}!\")  # \"Hello Alice!\"\n\n# In mutate\ndf %>% mutate(greeting = str_glue(\"Hi {name}!\"))\n```\n\n---\n\n## String Length & Substrings\n\n```r\nstr_length(\"Apple\")        # 5\nstr_sub(\"Apple\", 1, 3)     # \"App\"\nstr_sub(\"Apple\", -3, -1)   # \"ple\"\n```\n\n---\n\n## üéØ Your Task\n\nUse `str_glue()` to create a greeting that says \"Hello, {name}!\" for each row:\n\n1. Use `mutate()` with `str_glue()`\n2. Embed the `name` variable in curly braces\n\n> **Hint**: `mutate(df, greeting = str_glue(\"Hello, {name}!\"))`\n",
    "starter_code": "# Create greetings with str_glue\ndf <- tibble(name = c(\"Alice\", \"Bob\", \"Carol\"))\n\ndf %>%\n  mutate(greeting = str_glue(\"Hello, {...}!\"))\n",
    "solution_code": "df <- tibble(name = c(\"Alice\", \"Bob\", \"Carol\"))\ndf %>% mutate(greeting = str_glue(\"Hello, {name}!\"))",
    "expected_output": "[Tibble with greeting column]",
    "chapter_id": 14
  },
  "2230": {
    "id": 2230,
    "title": "Regex Basics",
    "chapter_title": "Ch 15: Regex",
    "content": "# üß© Regular Expressions\n\n## What is Regex?\n\nA concise language for describing **patterns** in strings.\n\n```r\nstr_view(fruit, \"berry\")  # Find all with \"berry\"\n```\n\n---\n\n## Metacharacters\n\n| Symbol | Meaning |\n|--------|---------|\n| `.` | Any single character |\n| `^` | Start of string |\n| `$` | End of string |\n| `\\b` | Word boundary |\n\n---\n\n## Quantifiers\n\n| Symbol | Meaning |\n|--------|---------|\n| `?` | 0 or 1 times (optional) |\n| `+` | 1 or more times |\n| `*` | 0 or more times |\n\n```r\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")  # matches \"ab\", \"abb\"\n```\n\n---\n\n## Character Classes\n\n- `[aeiou]` - Match any vowel\n- `[0-9]` - Match any digit\n- `[^aeiou]` - Match anything EXCEPT vowels\n\n```r\nstr_view(words, \"[aeiou]x[aeiou]\")  # vowel-x-vowel\n```\n\n---\n\n## Key Functions\n\n- `str_detect(x, pattern)` - Returns TRUE/FALSE\n- `str_count(x, pattern)` - Count matches\n- `str_replace(x, pattern, replacement)` - Replace matches\n\n---\n\n## üéØ Your Task\n\nUse `str_detect()` to find all words from `stringr::words` that **start with \"y\"**:\n\n1. Use the anchor `^` for start of string\n2. Pattern should be `\"^y\"`\n\n> **Hint**: `str_detect(words, \"^y\")` returns logical vector\n",
    "starter_code": "# Find words starting with \"y\"\nlibrary(stringr)\n\nwords[str_detect(words, \"...\")]\n",
    "solution_code": "words[str_detect(words, \"^y\")]",
    "expected_output": "[Character vector: words starting with y]",
    "chapter_id": 15
  },
  "2240": {
    "id": 2240,
    "title": "Factor Basics",
    "chapter_title": "Ch 16: Factors",
    "content": "# üìä Factors\n\n## Why Factors?\n\nStrings sort alphabetically. Factors sort in **your** order!\n\n```r\n# Problem: Months sort alphabetically\nsort(c(\"Dec\", \"Apr\", \"Jan\"))  # Apr, Dec, Jan\n\n# Solution: Use factors with levels\nmonth_levels <- c(\"Jan\", \"Feb\", \"Mar\", ...)\nfactor(months, levels = month_levels)\n```\n\n---\n\n## The forcats Package\n\nAll functions start with `fct_` for easy autocomplete!\n\n---\n\n## Reordering for Plots\n\n### `fct_reorder()` - Sort by another variable\n\n```r\n# Sort religions by TV hours watched\nggplot(data, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n  geom_point()\n```\n\n### `fct_infreq()` - Sort by frequency\n\n```r\n# Bar chart sorted by count\nggplot(gss_cat, aes(x = fct_infreq(marital))) +\n  geom_bar()\n```\n\n---\n\n## Modifying Levels\n\n### `fct_recode()` - Rename levels\n\n```r\nfct_recode(partyid,\n  \"Republican\" = \"Strong republican\",\n  \"Democrat\" = \"Strong democrat\"\n)\n```\n\n### `fct_collapse()` - Combine levels\n\n```r\nfct_collapse(partyid,\n  \"rep\" = c(\"Strong republican\", \"Not str republican\"),\n  \"dem\" = c(\"Strong democrat\", \"Not str democrat\")\n)\n```\n\n---\n\n## üéØ Your Task\n\nUsing the `mpg` dataset, create a plot with `class` on the y-axis, reordered by median `hwy`:\n\n1. Use `fct_reorder(class, hwy)` in the aesthetic\n2. Add `geom_boxplot()`\n\n![Reordered Boxplot](/assets/r-plots/mpg_boxplot_reordered.png)\n\n> **Hint**: The reordering goes inside `aes(y = fct_reorder(...))`\n",
    "starter_code": "# Reorder class by highway mpg\nggplot(mpg, aes(x = hwy, y = fct_reorder(..., ...))) +\n  geom_boxplot()\n",
    "solution_code": "ggplot(mpg, aes(x = hwy, y = fct_reorder(class, hwy))) +\n  geom_boxplot()",
    "expected_output": "[Boxplot sorted by median hwy]",
    "chapter_id": 16
  },
  "2250": {
    "id": 2250,
    "title": "Dates & Times",
    "chapter_title": "Ch 17: Dates and Times",
    "content": "# üìÖ Dates & Times\n\n## The lubridate Package\n\nMakes dates easy! Part of the tidyverse.\n\n---\n\n## Parsing Dates\n\nMatch the function to your date format:\n\n| Function | Example Input |\n|----------|---------------|\n| `ymd()` | \"2023-01-31\" |\n| `mdy()` | \"January 31st, 2023\" |\n| `dmy()` | \"31-Jan-2023\" |\n\nAdd `_hms` for times: `ymd_hms(\"2023-01-31 20:11:59\")`\n\n---\n\n## Extracting Components\n\n```r\ndatetime <- ymd_hms(\"2023-07-08 12:34:56\")\n\nyear(datetime)   # 2023\nmonth(datetime)  # 7\nday(datetime)    # 8\nwday(datetime, label = TRUE)  # \"Sat\"\nhour(datetime)   # 12\n```\n\n---\n\n## Date Math: Durations vs Periods\n\n### Durations = Exact seconds\n```r\nddays(1)   # 86400 seconds\ndyears(1)  # 31557600 seconds\n```\n\n### Periods = Human units\n```r\ndays(1)    # 1 day (respects DST)\nyears(1)   # 1 year (respects leap years)\n```\n\n---\n\n## Useful Functions\n\n```r\ntoday()            # Current date\nnow()              # Current date-time\nfloor_date(x, \"week\")  # Round down to week\n```\n\n---\n\n## üéØ Your Task\n\nParse the date \"March 15, 2023\" and extract the **day of the week**:\n\n1. Use `mdy()` to parse (month-day-year format)\n2. Use `wday()` with `label = TRUE` to get day name\n\n> **Hint**: `wday(mdy(\"...\"), label = TRUE)`\n",
    "starter_code": "# Parse and extract day of week\nlibrary(lubridate)\n\ndate <- mdy(\"March 15, 2023\")\nwday(date, label = ...)\n",
    "solution_code": "mdy(\"March 15, 2023\") %>% wday(label = TRUE)",
    "expected_output": "[Factor: Wed]",
    "chapter_id": 17
  },
  "2260": {
    "id": 2260,
    "title": "Missing Values",
    "chapter_title": "Ch 18: Missing Values",
    "content": "# üö´ Missing Values\n\n## Two Types of Missing\n\n### Explicit Missing\nYou see `NA` in your data.\n\n### Implicit Missing\nThe entire row is absent from your data!\n\n---\n\n## Handling Explicit NAs\n\n### Detect with `is.na()`\n```r\nx <- c(1, NA, 3)\nis.na(x)  # FALSE TRUE FALSE\n```\n\n### Replace with `coalesce()`\n```r\nx <- c(1, NA, 3)\ncoalesce(x, 0)  # 1, 0, 3\n```\n\n### Fill forward with `fill()`\n```r\ndf %>% fill(column_name)  # LOCF\n```\n\n### Convert value to NA with `na_if()`\n```r\nx <- c(1, 99, 3)  # 99 means missing\nna_if(x, 99)      # 1, NA, 3\n```\n\n---\n\n## Making Implicit Explicit\n\n### `complete()` fills missing combinations\n\n```r\nstocks <- tibble(\n  year = c(2020, 2020, 2021),\n  qtr = c(1, 2, 2),\n  price = c(1.5, 0.9, 1.2)\n)\n\n# Q1 2021 is implicitly missing!\nstocks %>% complete(year, qtr)\n```\n\n---\n\n## üéØ Your Task\n\nGiven this vector with a missing value, replace the `NA` with `0`:\n\n```r\nx <- c(5, NA, 10, NA, 15)\n```\n\n1. Use `coalesce()` to replace NAs with 0\n\n> **Hint**: `coalesce(vector, replacement_value)`\n",
    "starter_code": "# Replace NAs with 0\nx <- c(5, NA, 10, NA, 15)\n\ncoalesce(x, ...)\n",
    "solution_code": "x <- c(5, NA, 10, NA, 15)\ncoalesce(x, 0)",
    "expected_output": "[Vector: 5, 0, 10, 0, 15]",
    "chapter_id": 18
  },
  "2270": {
    "id": 2270,
    "title": "Mutating Joins",
    "chapter_title": "Ch 19: Joins",
    "content": "# üîó Mutating Joins\n\n## Keys: The Foundation\n\n### Primary Key\nUniquely identifies each row in its table.\n- `airlines$carrier` - Two-letter carrier code\n- `airports$faa` - Three-letter airport code\n- `planes$tailnum` - Plane tail number\n\n### Foreign Key\nReferences a primary key in another table.\n- `flights$carrier` ‚Üí `airlines$carrier`\n- `flights$dest` ‚Üí `airports$faa`\n\n---\n\n## The Four Mutating Joins\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Join Type   ‚îÇ What it Keeps                ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ left_join   ‚îÇ All rows from x              ‚îÇ\n‚îÇ right_join  ‚îÇ All rows from y              ‚îÇ\n‚îÇ inner_join  ‚îÇ Only matching rows           ‚îÇ\n‚îÇ full_join   ‚îÇ All rows from x AND y        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## Example: Add Airline Names to Flights\n\n```r\nflights %>%\n  left_join(airlines, join_by(carrier))\n```\n\n- Adds `name` column from `airlines`\n- Keeps ALL rows from `flights`\n- Unmatched rows get `NA`\n\n---\n\n## Specifying Keys with `join_by()`\n\n```r\n# Same column name\nleft_join(x, y, join_by(key))\n\n# Different column names\nleft_join(flights, airports, join_by(dest == faa))\n```\n\n---\n\n## üéØ Your Task\n\nJoin `flights` to `airlines` to add the full carrier name:\n\n1. Use `left_join()`\n2. The common key is `carrier`\n\n> **Hint**: `left_join(flights, airlines)`\n",
    "starter_code": "# Add airline names to flights\nlibrary(nycflights13)\n\nflights %>%\n  select(carrier, flight, origin, dest) %>%\n  left_join(..., join_by(...))\n",
    "solution_code": "flights %>%\n  select(carrier, flight, origin, dest) %>%\n  left_join(airlines, join_by(carrier))",
    "expected_output": "[Tibble with name column added]",
    "chapter_id": 19
  },
  "2271": {
    "id": 2271,
    "title": "Filtering Joins",
    "chapter_title": "Ch 19: Joins",
    "content": "# üîç Filtering Joins\n\n## Filter, Don't Add Columns!\n\nFiltering joins affect which **rows** are kept, not which columns.\n\n---\n\n## Two Types\n\n### `semi_join(x, y)`\nKeep rows in `x` that **have** a match in `y`\n\n```r\n# Find airports that appear as destinations\nairports %>%\n  semi_join(flights, join_by(faa == dest))\n```\n\n### `anti_join(x, y)`\nKeep rows in `x` that **don't have** a match in `y`\n\n```r\n# Find flights to airports NOT in our database\nflights %>%\n  anti_join(airports, join_by(dest == faa))\n```\n\n---\n\n## Finding Missing Data\n\nAnti-joins are perfect for finding implicit missing values:\n\n```r\n# Which planes are in flights but NOT in planes table?\nflights %>%\n  distinct(tailnum) %>%\n  anti_join(planes)\n\n# Result: 722 planes with no info!\n```\n\n---\n\n## Visual Comparison\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ semi_join  ‚îÇ Keep matches (filter IN)    ‚îÇ\n‚îÇ anti_join  ‚îÇ Keep non-matches (filter OUT)‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## üéØ Your Task\n\nFind all destinations (`dest`) in `flights` that are NOT in the `airports` table:\n\n1. Use `anti_join()` with `flights` and `airports`\n2. Match `dest` to `faa`\n3. Get unique destinations with `distinct()`\n\n> **Hint**: `flights %>% ... %>% anti_join(airports, join_by(dest == faa))`\n",
    "starter_code": "# Find destinations not in airports table\nlibrary(nycflights13)\n\nflights %>%\n  distinct(dest) %>%\n  anti_join(..., join_by(... == ...))\n",
    "solution_code": "flights %>%\n  distinct(dest) %>%\n  anti_join(airports, join_by(dest == faa))",
    "expected_output": "[Tibble: 4 missing airports - BQN, SJU, STT, PSE]",
    "chapter_id": 19
  },
  "2300": {
    "id": 2300,
    "title": "Reading Excel",
    "chapter_title": "Ch 20: Spreadsheets",
    "content": "# üìä Reading Excel Files\n\n## The readxl Package\n\nLoad separately - not part of core tidyverse!\n\n```r\nlibrary(readxl)\n```\n\n---\n\n## Basic Reading\n\n```r\n# Read first sheet\nstudents <- read_excel(\"students.xlsx\")\n\n# Read specific sheet\nread_excel(\"penguins.xlsx\", sheet = \"Torgersen Island\")\n```\n\n---\n\n## Common Arguments\n\n| Argument | Purpose | Example |\n|----------|---------|---------|\n| `sheet` | Which worksheet | `sheet = \"Sales\"` |\n| `skip` | Skip rows at top | `skip = 1` |\n| `na` | Strings to treat as NA | `na = c(\"\", \"N/A\")` |\n| `col_names` | Custom column names | `col_names = c(\"id\", \"name\")` |\n| `col_types` | Force types | `col_types = c(\"numeric\", \"text\")` |\n| `range` | Read specific cells | `range = \"A5:F15\"` |\n\n---\n\n## Working with Multiple Sheets\n\n```r\n# List all sheets\nexcel_sheets(\"penguins.xlsx\")\n# [1] \"Torgersen Island\" \"Biscoe Island\" \"Dream Island\"\n\n# Read each, then combine\ntorgersen <- read_excel(\"penguins.xlsx\", sheet = \"Torgersen Island\")\nbiscoe <- read_excel(\"penguins.xlsx\", sheet = \"Biscoe Island\")\n\npenguins <- bind_rows(torgersen, biscoe)\n```\n\n---\n\n## Reading Cell Ranges\n\n```r\n# Skip headers and footers - read only data!\nread_excel(\"messy.xlsx\", range = \"A5:F15\")\n```\n\n---\n\n## üéØ Your Task\n\nRead the \"Sales\" sheet from an Excel file, skipping the first 2 rows:\n\n1. Use `read_excel()` with `sheet = \"Sales\"`\n2. Add `skip = 2` to skip header rows\n\n> **Hint**: `read_excel(\"data.xlsx\", sheet = \"...\", skip = ...)`\n",
    "starter_code": "# Read Sales sheet, skip 2 rows\nlibrary(readxl)\n\nread_excel(\"data.xlsx\", sheet = ..., skip = ...)\n",
    "solution_code": "read_excel(\"data.xlsx\", sheet = \"Sales\", skip = 2)",
    "expected_output": "[Tibble: Sales data]",
    "chapter_id": 20
  },
  "2310": {
    "id": 2310,
    "title": "Databases & dbplyr",
    "chapter_title": "Ch 21: Databases",
    "content": "# üóÑÔ∏è Databases & dbplyr\n\n## Why Databases?\n\nData frames live in RAM (limited). Databases live on disk (unlimited!).\n\n---\n\n## Connecting with DBI\n\n```r\nlibrary(DBI)\nlibrary(dbplyr)\n\n# Connect to duckdb (in-process database)\ncon <- DBI::dbConnect(duckdb::duckdb())\n\n# Write data to database\ndbWriteTable(con, \"flights\", nycflights13::flights)\n```\n\n---\n\n## The Magic of dbplyr\n\n### Create a table reference\n```r\nflights_db <- tbl(con, \"flights\")\n```\n\n### Write dplyr code - it's LAZY!\n```r\n# This runs INSTANTLY (no data downloaded!)\nquery <- flights_db |>\n  filter(dest == \"IAH\") |>\n  select(carrier, flight, dep_delay)\n```\n\n### See the generated SQL\n```r\nquery |> show_query()\n# SELECT carrier, flight, dep_delay\n# FROM flights\n# WHERE (dest = 'IAH')\n```\n\n### Download results with `collect()`\n```r\nresult <- query |> collect()  # NOW data downloads!\n```\n\n---\n\n## SQL Clause Mapping\n\n| dplyr | SQL Clause |\n|-------|------------|\n| `select()` | SELECT |\n| `filter()` | WHERE |\n| `arrange()` | ORDER BY |\n| `group_by() + summarize()` | GROUP BY |\n| `mutate()` | SELECT (with expressions) |\n\n---\n\n## üéØ Your Task\n\nGiven a database table `diamonds_db`, filter for `price > 10000` and view the SQL:\n\n1. Use `filter()` to select expensive diamonds\n2. Use `show_query()` to see what SQL was generated\n\n> **Hint**: `diamonds_db |> filter(...) |> show_query()`\n",
    "starter_code": "# Filter and view SQL\ndiamonds_db |>\n  filter(price > ...) |>\n  show_query()\n",
    "solution_code": "diamonds_db |>\n  filter(price > 10000) |>\n  show_query()",
    "expected_output": "[SQL: SELECT * FROM diamonds WHERE (price > 10000)]",
    "chapter_id": 21
  },
  "2320": {
    "id": 2320,
    "title": "Arrow & Parquet",
    "chapter_title": "Ch 22: Arrow",
    "content": "# üèπ Arrow & Parquet\n\n## The Problem with CSV\n\nCSV files are human-readable but **slow**:\n- No type information (everything is text)\n- Row-oriented (must read entire rows)\n- Large file sizes\n\n---\n\n## Parquet: The Solution\n\nParquet is a binary format designed for big data:\n\n| Feature | CSV | Parquet |\n|---------|-----|---------|\n| Size | Large | ~50% smaller |\n| Types | None | Rich types |\n| Structure | Row-based | Column-based |\n| Speed | Slow | 100x faster! |\n\n---\n\n## Opening Datasets with Arrow\n\n```r\nlibrary(arrow)\n\n# Open a parquet file or directory\nseattle_pq <- open_dataset(\"data/seattle-checkouts/\")\n\n# It's LAZY - no data loaded yet!\nseattle_pq |> glimpse()\n```\n\n---\n\n## Write Partitioned Data\n\nSplit large datasets into manageable chunks:\n\n```r\n# Partition by year\ndata |>\n  group_by(year) |>\n  write_dataset(\"data/output/\", format = \"parquet\")\n```\n\nCreates folders like:\n```\ndata/output/\n‚îú‚îÄ‚îÄ year=2020/part-0.parquet\n‚îú‚îÄ‚îÄ year=2021/part-0.parquet\n‚îî‚îÄ‚îÄ year=2022/part-0.parquet\n```\n\n---\n\n## Performance Magic\n\nArrow is smart! When you filter:\n```r\nseattle_pq |>\n  filter(CheckoutYear == 2021)  # Only reads that folder!\n```\n\nResult: **~100x speedup** compared to CSV!\n\n---\n\n## Arrow + DuckDB\n\nSeamlessly switch to SQL:\n```r\nseattle_pq |>\n  to_duckdb() |>\n  # Now it's a database table!\n```\n\n---\n\n## üéØ Your Task\n\nOpen a parquet dataset from a directory and filter it:\n\n1. Use `open_dataset(\"data/flights-parquet/\")`\n2. Filter for `year == 2013`\n3. Use `collect()` to download results\n\n> **Hint**: `open_dataset(...) |> filter(...) |> collect()`\n",
    "starter_code": "# Open and filter parquet data\nlibrary(arrow)\n\nopen_dataset(\"data/flights-parquet/\") |>\n  filter(year == ...) |>\n  collect()\n",
    "solution_code": "open_dataset(\"data/flights-parquet/\") |>\n  filter(year == 2013) |>\n  collect()",
    "expected_output": "[Tibble: Filtered flight data]",
    "chapter_id": 22
  },
  "2330": {
    "id": 2330,
    "title": "Lists & Rectangling",
    "chapter_title": "Ch 23: Hierarchical Data",
    "content": "# üå≥ Hierarchical Data\n\n## Lists: The Foundation\n\nLists can hold **anything** - even other lists!\n\n```r\nx <- list(\n  name = \"John\",\n  age = 34,\n  scores = c(90, 85, 88)\n)\nstr(x)  # See structure\n```\n\n---\n\n## List-Columns in Tibbles\n\nData frames can have **list columns**:\n\n```r\ndf <- tibble(\n  id = 1:2,\n  info = list(\n    list(a = 1, b = 2),\n    list(a = 3, b = 4)\n  )\n)\n```\n\n---\n\n## Rectangling: Making Lists Flat\n\n### `unnest_wider()` - Named lists ‚Üí Columns\n\n```r\ndf <- tibble(\n  x = 1:2,\n  y = list(list(a=1, b=2), list(a=3, b=4))\n)\n\ndf |> unnest_wider(y)\n#       x     a     b\n#   <int> <dbl> <dbl>\n# 1     1     1     2\n# 2     2     3     4\n```\n\n### `unnest_longer()` - Unnamed lists ‚Üí Rows\n\n```r\ndf <- tibble(\n  x = 1:2,\n  y = list(c(\"a\", \"b\"), c(\"c\", \"d\", \"e\"))\n)\n\ndf |> unnest_longer(y)\n#       x y    \n#   <int> <chr>\n# 1     1 a    \n# 2     1 b    \n# 3     2 c    \n# ...\n```\n\n---\n\n## Decision Tree\n\n```\nIs the list-column NAMED?\n  ‚îú‚îÄ‚îÄ YES ‚Üí unnest_wider() (make columns)\n  ‚îî‚îÄ‚îÄ NO  ‚Üí unnest_longer() (make rows)\n```\n\n---\n\n## üéØ Your Task\n\nGiven this data with a **named** list-column:\n\n```r\ndf <- tibble(\n  id = 1:2,\n  info = list(\n    list(name = \"Alice\", score = 95),\n    list(name = \"Bob\", score = 87)\n  )\n)\n```\n\nUnnest `info` into separate columns (name and score):\n\n> **Hint**: Use `unnest_wider(info)` since the lists are named\n",
    "starter_code": "# Unnest named list-column into columns\ndf <- tibble(\n  id = 1:2,\n  info = list(\n    list(name = \"Alice\", score = 95),\n    list(name = \"Bob\", score = 87)\n  )\n)\n\ndf |> unnest_wider(...)\n",
    "solution_code": "df |> unnest_wider(info)",
    "expected_output": "[Tibble: id, name, score columns]",
    "chapter_id": 23
  },
  "2340": {
    "id": 2340,
    "title": "Web Scraping with rvest",
    "chapter_title": "Ch 24: Web Scraping",
    "content": "# üï∑Ô∏è Web Scraping with rvest\n\n## The Workflow\n\n```\n1. read_html(url)      ‚Üí Get the page\n2. html_elements(css)  ‚Üí Find elements\n3. html_text2()        ‚Üí Extract text\n   html_attr()         ‚Üí Extract attributes\n   html_table()        ‚Üí Extract tables\n```\n\n---\n\n## CSS Selectors\n\n| Selector | Meaning | Example |\n|----------|---------|---------|\n| `p` | All `<p>` tags | `html_elements(\"p\")` |\n| `.title` | Class \"title\" | `html_elements(\".title\")` |\n| `#first` | ID \"first\" | `html_elements(\"#first\")` |\n\n---\n\n## Example: Extracting Text\n\n```r\nhtml <- read_html(\"https://example.com\")\n\n# Get all paragraph text\nhtml |>\n  html_elements(\"p\") |>\n  html_text2()\n```\n\n---\n\n## html_element vs html_elements\n\n```r\n# html_elements() ‚Üí Returns ALL matches (vector)\nhtml |> html_elements(\"p\")  # All paragraphs\n\n# html_element() ‚Üí Returns ONE per input (NA if missing)\nhtml |> html_element(\"h1\")  # First h1 only\n```\n\n---\n\n## Extracting Attributes\n\n```r\n# Get href from links\nhtml |>\n  html_elements(\"a\") |>\n  html_attr(\"href\")\n\n# Get image sources\nhtml |>\n  html_elements(\"img\") |>\n  html_attr(\"src\")\n```\n\n---\n\n## Extracting Tables\n\n```r\nhtml |>\n  html_element(\"table\") |>\n  html_table()  # ‚Üí Returns a tibble!\n```\n\n---\n\n## üéØ Your Task\n\nGiven this HTML stored in `page`:\n\n```html\n<html>\n  <h1>Welcome</h1>\n  <p class=\"intro\">Hello world!</p>\n  <p class=\"content\">More text here.</p>\n</html>\n```\n\nExtract the text from the element with class \"intro\":\n\n1. Use `html_element(\".intro\")`\n2. Then use `html_text2()` to get the text\n\n> **Hint**: `page |> html_element(\"...\") |> html_text2()`\n",
    "starter_code": "# Extract text from .intro class\npage |>\n  html_element(\"...\") |>\n  html_text2()\n",
    "solution_code": "page |>\n  html_element(\".intro\") |>\n  html_text2()",
    "expected_output": "[1] \"Hello world!\"",
    "chapter_id": 24
  },
  "2400": {
    "id": 2400,
    "title": "Functions & Tidy Eval",
    "chapter_title": "Ch 25: Functions",
    "content": "# üîß Functions & Tidy Eval\n\n## Vector Functions\n\nSimple functions that take a vector and return a vector.\n\n```r\nrescale01 <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n```\n\n## Data Frame Functions & `{{ }}`\n\nWhen writing functions for **dplyr** or **ggplot2**, you can't just pass variable names. You must **embrace** them with `{{ }}` (curly-curly).\n\nThis is called **Tidy Evaluation**.\n\n```r\n# BAD: group_var is not found\nmy_mean <- function(df, group_var, x) {\n  df |> group_by(group_var) |> summarize(mean(x))\n}\n\n# GOOD: Use {{ }} to \"tunnel\" into the dataframe\nmy_mean <- function(df, group_var, x) {\n  df |> \n    group_by({{ group_var }}) |> \n    summarize(mean({{ x }}))\n}\n```\n\n## Plot Functions\n\nWorks the same way for `aes()`!\n\n```r\nplot_hist <- function(df, var) {\n  df |> \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram()\n}\n```\n\n---\n\n## üéØ Your Task\n\nWrite a function `count_prop` that calculates counts and proportions for a grouped variable:\n\n1. Arguments: `df` and `var`\n2. embrace `var` inside `count()`\n3. Calculate prop as `n / sum(n)`\n\n> **Hint**: `count({{ var }})`\n",
    "starter_code": "# Write a function with tidy evaluation\ncount_prop <- function(df, var) {\n  df |>\n    count(...) |>\n    mutate(prop = n / sum(n))\n}\n",
    "solution_code": "count_prop <- function(df, var) {\n  df |>\n    count({{ var }}) |>\n    mutate(prop = n / sum(n))\n}",
    "expected_output": "[Function: count_prop]",
    "chapter_id": 25
  },
  "2410": {
    "id": 2410,
    "title": "Iteration: across & map",
    "chapter_title": "Ch 26: Iteration",
    "content": "# üîÅ Iteration: across & map\n\n## Modifying Multiple Columns (`across`)\n\nStop copy-pasting code! Use `across()` inside `mutate()` or `summarize()`.\n\n- **.cols**: Which columns? (e.g., `a:d`, `everything()`, `where(is.numeric)`)\n- **.fns**: What function? (e.g., `mean`, `\\(x) x * 2`)\n\n```r\ndf <- tibble(a = 1:10, b = 11:20, c = \"text\")\n\n# Calculate mean for ALL numeric columns\ndf |> summarize(across(where(is.numeric), mean))\n```\n\n## Reading Multiple Files (`map`)\n\nUse `map()` to apply a function to a list (like a list of filenames).\n\n```r\n# 1. Get file paths\npaths <- list.files(\"data\", pattern = \"\\\\.xlsx$\")\n\n# 2. Read all files into a list of dataframes\nfiles <- map(paths, read_excel)\n\n# 3. Combine into one big dataframe\ndf <- list_rbind(files)\n```\n\n## Saving Outputs (`walk`)\n\nUse `walk()` when you don't need a return value (like saving files).\n\n```r\n# Save a plot for each diamond cut\nwalk2(paths, plots, ggsave)\n```\n\n---\n\n## üéØ Your Task\n\nCalculate the **mean** of **all numeric columns** in the `diamonds` dataset, grouped by `cut`.\n\n1. `group_by(cut)`\n2. `summarize(...)`\n3. Inside summarize, use `across()`\n4. Select columns with `where(is.numeric)`\n5. Apply the `mean` function\n\n> **Hint**: `across(where(is.numeric), mean)`\n",
    "starter_code": "# Calculate mean of all numeric columns by cut\ndiamonds |>\n  group_by(cut) |>\n  summarize(across(...))\n",
    "solution_code": "diamonds |>\n  group_by(cut) |>\n  summarize(across(where(is.numeric), mean))",
    "expected_output": "[Tibble: Grouped means]",
    "chapter_id": 26
  },
  "2420": {
    "id": 2420,
    "title": "A Field Guide to Base R",
    "chapter_title": "Ch 27: Base R",
    "content": "# üèõÔ∏è A Field Guide to Base R\n\n## Subsetting: The Pepper Shaker üßÇ\n\nUnderstanding `[` vs `[[` is crucial.\n\n- `x[1]`: Returns a **list** containing the first item. (The shaker with one packet).\n- `x[[1]]`: Returns the **item itself**. (The packet content).\n- `x$name`: Same as `x[[\"name\"]]`, extracts by name.\n\n```r\nl <- list(a = 1:3, b = \"hello\")\nl[1]   # List of 1 ($a)\nl[[1]] # Integer vector (1 2 3)\nl$a    # Integer vector (1 2 3)\n```\n\n## Subsetting Data Frames\n\n- `df[rows, cols]`: Select rows and columns.\n- `df[df$x > 5, ]`: Filter rows where `x > 5` (basic filtering).\n- `df$col`: Extract column as vector.\n\n## The `apply` Family\n\n- `lapply(x, fun)`: Like `map()`, applies function to list/vector, returns list.\n- `sapply(x, fun)`: Tries to simplify result to vector (risky for programming).\n\n```r\nlapply(df, mean) # Returns list of means\n```\n\n## Loops\n\n`for` loops are foundational (and not inherently slow in R, growing vectors is!).\n\n```r\nfor (i in 1:5) {\n  print(i * 2)\n}\n```\n\n---\n\n## üéØ Your Task\n\nGiven the list `my_list <- list(a = 10, b = 20)`, extract the **value** of the first element using double brackets `[[ ]]`.\n",
    "starter_code": "my_list <- list(a = 10, b = 20)\n\n# Extract first element value\nmy_list...",
    "solution_code": "my_list[[1]]",
    "expected_output": "[1] 10",
    "chapter_id": 27
  },
  "2500": {
    "id": 2500,
    "title": "Quarto Basics",
    "chapter_title": "Ch 28: Quarto",
    "content": "# üìù Quarto Basics\n\n## Code + Prose = Magic\n\nQuarto (`.qmd`) unifies your code and narrative. It has three parts:\n\n### 1. YAML Header\nMetadata between `---` at the top.\n```yaml\n---\ntitle: \"My Report\"\nformat: html\neditor: visual\n---\n```\n\n### 2. Markdown Text\nWrite just like you do in standard markdown.\n- `**Bold**`, `*Italic*`\n- `# Headings`\n- `[Links](url)`\n- `![Images](path.png)`\n\n### 3. Code Chunks\nExecute R code and show results.\n```r\n#| label: plot-diamonds\n#| echo: false\n\nggplot(diamonds) + ...\n```\n\n## Special Chunk Options (`#|`)\n\n| Option | Effect |\n|--------|--------|\n| `eval: false` | Don't run code |\n| `echo: false` | Hide code, show output |\n| `include: false` | Run code, hide everything |\n| `message: false` | Hide messages |\n\n---\n\n## üéØ Your Task\n\nCreate a code chunk that calculates `1 + 1`, but **hides the code** from the final report (show only the output).\n\n1. Start with ````{r}`\n2. Add the option `#| echo: false`\n3. Add the code `1 + 1`\n4. End with ` ``` `\n",
    "starter_code": "```{r}\n#| ...\n1 + 1\n```",
    "solution_code": "```{r}\n#| echo: false\n1 + 1\n```",
    "expected_output": "[1] 2",
    "chapter_id": 28
  },
  "2510": {
    "id": 2510,
    "title": "Quarto Formats",
    "chapter_title": "Ch 29: Quarto Formats",
    "content": "# üìë Quarto Formats\n\n## Many Outputs, One Source\n\nChange the `format` in YAML to switch outputs.\n\n- **Documents**: `html`, `pdf`, `docx`\n- **Presentations**: `revealjs` (HTML slides), `pptx` (PowerPoint), `beamer` (PDF slides)\n\n```yaml\n---\ntitle: \"My Slides\"\nformat: revealjs\n---\n```\n\n## Customizing Output\n\nYou can add options under the format.\n\n```yaml\nformat:\n  html:\n    toc: true          # Table of contents\n    code-fold: true    # Hide code by default\n    theme: cosmo       # Color theme\n```\n\n## Interactivity\n\n- **htmlwidgets**: Interactive plots (Leaflet maps, Plotly) that work in static HTML.\n- **Shiny**: Server-side R logic for dynamic apps (`server: shiny`).\n\n## Websites & Books\n\nQuarto can build complex multi-page sites using `_quarto.yml` configuration (type: `website` or `book`).\n\n---\n\n## üéØ Your Task\n\nConfigure the YAML to produce a **PowerPoint** presentation with a **Table of Contents**.\n\n1. Set `format` to `pptx`.\n2. Add `toc: true` (indented under `pptx`).\n",
    "starter_code": "---\ntitle: \"My Presentation\"\nformat:\n  ...\n---",
    "solution_code": "---\ntitle: \"My Presentation\"\nformat:\n  pptx:\n    toc: true\n---",
    "expected_output": "Format: pptx (with TOC)",
    "chapter_id": 29
  },
  "20011": {
    "title": "Analogy: Variables are Boxes",
    "content": "# üì¶ Variables are Boxes\n\nImagine you have a moving box. You write \"Kitchen\" on the outside and put **plates** inside.\n\nIn R, the box name is the variable (`kitchen`) and the content is the value (`\"plates\"`).\n\n```r\nkitchen <- \"plates\"\n```\n\nNow whenever you ask for `kitchen`, R gives you `\"plates\"`.\n\n## üéØ Your Task\nCreate a box named `pet` and put `\"dog\"` inside.",
    "starter_code": "# Create the variable\npet <- ...",
    "solution_code": "pet <- \"dog\"",
    "expected_output": "[1] \"dog\"",
    "id": 20011,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20012": {
    "title": "Variation: Changing Values",
    "content": "# üîÑ Changing the Box Content\n\nYou can empty the box and put something else in. The old stuff is gone!\n\n```r\nscore <- 10\nscore <- 20 # Now score is 20\n```\n\n## üéØ Your Task\n1. Set `apples` to 5.\n2. Change `apples` to 10.",
    "starter_code": "apples <- 5\n# Change it to 10\napples <- ...",
    "solution_code": "apples <- 5\napples <- 10",
    "expected_output": "[1] 10",
    "id": 20012,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20013": {
    "title": "Fix the Code: Broken Arrow",
    "content": "# üîß Fix the Code\n\nThe assignment arrow `<-` needs to be together. It's not `< -` (less than minus)!\n\n## üéØ Your Task\nFix the broken arrow so the code runs.",
    "starter_code": "my_number < - 100",
    "solution_code": "my_number <- 100",
    "expected_output": "[1] 100",
    "id": 20013,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20014": {
    "title": "Challenge: Your Own Vars",
    "content": "# ü¶∏ Your Turn\n\nTime to pack your own boxes without help.\n\n## üéØ Your Task\nCreate a variable named `favorite_color` and assign it the text `\"blue\"` (or any color).",
    "starter_code": "# Write it from scratch!",
    "solution_code": "favorite_color <- \"blue\"",
    "expected_output": "[1] \"blue\"",
    "id": 20014,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20021": {
    "title": "Analogy: Secret Notes",
    "content": "# ü§´ Secret Notes\n\nComments (`#`) are like whispering to yourself. The computer can't hear you.\n\n```r\n1 + 1 # The computer calculates 2 and ignores this text\n```\n\n## üéØ Your Task\nRun the code below. The computer will ignore the comment.",
    "starter_code": "# This is a secret message\n10 + 10",
    "solution_code": "# This is a secret message\n10 + 10",
    "expected_output": "[1] 20",
    "id": 20021,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20022": {
    "title": "Variation: Silencing Code",
    "content": "# üîá Silencing Code\n\nYou can use comments to temporarily \"turn off\" a line of code without deleting it.\n\n## üéØ Your Task\nComment out the line that subtracts 50 so it doesn't happen.",
    "starter_code": "100 + 100\n# Comment out the next line:\n- 50",
    "solution_code": "100 + 100\n# - 50",
    "expected_output": "[1] 200",
    "id": 20022,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20023": {
    "title": "Fix the Code: Missing #",
    "content": "# üîß Fix the Code\n\nThe programmer wrote a note but forgot the `#`. R thinks it's code and is confused!\n\n## üéØ Your Task\nAdd a `#` before the text \"Calculate sum\".",
    "starter_code": "Calculate sum\n5 + 5",
    "solution_code": "# Calculate sum\n5 + 5",
    "expected_output": "[1] 10",
    "id": 20023,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20024": {
    "title": "Challenge: Annotate",
    "content": "# ü¶∏ Your Turn\n\nGood code explains itself.\n\n## üéØ Your Task\n1. Write a comment `# My calculation`.\n2. On the next line, calculate `10 * 10`.",
    "starter_code": "# Write your comment and code below\n",
    "solution_code": "# My calculation\n10 * 10",
    "expected_output": "[1] 100",
    "id": 20024,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20031": {
    "title": "Analogy: The Sticker Album",
    "content": "# üìì The Sticker Album\n\nA pandas DataFrame (or R Data Frame) is like a sticker album. It organizes your data into rows (stickers) and columns (features).\n\nIn this lesson we use `mpg`. It's a built-in album about cars.\n\n## üéØ Your Task\nJust type `mpg` to look at the album pages.",
    "starter_code": "# View the dataset\nmpg",
    "solution_code": "mpg",
    "expected_output": "tibble [234 x 11]",
    "id": 20031,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20032": {
    "title": "Variation: How Big?",
    "content": "# üìè How Big is the Algebra?\n\nYou can ask \"How many stickers?\" (`nrow`) or \"How many features?\" (`ncol`).\n\n## üéØ Your Task\nFind out how many rows (cars) are in `mpg` using `nrow(mpg)`.",
    "starter_code": "# Count the rows\n...",
    "solution_code": "nrow(mpg)",
    "expected_output": "[1] 234",
    "id": 20032,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20033": {
    "title": "Fix the Code: Typo",
    "content": "# üîß Fix the Code\n\nR is very specific about spelling. `Mpg` is not `mpg`.\n\n## üéØ Your Task\nFix the spelling to view the data.",
    "starter_code": "Mpg",
    "solution_code": "mpg",
    "expected_output": "tibble [234 x 11]",
    "id": 20033,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20034": {
    "title": "Challenge: Another Dataset",
    "content": "# ü¶∏ Your Turn\n\nThere is another dataset called `mtcars`. It's a classic.\n\n## üéØ Your Task\nPrint the `mtcars` dataset to the screen.",
    "starter_code": "",
    "solution_code": "mtcars",
    "expected_output": "Mazda RX4 ...",
    "id": 20034,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20041": {
    "title": "Analogy: Connect the Dots",
    "content": "# üé® Connect the Dots\n\n`ggplot()` is your canvas. `geom_point()` is the paint brush that makes dots.\n\n## üéØ Your Task\nMake a simple plot of Engine Size (`displ`) vs Highway MPG (`hwy`).\n\n> **Analogy**: It's like putting stickers on a graph paper where `x` is the horizontal line and `y` is the vertical line.",
    "starter_code": "ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))",
    "solution_code": "ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))",
    "expected_output": "[Plot]",
    "id": 20041,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20042": {
    "title": "Variation: City Driving",
    "content": "# üèôÔ∏è City Driving\n\nLet's change what we plot. Instead of Highway (`hwy`), let's see City (`cty`) mileage.\n\n## üéØ Your Task\nChange `y = hwy` to `y = cty`.",
    "starter_code": "ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = ...))",
    "solution_code": "ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = cty))",
    "expected_output": "[Plot]",
    "id": 20042,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20043": {
    "title": "Fix the Code: The Plus Sign",
    "content": "# üîß Fix the Code\n\nIn `ggplot`, layers are added with `+`. The plus sign must be at the **end** of the line, not the start of the next one!\n\n## üéØ Your Task\nFix the misplaced `+`.",
    "starter_code": "ggplot(data = mpg) \n+ geom_point(mapping = aes(x = displ, y = hwy))",
    "solution_code": "ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))",
    "expected_output": "[Plot]",
    "id": 20043,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20044": {
    "title": "Challenge: Cylinders",
    "content": "# ü¶∏ Your Turn\n\nPlot `cyl` (cylinders) on the x-axis and `hwy` on the y-axis.\n\n*(Don't forget the `+`)*",
    "starter_code": "ggplot(data = mpg) ...",
    "solution_code": "ggplot(data = mpg) + geom_point(aes(x = cyl, y = hwy))",
    "expected_output": "[Plot]",
    "id": 20044,
    "chapter_title": "Ch 1: Data Visualization",
    "chapter_id": 1
  },
  "20101": {
    "title": "Analogy: Calling for Pizza",
    "content": "# üçï Calling Functions\n\nUsing a function is like calling a pizza place. You give them inputs (toppings), and they give you output (pizza).\n\n```r\nseq(1, 10)\n```\n`seq` is the shop. `1` and `10` are the order details.\n\n## üéØ Your Task\nOrder a sequence from 1 to 5.",
    "starter_code": "# Call the function\nseq(..., ...)",
    "solution_code": "seq(1, 5)",
    "expected_output": "[1] 1 2 3 4 5",
    "id": 20101,
    "chapter_title": "Ch 2: Workflow: Basics",
    "chapter_id": 2
  },
  "20102": {
    "title": "Variation: Variables in Functions",
    "content": "# üì¶ Variables in Functions\n\nYou can put variables into functions too.\n\n```r\nx <- 10\nseq(1, x)\n```\n\n## üéØ Your Task\n1. set `y` to 20.\n2. Create a sequence from 10 to `y`.",
    "starter_code": "y <- ...\nseq(10, ...)",
    "solution_code": "y <- 20\nseq(10, y)",
    "expected_output": "[1] 10 ... 20",
    "id": 20102,
    "chapter_title": "Ch 2: Workflow: Basics",
    "chapter_id": 2
  },
  "20103": {
    "title": "Fix the Code: Typos",
    "content": "# üîß Fix the Code\n\nComputers are terrible at guessing. `seqq` is not `seq`.\n\n## üéØ Your Task\nFix the function name.",
    "starter_code": "seqq(1, 10)",
    "solution_code": "seq(1, 10)",
    "expected_output": "[1] 1 2 ... 10",
    "id": 20103,
    "chapter_title": "Ch 2: Workflow: Basics",
    "chapter_id": 2
  },
  "20104": {
    "title": "Challenge: Math",
    "content": "# ü¶∏ Your Turn\n\nLet's do some math with parenthesis.\n\n## ÔøΩÔøΩ Your Task\nCalculate `(5 + 5) * 2`.",
    "starter_code": "# Calculate it",
    "solution_code": "(5 + 5) * 2",
    "expected_output": "[1] 20",
    "id": 20104,
    "chapter_title": "Ch 2: Workflow: Basics",
    "chapter_id": 2
  },
  "20201": {
    "title": "Analogy: Collecting Red M&Ms",
    "content": "# üç¨ Collecting Red M&Ms\n\n`filter()` is like picking only the red M&Ms from the bowl.\n\n```r\nfilter(candy, color == \"red\")\n```\n\nWe use `==` (double equals) to check if it matches.\n\n## üéØ Your Task\nFilter `flights` for flights in January (`month == 1`).",
    "starter_code": "filter(flights, month == ...)",
    "solution_code": "filter(flights, month == 1)",
    "expected_output": "[Tibble: January flights]",
    "id": 20201,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20202": {
    "title": "Variation: Different Month",
    "content": "# üóìÔ∏è Different Month\n\nLet's try finding flights in December (`month == 12`).\n\n## üéØ Your Task\nFilter for December.",
    "starter_code": "filter(flights, ...)",
    "solution_code": "filter(flights, month == 12)",
    "expected_output": "[Tibble: December flights]",
    "id": 20202,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20203": {
    "title": "Fix the Code: Equals Sign",
    "content": "# üîß Fix the Code\n\nA common mistake: using `=` instead of `==` for comparisons.\n\n## üéØ Your Task\nFix the code to filter where day is 1.",
    "starter_code": "filter(flights, day = 1)",
    "solution_code": "filter(flights, day == 1)",
    "expected_output": "[Tibble: 1st of month]",
    "id": 20203,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20204": {
    "title": "Challenge: Delays",
    "content": "# ü¶∏ Your Turn\n\nFind distinct rows where `dep_delay` is greater than 120 (minutes).\n\nUse `>` (greater than).",
    "starter_code": "filter(flights, ...)",
    "solution_code": "filter(flights, dep_delay > 120)",
    "expected_output": "[Tibble: Delayed flights]",
    "id": 20204,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20211": {
    "title": "Analogy: Sorting Cards",
    "content": "# üÉè Sorting Cards\n\n`arrange()` puts rows in order, like sorting playing cards from Ace to King.\n\n```r\narrange(cards, rank)\n```\n\n## üéØ Your Task\nSort `flights` by `year`.",
    "starter_code": "arrange(flights, year)",
    "solution_code": "arrange(flights, year)",
    "expected_output": "[Tibble: Sorted]",
    "id": 20211,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20212": {
    "title": "Variation: Two Levels",
    "content": "# ü•à Two Levels\n\nSort by `month`, then by `day`.\n\n## üéØ Your Task\nArrange `flights` by `month` and `day`.",
    "starter_code": "arrange(flights, ..., ...)",
    "solution_code": "arrange(flights, month, day)",
    "expected_output": "[Tibble: Sorted]",
    "id": 20212,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20213": {
    "title": "Fix the Code: Missing Parenthesis",
    "content": "# üîß Fix the Code\n\nDon't forget to close the parenthesis!\n\n## üéØ Your Task\nFix the code.",
    "starter_code": "arrange(flights, dep_delay",
    "solution_code": "arrange(flights, dep_delay)",
    "expected_output": "[Tibble: Sorted]",
    "id": 20213,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20214": {
    "title": "Challenge: Backwards",
    "content": "# ÔøΩÔøΩ Your Turn\n\nUse `desc()` inside `arrange()` to sort backwards (biggest numbers first).\n\n## üéØ Your Task\nSort by `arr_delay` in descending order.",
    "starter_code": "arrange(flights, ...)",
    "solution_code": "arrange(flights, desc(arr_delay))",
    "expected_output": "[Tibble: Descending]",
    "id": 20214,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20221": {
    "title": "Analogy: Chopping Ingredients",
    "content": "# üî™ Chopping Ingredients\n\n`select()` picks which columns to keep and throws away the rest. It's like chopping veggies for a salad and keeping only the carrots.\n\n## üéØ Your Task\nKeep only the `year`, `month`, and `day` columns.",
    "starter_code": "select(flights, year, month, day)",
    "solution_code": "select(flights, year, month, day)",
    "expected_output": "[Tibble: 3 cols]",
    "id": 20221,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20222": {
    "title": "Variation: Excluding",
    "content": "# üóëÔ∏è Excluding\n\nPut a `-` (minus) before a column to throw it away.\n\n## üéØ Your Task\nRemove `year` from the dataset.",
    "starter_code": "select(flights, -year)",
    "solution_code": "select(flights, -year)",
    "expected_output": "[Tibble: No year]",
    "id": 20222,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20223": {
    "title": "Fix the Code: Comma",
    "content": "# üîß Fix the Code\n\nColumns verify values need commas between them.\n\n## üéØ Your Task\nFix the missing comma.",
    "starter_code": "select(flights, year month)",
    "solution_code": "select(flights, year, month)",
    "expected_output": "[Tibble: 2 cols]",
    "id": 20223,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20224": {
    "title": "Challenge: Range",
    "content": "# ü¶∏ Your Turn\n\nUse `:` to select a range of columns.\n\n## üéØ Your Task\nSelect columns from `year` to `day` (`year:day`).",
    "starter_code": "select(flights, ...)",
    "solution_code": "select(flights, year:day)",
    "expected_output": "[Tibble: Range]",
    "id": 20224,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20231": {
    "title": "Analogy: The Mutation Ray",
    "content": "# üß™ The Mutation Ray\n\n`mutate()` adds NEW columns based on old ones. It's like a mutation ray that gives your data superpowers.\n\n```r\nmutate(df, new_col = old_col * 2)\n```\n\n## üéØ Your Task\nCreate a new column `gain` that is `dep_delay - arr_delay`.",
    "starter_code": "mutate(flights, gain = ...)",
    "solution_code": "mutate(flights, gain = dep_delay - arr_delay)",
    "expected_output": "[Tibble: with gain]",
    "id": 20231,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20232": {
    "title": "Variation: Math",
    "content": "# ‚ûó Simple Math\n\nCreate a column `speed`.\n\n## üéØ Your Task\nSet `speed` to `distance / air_time * 60`.",
    "starter_code": "mutate(flights, speed = ...)",
    "solution_code": "mutate(flights, speed = distance / air_time * 60)",
    "expected_output": "[Tibble: with speed]",
    "id": 20232,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20233": {
    "title": "Fix the Code: Name It",
    "content": "# üîß Fix the Code\n\nYou MUST give the new column a name.\n\n## üéØ Your Task\nName the new column `hours`.",
    "starter_code": "mutate(flights, air_time / 60)",
    "solution_code": "mutate(flights, hours = air_time / 60)",
    "expected_output": "[Tibble: with hours]",
    "id": 20233,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20234": {
    "title": "Challenge: Double Up",
    "content": "# ü¶∏ Your Turn\n\nCreate a new column `double_delay` that is `dep_delay * 2`.",
    "starter_code": "mutate(flights, ...)",
    "solution_code": "mutate(flights, double_delay = dep_delay * 2)",
    "expected_output": "[Tibble: double_delay]",
    "id": 20234,
    "chapter_title": "Ch 3: Data Transformation",
    "chapter_id": 3
  },
  "20301": {
    "title": "Analogy: Grammar",
    "content": "# üìñ Grammar Matters\n\nCode is for humans to read. Put spaces around math operators.\n\n**Bad**: `1+1`\n**Good**: `1 + 1`\n\n## üéØ Your Task\nRewrite `x<-10` with proper spacing.",
    "starter_code": "x<-10",
    "solution_code": "x <- 10",
    "expected_output": "[1] 10",
    "id": 20301,
    "chapter_title": "Ch 4: Workflow: Code Style",
    "chapter_id": 4
  },
  "20302": {
    "title": "Variation: Pipes",
    "content": "# ü™à The Pipe\n\nUsing `|>` is like saying \"and then\".\n\n`x |> f()`\n\nPut the pipe at the end of the line.\n\n## üéØ Your Task\nFix the pipe structure.",
    "starter_code": "flights \n|> filter(year == 2013)",
    "solution_code": "flights |> \n  filter(year == 2013)",
    "expected_output": "[Tibble]",
    "id": 20302,
    "chapter_title": "Ch 4: Workflow: Code Style",
    "chapter_id": 4
  },
  "20303": {
    "title": "Fix the Code: Spacing",
    "content": "# üîß Fix the Code\n\nThis code looks squashed.\n\n## üéØ Your Task\nAdd spaces around `==` and `+`.",
    "starter_code": "filter(flights,month==1+1)",
    "solution_code": "filter(flights, month == 1 + 1)",
    "expected_output": "[Tibble]",
    "id": 20303,
    "chapter_title": "Ch 4: Workflow: Code Style",
    "chapter_id": 4
  },
  "20304": {
    "title": "Challenge: Naming",
    "content": "# ü¶∏ Your Turn\n\nUse snake_case for names. No `Caps` or `dots.`.\n\n## üéØ Your Task\nCreate a variable `my_cool_variable` sets to 1.",
    "starter_code": "",
    "solution_code": "my_cool_variable <- 1",
    "expected_output": "[1] 1",
    "id": 20304,
    "chapter_title": "Ch 4: Workflow: Code Style",
    "chapter_id": 4
  },
  "20401": {
    "title": "Analogy: Stacking Pancakes",
    "content": "# ü•û Stacking Pancakes\n\n`pivot_longer()` takes wide data (pancakes side-by-side) and stacks them into a tall pile.\n\n```r\npivot_longer(data, cols = c(1999, 2000))\n```\n\n## üéØ Your Task\nPivot the columns `1999` and `2000` into a longer format.",
    "starter_code": "pivot_longer(table4a, cols = ...)",
    "solution_code": "pivot_longer(table4a, cols = c(\"1999\", \"2000\"))",
    "expected_output": "[Tibble: Long format]",
    "id": 20401,
    "chapter_title": "Ch 5: Data Tidying",
    "chapter_id": 5
  },
  "20402": {
    "title": "Variation: Naming Columns",
    "content": "# üè∑Ô∏è Naming The Stack\n\nWhen you stack them, give the new columns names using `names_to` and `values_to`.\n\n## üéØ Your Task\nSet `names_to = \"year\"`.",
    "starter_code": "pivot_longer(table4a, cols = c(\"1999\", \"2000\"), names_to = ...)",
    "solution_code": "pivot_longer(table4a, cols = c(\"1999\", \"2000\"), names_to = \"year\")",
    "expected_output": "[Tibble: year column]",
    "id": 20402,
    "chapter_title": "Ch 5: Data Tidying",
    "chapter_id": 5
  },
  "20403": {
    "title": "Fix the Code: Missing Quote",
    "content": "# üîß Fix the Code\n\nColumn names generally need quotes if they are numbers (like \"1999\").\n\n## üéØ Your Task\nFix the code by adding quotes.",
    "starter_code": "pivot_longer(table4a, cols = c(1999, 2000))",
    "solution_code": "pivot_longer(table4a, cols = c(\"1999\", \"2000\"))",
    "expected_output": "[Tibble]",
    "id": 20403,
    "chapter_title": "Ch 5: Data Tidying",
    "chapter_id": 5
  },
  "20404": {
    "title": "Challenge: Pivot It",
    "content": "# ü¶∏ Your Turn\n\nPivot columns `c(x, y, z)` into longer format.\n\n## üéØ Your Task\nUse `pivot_longer` on `df` with columns `x`, `y`, `z`.",
    "starter_code": "pivot_longer(df, ...)",
    "solution_code": "pivot_longer(df, cols = c(x, y, z))",
    "expected_output": "[Tibble]",
    "id": 20404,
    "chapter_title": "Ch 5: Data Tidying",
    "chapter_id": 5
  },
  "20501": {
    "title": "Analogy: The Recipe Card",
    "content": "# üìú The Recipe Card\n\nA script (`.R` file) is like a recipe card. You can save it and cook (run) it later.\n\n## üéØ Your Task\nWrite code in a script to load the `tidyverse` library.",
    "starter_code": "# Load the library\nlibrary(...)",
    "solution_code": "library(tidyverse)",
    "expected_output": "[Attached packages]",
    "id": 20501,
    "chapter_title": "Ch 6: Workflow: Scripts",
    "chapter_id": 6
  },
  "20502": {
    "title": "Variation: Saving Plots",
    "content": "# üíæ Saving\n\nScripts let you save your work. Imagine this is a file `plot.R`.\n\n## üéØ Your Task\nRun the `ggplot` code provided.",
    "starter_code": "ggplot(mpg, aes(hwy, cty)) + geom_point()",
    "solution_code": "ggplot(mpg, aes(hwy, cty)) + geom_point()",
    "expected_output": "[Plot]",
    "id": 20502,
    "chapter_title": "Ch 6: Workflow: Scripts",
    "chapter_id": 6
  },
  "20503": {
    "title": "Fix the Code: Typos",
    "content": "# üîß Fix the Code\n\n`libary` is spelled `library`.\n\n## üéØ Your Task\nFix the typo.",
    "starter_code": "libary(dplyr)",
    "solution_code": "library(dplyr)",
    "expected_output": "[Attached]",
    "id": 20503,
    "chapter_title": "Ch 6: Workflow: Scripts",
    "chapter_id": 6
  },
  "20504": {
    "title": "Challenge: Comments",
    "content": "# ü¶∏ Your Turn\n\nWrite a header comment for your script.\n\n## üéØ Your Task\nWrite `# Analysis of Cars`.",
    "starter_code": "",
    "solution_code": "# Analysis of Cars",
    "expected_output": "",
    "id": 20504,
    "chapter_title": "Ch 6: Workflow: Scripts",
    "chapter_id": 6
  },
  "20601": {
    "title": "Analogy: Opening Packages",
    "content": "# üì¶ Opening Packages\n\n`read_csv()` is like opening a package you received in the mail. It takes a file and opens it in R.\n\n```r\nread_csv(\"data.csv\")\n```\n\n## üéØ Your Task\nRead the file `\"students.csv\"`.",
    "starter_code": "read_csv(\"...\")",
    "solution_code": "read_csv(\"students.csv\")",
    "expected_output": "[Tibble]",
    "id": 20601,
    "chapter_title": "Ch 7: Data Import",
    "chapter_id": 7
  },
  "20602": {
    "title": "Variation: Inspecting",
    "content": "# üîç Inspecting\n\nAfter opening, look effectively. `glimpse()` is a quick peek inside.\n\n## üéØ Your Task\nGlimpse the `students` data.",
    "starter_code": "glimpse(students)",
    "solution_code": "glimpse(students)",
    "expected_output": "Rows: X Cols: Y",
    "id": 20602,
    "chapter_title": "Ch 7: Data Import",
    "chapter_id": 7
  },
  "20603": {
    "title": "Fix the Code: Function Name",
    "content": "# üîß Fix the Code\n\nIt is `read_csv` (snake_case), not `read.csv` (dot case, which is old base R). Use the tidyverse version!\n\n## üéØ Your Task\nChange `read.csv` to `read_csv`.",
    "starter_code": "read.csv(\"data.csv\")",
    "solution_code": "read_csv(\"data.csv\")",
    "expected_output": "[Tibble]",
    "id": 20603,
    "chapter_title": "Ch 7: Data Import",
    "chapter_id": 7
  },
  "20604": {
    "title": "Challenge: Assigning",
    "content": "# ü¶∏ Your Turn\n\nRead `\"pets.csv\"` and save it to a variable named `pets`.\n\n## üéØ Your Task\nAssign the result to `pets`.",
    "starter_code": "",
    "solution_code": "pets <- read_csv(\"pets.csv\")",
    "expected_output": "[Tibble]",
    "id": 20604,
    "chapter_title": "Ch 7: Data Import",
    "chapter_id": 7
  },
  "21001": {
    "title": "Analogy: Coloring Book",
    "content": "# üñçÔ∏è Coloring Book\n\n`aes(color = variable)` tells R to color the dots based on a column. It's like coloring all 'SUV' dots red and 'Compact' dots blue.\n\n## üéØ Your Task\nMap `color` to `class` in the plot.",
    "starter_code": "ggplot(mpg, aes(x = displ, y = hwy, color = ...)) + geom_point()",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()",
    "expected_output": "[Plot with colors]",
    "id": 21001,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21002": {
    "title": "Variation: Size",
    "content": "# üéà Size Matters\n\nYou can also change the `size` of dots based on data.\n\n## üéØ Your Task\nMap `size` to `cyl` (cylinders). Bigger engine = bigger dot.",
    "starter_code": "ggplot(mpg, aes(x = displ, y = hwy, size = ...)) + geom_point()",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy, size = cyl)) + geom_point()",
    "expected_output": "[Plot with sizes]",
    "id": 21002,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21003": {
    "title": "Fix the Code: Inside or Out?",
    "content": "# üîß Fix the Code\n\nIf you want ALL dots blue, put `color=\"blue\"` **outside** `aes()`. If you want color based on data, put it **inside** `aes()`.\n\nThis code tries to make them all blue but fails because it's inside `aes()`.\n\n## üéØ Your Task\nMove `color = \"blue\"` outside of `aes()` closing parenthesis.",
    "starter_code": "ggplot(mpg) + geom_point(aes(x = displ, y = hwy, color = \"blue\"))",
    "solution_code": "ggplot(mpg) + geom_point(aes(x = displ, y = hwy), color = \"blue\")",
    "expected_output": "[Blue dots]",
    "id": 21003,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21004": {
    "title": "Challenge: Shape",
    "content": "# ü¶∏ Your Turn\n\nMap the `shape` aesthetics to `drv` (drive type).\n\n## üéØ Your Task\nCreate a plot with `x=displ`, `y=hwy`, and `shape=drv`.",
    "starter_code": "ggplot(mpg, ...) + ...",
    "solution_code": "ggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point()",
    "expected_output": "[Plot with shapes]",
    "id": 21004,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21011": {
    "title": "Analogy: Different Brushes",
    "content": "# üñåÔ∏è Different Brushes\n\n`geom_point` is a dot brush. `geom_smooth` is a smooth line brush. You can switch brushes!\n\n## üéØ Your Task\nUse `geom_smooth()` instead of `geom_point()`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + ...",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_smooth()",
    "expected_output": "[Smooth Line]",
    "id": 21011,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21012": {
    "title": "Variation: Two Layers",
    "content": "# ü•û Layering\n\nYou can use BOTH brushes. Just add them together with `+`.\n\n## üéØ Your Task\nAdd `geom_point()` AND `geom_smooth()`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  ...",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth()",
    "expected_output": "[Dots + Line]",
    "id": 21012,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21013": {
    "title": "Fix the Code: Typos",
    "content": "# üîß Fix the Code\n\nIt's `geom`, short for geometry.\n\n## üéØ Your Task\nFix the typo `geometry_point`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + geometry_point()",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point()",
    "expected_output": "[Plot]",
    "id": 21013,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21014": {
    "title": "Challenge: Boxplot",
    "content": "# ü¶∏ Your Turn\n\nUse `geom_boxplot()` to show distribution. Map `x = class` and `y = hwy`.\n\n## ÔøΩÔøΩ Your Task\nCreate a boxplot.",
    "starter_code": "",
    "solution_code": "ggplot(mpg, aes(class, hwy)) + geom_boxplot()",
    "expected_output": "[Boxplot]",
    "id": 21014,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21021": {
    "title": "Analogy: Small Multiples",
    "content": "# üñºÔ∏è Small Multiples\n\n`facet_wrap(~ variable)` splits your one big plot into many small plots, one for each category.\n\n## üéØ Your Task\nFacet by `class`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  facet_wrap(~ ...)",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~ class)",
    "expected_output": "[Facetted Plot]",
    "id": 21021,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21022": {
    "title": "Variation: Row Count",
    "content": "# üî¢ Rows\n\nYou can control how many rows of plots you get with `nrow`.\n\n## üéØ Your Task\nFacet by `class` with `nrow = 2`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  facet_wrap(~ class, nrow = ...)",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~ class, nrow = 2)",
    "expected_output": "[2 Rows]",
    "id": 21022,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21023": {
    "title": "Fix the Code: Tilde",
    "content": "# üîß Fix the Code\n\nFacets need a tilde `~` before the variable name. It means \"by\".\n\n## üéØ Your Task\nAdd the missing `~`.",
    "starter_code": "facet_wrap(class)",
    "solution_code": "facet_wrap(~ class)",
    "expected_output": "[Plot]",
    "id": 21023,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21024": {
    "title": "Challenge: Grid",
    "content": "# ü¶∏ Your Turn\n\n`facet_grid(row ~ col)` makes a matrix of plots.\n\n## üéØ Your Task\nFacet grid with `drv` as rows and `cyl` as columns.",
    "starter_code": "",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_grid(drv ~ cyl)",
    "expected_output": "[Grid Plot]",
    "id": 21024,
    "chapter_title": "Ch 9: Layers",
    "chapter_id": 9
  },
  "21101": {
    "title": "Analogy: Sorting Buckets",
    "content": "# üóëÔ∏è Sorting Buckets\n\nA histogram works by creating buckets (bins) and dropping values into them. It shows you the shape of your data.\n\n## üéØ Your Task\nCreate a histogram of `hwy`.",
    "starter_code": "ggplot(mpg, aes(hwy)) + geom_histogram()",
    "solution_code": "ggplot(mpg, aes(hwy)) + geom_histogram()",
    "expected_output": "[Histogram]",
    "id": 21101,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21102": {
    "title": "Variation: Binwidth",
    "content": "# üìè Bin Width\n\nYou can make the buckets wider or narrower.\n\n## üéØ Your Task\nSet `binwidth` to 5.",
    "starter_code": "ggplot(mpg, aes(hwy)) + geom_histogram(binwidth = ...)",
    "solution_code": "ggplot(mpg, aes(hwy)) + geom_histogram(binwidth = 5)",
    "expected_output": "[Plot]",
    "id": 21102,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21103": {
    "title": "Fix the Code: Y Axis",
    "content": "# üîß Fix the Code\n\nHistograms only need an `x`. `y` is calculated automatically (count).\n\n## üéØ Your Task\nRemove `y = cty` from the code.",
    "starter_code": "ggplot(mpg, aes(x = hwy, y = cty)) + geom_histogram()",
    "solution_code": "ggplot(mpg, aes(x = hwy)) + geom_histogram()",
    "expected_output": "[Histogram]",
    "id": 21103,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21104": {
    "title": "Challenge: Bar Chart",
    "content": "# ü¶∏ Your Turn\n\nFor categories (like `class`), use `geom_bar()` instead of histogram.\n\n## üéØ Your Task\nCreate a bar chart of `class`.",
    "starter_code": "",
    "solution_code": "ggplot(mpg, aes(class)) + geom_bar()",
    "expected_output": "[Bar Chart]",
    "id": 21104,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21111": {
    "title": "Analogy: Box and Whiskers",
    "content": "# üì¶ Box and Whiskers\n\nA boxplot summarizes a distribution with 5 numbers: min, q1, median, q3, max. It's great for comparing categories.\n\n## üéØ Your Task\nPlot `class` vs `hwy` using `geom_boxplot`.",
    "starter_code": "ggplot(mpg, aes(class, hwy)) + ...",
    "solution_code": "ggplot(mpg, aes(class, hwy)) + geom_boxplot()",
    "expected_output": "[Boxplot]",
    "id": 21111,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21112": {
    "title": "Variation: Reorder",
    "content": "# ‚ÜîÔ∏è Reordering\n\nBoxplots are easier to read if sorted. Use `reorder(class, hwy)`.\n\n## üéØ Your Task\nReorder the x-axis.",
    "starter_code": "ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_boxplot()",
    "solution_code": "ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_boxplot()",
    "expected_output": "[Sorted Boxplot]",
    "id": 21112,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21113": {
    "title": "Fix the Code: Flip",
    "content": "# üîß Fix the Code\n\nYou can flip coordinates to make labels readable using `coord_flip()`.\n\n## üéØ Your Task\nAdd `coord_flip()`.",
    "starter_code": "ggplot(mpg, aes(class, hwy)) + geom_boxplot() + ...",
    "solution_code": "ggplot(mpg, aes(class, hwy)) + geom_boxplot() + coord_flip()",
    "expected_output": "[Flipped Plot]",
    "id": 21113,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21114": {
    "title": "Challenge: Heatmap",
    "content": "# ü¶∏ Your Turn\n\nUse `geom_bin2d()` to see where points overlap (counts).\n\n## üéØ Your Task\nPlot `hwy` vs `cty` with `geom_bin2d()`.",
    "starter_code": "",
    "solution_code": "ggplot(mpg, aes(hwy, cty)) + geom_bin2d()",
    "expected_output": "[Heatmap]",
    "id": 21114,
    "chapter_title": "Ch 10: EDA",
    "chapter_id": 10
  },
  "21201": {
    "title": "Analogy: Name Tags",
    "content": "# üè∑Ô∏è Name Tags\n\nGood plots assume the reader knows nothing. Use `labs()` to add titles and labels.\n\n## üéØ Your Task\nAdd a title \"Fuel Efficiency\".",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  labs(title = \"...\")",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + labs(title = \"Fuel Efficiency\")",
    "expected_output": "[Plot with Title]",
    "id": 21201,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21202": {
    "title": "Variation: Axis Labels",
    "content": "# üìè Axis Labels\n\nChange `x` and `y` labels to be descriptive.\n\n## üéØ Your Task\nSet `x` to \"Engine Size\" and `y` to \"MPG\".",
    "starter_code": "labs(x = \"...\", y = \"...\")",
    "solution_code": "labs(x = \"Engine Size\", y = \"MPG\")",
    "expected_output": "[Plot with Axis Labels]",
    "id": 21202,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21203": {
    "title": "Fix the Code: Quotes",
    "content": "# üîß Fix the Code\n\nText in titles must be in quotes.\n\n## üéØ Your Task\nFix the title.",
    "starter_code": "labs(title = My Plot)",
    "solution_code": "labs(title = \"My Plot\")",
    "expected_output": "[Plot]",
    "id": 21203,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21204": {
    "title": "Challenge: Caption",
    "content": "# ü¶∏ Your Turn\n\nAdd a `caption` finding \"Data from 1999-2008\" at the bottom right.\n\n## üéØ Your Task\nAdd a caption.",
    "starter_code": "",
    "solution_code": "labs(caption = \"Data from 1999-2008\")",
    "expected_output": "[Plot with Caption]",
    "id": 21204,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21211": {
    "title": "Analogy: Changing Outfits",
    "content": "# üëî Changing Outfits\n\nThemes change the look of the non-data parts (background, grid lines). `theme_minimal()` is a clean, modern look.\n\n## üéØ Your Task\nAdd `theme_minimal()`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + ...",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_minimal()",
    "expected_output": "[Minimal Plot]",
    "id": 21211,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21212": {
    "title": "Variation: Classic",
    "content": "# üèõÔ∏è Classic Look\n\n`theme_classic()` looks like a traditional scientific plot (no grid).\n\n## üéØ Your Task\nUse `theme_classic()`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + ...",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_classic()",
    "expected_output": "[Classic Plot]",
    "id": 21212,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21213": {
    "title": "Fix the Code: Parentheses",
    "content": "# üîß Fix the Code\n\nThemes are functions, they need `()`.\n\n## üéØ Your Task\nFix `theme_bw`.",
    "starter_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_bw",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_bw()",
    "expected_output": "[BW Plot]",
    "id": 21213,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "21214": {
    "title": "Challenge: Dark Mode",
    "content": "# ü¶∏ Your Turn\n\nUse `theme_dark()` for a dark background.\n\n## üéØ Your Task\nApply the dark theme.",
    "starter_code": "",
    "solution_code": "ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_dark()",
    "expected_output": "[Dark Plot]",
    "id": 21214,
    "chapter_title": "Ch 11: Communication",
    "chapter_id": 11
  },
  "22001": {
    "title": "Analogy: The Light Switch",
    "content": "# üí° The Light Switch\n\nA Boolean is like a light switch: it's either **ON** (`TRUE`) or **OFF** (`FALSE`).\n\n```r\nx <- TRUE\ny <- FALSE\n```\n\n## üéØ Your Task\nSet `lights_on` to `TRUE`.",
    "starter_code": "lights_on <- ...",
    "solution_code": "lights_on <- TRUE",
    "expected_output": "[1] TRUE",
    "id": 22001,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22002": {
    "title": "Variation: Comparisons",
    "content": "# ‚öñÔ∏è Comparisons\n\nYou get Booleans by asking questions. \"Is 10 greater than 5?\"\n\n```r\n10 > 5 # TRUE\n```\n\n## üéØ Your Task\nCheck if 1 + 1 equals 2 using `==`.",
    "starter_code": "1 + 1 ... 2",
    "solution_code": "1 + 1 == 2",
    "expected_output": "[1] TRUE",
    "id": 22002,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22003": {
    "title": "Fix the Code: Case Sensitive",
    "content": "# üîß Fix the Code\n\nR knows `TRUE`, not `True`. It yells if you don't shout.\n\n## üéØ Your Task\nFix `True`.",
    "starter_code": "x <- True",
    "solution_code": "x <- TRUE",
    "expected_output": "[1] TRUE",
    "id": 22003,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22004": {
    "title": "Challenge: Not Equal",
    "content": "# ü¶∏ Your Turn\n\nUse `!=` to check if \"cat\" is NOT equal to \"dog\".\n\n## üéØ Your Task\nCheck inequality.",
    "starter_code": "",
    "solution_code": "\"cat\" != \"dog\"",
    "expected_output": "[1] TRUE",
    "id": 22004,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22011": {
    "title": "Analogy: Decisions",
    "content": "# üö¶ Decisions\n\nSometimes you need TWO things to be true. logic `&` (AND) means both must be true.\n\n`TRUE & TRUE` is `TRUE`.\n`TRUE & FALSE` is `FALSE`.\n\n## üéØ Your Task\nCheck if `1 == 1` AND `2 == 2`.",
    "starter_code": "1 == 1 ... 2 == 2",
    "solution_code": "1 == 1 & 2 == 2",
    "expected_output": "[1] TRUE",
    "id": 22011,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22012": {
    "title": "Variation: Or",
    "content": "# ü§∑ Either Or\n\nLogic `|` (OR) means AT LEAST ONE must be true.\n\n`TRUE | FALSE` is `TRUE`.\n\n## üéØ Your Task\nCheck if `1 == 1` OR `1 == 5`.",
    "starter_code": "1 == 1 ... 1 == 5",
    "solution_code": "1 == 1 | 1 == 5",
    "expected_output": "[1] TRUE",
    "id": 22012,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22013": {
    "title": "Fix the Code: Not Python",
    "content": "# üîß Fix the Code\n\nR uses symbols `&`, `|`, not words `and`, `or`.\n\n## üéØ Your Task\nFix the code.",
    "starter_code": "TRUE and TRUE",
    "solution_code": "TRUE & TRUE",
    "expected_output": "[1] TRUE",
    "id": 22013,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22014": {
    "title": "Challenge: Negation",
    "content": "# ü¶∏ Your Turn\n\n`!` means NOT. `!TRUE` is `FALSE`.\n\n## üéØ Your Task\nCheck if NOT `FALSE`.",
    "starter_code": "",
    "solution_code": "!FALSE",
    "expected_output": "[1] TRUE",
    "id": 22014,
    "chapter_title": "Ch 12: Logical Vectors",
    "chapter_id": 12
  },
  "22101": {
    "title": "Analogy: Counting Heads",
    "content": "# üó£Ô∏è Counting Heads\n\n`count()` counts how many rows fall into each category. Like counting how many people want Pizza vs Tacos.\n\n## üéØ Your Task\nCount the `cut` of diamonds.",
    "starter_code": "count(diamonds, ...)",
    "solution_code": "count(diamonds, cut)",
    "expected_output": "[Tibble: counts]",
    "id": 22101,
    "chapter_title": "Ch 13: Numbers",
    "chapter_id": 13
  },
  "22102": {
    "title": "Variation: Summarize",
    "content": "# üìù Summarize\n\n`summarize()` squashes data into one number (like the average).\n\n## üéØ Your Task\nCalculate the `mean` of `price`.",
    "starter_code": "summarize(diamonds, avg_price = mean(...))",
    "solution_code": "summarize(diamonds, avg_price = mean(price))",
    "expected_output": "[Tibble]",
    "id": 22102,
    "chapter_title": "Ch 13: Numbers",
    "chapter_id": 13
  },
  "22103": {
    "title": "Fix the Code: Parentheses",
    "content": "# üîß Fix the Code\n\nFunctions need `()`.\n\n## üéØ Your Task\nFix `n`.",
    "starter_code": "summarize(diamonds, total = n)",
    "solution_code": "summarize(diamonds, total = n())",
    "expected_output": "[Tibble]",
    "id": 22103,
    "chapter_title": "Ch 13: Numbers",
    "chapter_id": 13
  },
  "22104": {
    "title": "Challenge: Max",
    "content": "# ü¶∏ Your Turn\n\nFind the `max()` price.\n\n## üéØ Your Task\nSummarize max price.",
    "starter_code": "",
    "solution_code": "summarize(diamonds, max_price = max(price))",
    "expected_output": "[Tibble]",
    "id": 22104,
    "chapter_title": "Ch 13: Numbers",
    "chapter_id": 13
  },
  "22201": {
    "title": "Analogy: Glue Stick",
    "content": "# üß¥ Glue Stick\n\n`str_glue()` sticks text and variables together easily. Use `{variable}` to insert values.\n\n```r\nname <- \"Bond\"\nstr_glue(\"James {name}\")\n```\n\n## üéØ Your Task\nGlue \"Value: \" with `x`.",
    "starter_code": "x <- 100\nstr_glue(\"Value: ...\")",
    "solution_code": "x <- 100\nstr_glue(\"Value: {x}\")",
    "expected_output": "Value: 100",
    "id": 22201,
    "chapter_title": "Ch 14: Strings",
    "chapter_id": 14
  },
  "22202": {
    "title": "Variation: Combining",
    "content": "# üîó Combining Strings\n\n`str_c()` joins strings directly.\n\n## üéØ Your Task\nCombine \"A\" and \"B\".",
    "starter_code": "str_c(\"A\", ...)",
    "solution_code": "str_c(\"A\", \"B\")",
    "expected_output": "[1] \"AB\"",
    "id": 22202,
    "chapter_title": "Ch 14: Strings",
    "chapter_id": 14
  },
  "22203": {
    "title": "Fix the Code: Spelling",
    "content": "# üîß Fix the Code\n\nIt's `str`, short for string.\n\n## üéØ Your Task\nFix `string_length`.",
    "starter_code": "string_length(\"abc\")",
    "solution_code": "str_length(\"abc\")",
    "expected_output": "[1] 3",
    "id": 22203,
    "chapter_title": "Ch 14: Strings",
    "chapter_id": 14
  },
  "22204": {
    "title": "Challenge: Substring",
    "content": "# ü¶∏ Your Turn\n\nGet the first letter using `str_sub(text, 1, 1)`.\n\n## üéØ Your Task\nGet 'H' from 'Hello'.",
    "starter_code": "",
    "solution_code": "str_sub(\"Hello\", 1, 1)",
    "expected_output": "[1] \"H\"",
    "id": 22204,
    "chapter_title": "Ch 14: Strings",
    "chapter_id": 14
  },
  "22301": {
    "title": "Analogy: Ctrl+F",
    "content": "# üîç Ctrl+F\n\n`str_detect` returns TRUE if it finds the pattern. It's like finding a needle in a haystack.\n\n## üéØ Your Task\nDoes \"banana\" contain \"ana\"?",
    "starter_code": "str_detect(\"banana\", ...)",
    "solution_code": "str_detect(\"banana\", \"ana\")",
    "expected_output": "[1] TRUE",
    "id": 22301,
    "chapter_title": "Ch 15: Regex",
    "chapter_id": 15
  },
  "22302": {
    "title": "Variation: Start Anchor",
    "content": "# ‚öì Start Anchor\n\n`^` matches the START of a string.\n\n`^a` matches \"apple\" but not \"banana\".\n\n## üéØ Your Task\nCheck if \"apple\" starts with \"a\".",
    "starter_code": "str_detect(\"apple\", \"^a\")",
    "solution_code": "str_detect(\"apple\", \"^a\")",
    "expected_output": "[1] TRUE",
    "id": 22302,
    "chapter_title": "Ch 15: Regex",
    "chapter_id": 15
  },
  "22303": {
    "title": "Fix the Code: Characters",
    "content": "# üîß Fix the Code\n\nRegex patterns need quotes.\n\n## üéØ Your Task\nFix the pattern.",
    "starter_code": "str_detect(\"abc\", a)",
    "solution_code": "str_detect(\"abc\", \"a\")",
    "expected_output": "[1] TRUE",
    "id": 22303,
    "chapter_title": "Ch 15: Regex",
    "chapter_id": 15
  },
  "22304": {
    "title": "Challenge: Digits",
    "content": "# ü¶∏ Your Turn\n\n`\\d` matches any digit (0-9). In R strings, type it as `\\\\d`.\n\n## üéØ Your Task\nDetect a digit in \"R2D2\".",
    "starter_code": "",
    "solution_code": "str_detect(\"R2D2\", \"\\\\d\")",
    "expected_output": "[1] TRUE",
    "id": 22304,
    "chapter_title": "Ch 15: Regex",
    "chapter_id": 15
  },
  "22401": {
    "title": "Analogy: Sorting Shirts",
    "content": "# üëï Sorting Shirts\n\nFactors are for categorical data (Small, Medium, Large). They have a specific order, unlike plain text.\n\n## üéØ Your Task\nCreate a factor of sizes.",
    "starter_code": "factor(c(\"S\", \"M\", \"L\"), levels = ...)",
    "solution_code": "factor(c(\"S\", \"M\", \"L\"), levels = c(\"S\", \"M\", \"L\"))",
    "expected_output": "[Factor]",
    "id": 22401,
    "chapter_title": "Ch 16: Factors",
    "chapter_id": 16
  },
  "22402": {
    "title": "Variation: Reordering",
    "content": "# üìä Reordering Plots\n\nUse `fct_reorder(cat, numeric)` to sort categories by a value.\n\n## üéØ Your Task\nReorder `class` by `hwy`.",
    "starter_code": "fct_reorder(class, ...)",
    "solution_code": "fct_reorder(class, hwy)",
    "expected_output": "[Factor]",
    "id": 22402,
    "chapter_title": "Ch 16: Factors",
    "chapter_id": 16
  },
  "22403": {
    "title": "Fix the Code: Levels",
    "content": "# üîß Fix the Code\n\nLevels must match exactly.\n\n## üéØ Your Task\nFix the typo in levels.",
    "starter_code": "factor(\"A\", levels = c(\"B\"))",
    "solution_code": "factor(\"A\", levels = c(\"A\", \"B\"))",
    "expected_output": "[Factor]",
    "id": 22403,
    "chapter_title": "Ch 16: Factors",
    "chapter_id": 16
  },
  "22404": {
    "title": "Challenge: Count",
    "content": "# ü¶∏ Your Turn\n\nCount the factors using `fct_count()`.\n\n## üéØ Your Task\nCount `f`.",
    "starter_code": "f <- factor(\"a\")\n...",
    "solution_code": "fct_count(f)",
    "expected_output": "[Tibble]",
    "id": 22404,
    "chapter_title": "Ch 16: Factors",
    "chapter_id": 16
  },
  "22501": {
    "title": "Analogy: ISO Standard",
    "content": "# üìÖ ISO Standard\n\nDates are messy. `lubridate` makes them standard. `ymd()` parses \"2023-01-01\".\n\n## üéØ Your Task\nParse \"2023-01-01\".",
    "starter_code": "ymd(\"...\")",
    "solution_code": "ymd(\"2023-01-01\")",
    "expected_output": "[Date]",
    "id": 22501,
    "chapter_title": "Ch 17: Dates and Times",
    "chapter_id": 17
  },
  "22502": {
    "title": "Variation: MDY",
    "content": "# üá∫üá∏ MDY Format\n\nIf the date is \"January 1st, 2023\", use `mdy()`.\n\n## üéØ Your Task\nParse \"01-31-2023\".",
    "starter_code": "mdy(\"...\")",
    "solution_code": "mdy(\"01-31-2023\")",
    "expected_output": "[Date]",
    "id": 22502,
    "chapter_title": "Ch 17: Dates and Times",
    "chapter_id": 17
  },
  "22503": {
    "title": "Fix the Code: Quote",
    "content": "# üîß Fix the Code\n\nDates are strings until parsed.\n\n## üéØ Your Task\nAdd quotes.",
    "starter_code": "ymd(2023-01-01)",
    "solution_code": "ymd(\"2023-01-01\")",
    "expected_output": "[Date]",
    "id": 22503,
    "chapter_title": "Ch 17: Dates and Times",
    "chapter_id": 17
  },
  "22504": {
    "title": "Challenge: Get Year",
    "content": "# ü¶∏ Your Turn\n\nExtract year with `year(date)`.\n\n## üéØ Your Task\nGet year from today.",
    "starter_code": "",
    "solution_code": "year(today())",
    "expected_output": "[Year]",
    "id": 22504,
    "chapter_title": "Ch 17: Dates and Times",
    "chapter_id": 17
  },
  "22601": {
    "title": "Analogy: Empty Seat",
    "content": "# ü™ë Empty Seat\n\n`NA` means \"Not Available\". It's like an empty seat at a dinner party. Someone was supposed to be there, but isn't.\n\n## üéØ Your Task\nCheck for NAs with `is.na()`.",
    "starter_code": "is.na(NA)",
    "solution_code": "is.na(NA)",
    "expected_output": "[TRUE]",
    "id": 22601,
    "chapter_title": "Ch 18: Missing Values",
    "chapter_id": 18
  },
  "22602": {
    "title": "Variation: Replacing",
    "content": "# ü©π Replacing\n\n`replace_na()` fills the empty seats.\n\n## üéØ Your Task\nReplace NA with 0.",
    "starter_code": "replace_na(c(1, NA), ...)",
    "solution_code": "replace_na(c(1, NA), 0)",
    "expected_output": "[1, 0]",
    "id": 22602,
    "chapter_title": "Ch 18: Missing Values",
    "chapter_id": 18
  },
  "22603": {
    "title": "Fix the Code: Comparison",
    "content": "# üîß Fix the Code\n\nYou cannot do `x == NA`. Use `is.na(x)`.\n\n## üéØ Your Task\nFix the comparison.",
    "starter_code": "x == NA",
    "solution_code": "is.na(x)",
    "expected_output": "[Logical]",
    "id": 22603,
    "chapter_title": "Ch 18: Missing Values",
    "chapter_id": 18
  },
  "22604": {
    "title": "Challenge: Drop",
    "content": "# ü¶∏ Your Turn\n\n`drop_na()` removes rows with missing values.\n\n## üéØ Your Task\nDrop NAs from `df`.",
    "starter_code": "",
    "solution_code": "drop_na(df)",
    "expected_output": "[Tibble]",
    "id": 22604,
    "chapter_title": "Ch 18: Missing Values",
    "chapter_id": 18
  },
  "22701": {
    "title": "Analogy: Identification Card",
    "content": "# üÜî Identification Card\n\nTo join tables, you need a key (ID). It's like checking an ID card to match a person to a record.\n\n## üéØ Your Task\nJoin `flights` and `airlines` by `carrier`.",
    "starter_code": "left_join(flights, airlines, by = \"...\")",
    "solution_code": "left_join(flights, airlines, by = \"carrier\")",
    "expected_output": "[Joined Tibble]",
    "id": 22701,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22702": {
    "title": "Variation: Right Join",
    "content": "# ‚û°Ô∏è Right Join\n\n`right_join` keeps everything from the second table.\n\n## üéØ Your Task\nRight join airlines to flights.",
    "starter_code": "right_join(flights, airlines)",
    "solution_code": "right_join(flights, airlines)",
    "expected_output": "[Joined Tibble]",
    "id": 22702,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22703": {
    "title": "Fix the Code: By Argument",
    "content": "# üîß Fix the Code\n\n`by` expects a string.\n\n## üéØ Your Task\nAdd quotes to `origin`.",
    "starter_code": "left_join(flights, airports, by = origin)",
    "solution_code": "left_join(flights, airports, by = \"origin\")",
    "expected_output": "[Tibble]",
    "id": 22703,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22704": {
    "title": "Challenge: Inner Join",
    "content": "# ü¶∏ Your Turn\n\n`inner_join` keeps only matches.\n\n## üéØ Your Task\nInner join matches.",
    "starter_code": "",
    "solution_code": "inner_join(x, y)",
    "expected_output": "[Tibble]",
    "id": 22704,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22711": {
    "title": "Analogy: The Bouncer",
    "content": "# üï¥Ô∏è The Bouncer\n\nFiltering joins (`semi_join`) act like a bouncer. They look at a list (second table) and only let people in if they are on the list.\n\n## üéØ Your Task\nKeep flights that went to top destinations.",
    "starter_code": "semi_join(flights, top_dest)",
    "solution_code": "semi_join(flights, top_dest)",
    "expected_output": "[Filtered Tibble]",
    "id": 22711,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22712": {
    "title": "Variation: Anti Join",
    "content": "# ÔøΩÔøΩ Anti Join\n\n`anti_join` keeps rows that are NOT in the second table. Like finding people missing from the list.\n\n## üéØ Your Task\nFind planes not in `planes` table.",
    "starter_code": "anti_join(flights, planes, by = \"tailnum\")",
    "solution_code": "anti_join(flights, planes, by = \"tailnum\")",
    "expected_output": "[Missing Planes]",
    "id": 22712,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22713": {
    "title": "Fix the Code: Columns",
    "content": "# üîß Fix the Code\n\nJoins don't add columns! They filter.\n\n## üéØ Your Task\nVerify `semi_join` output cols.",
    "starter_code": "semi_join(x, y)",
    "solution_code": "semi_join(x, y)",
    "expected_output": "[Tibble]",
    "id": 22713,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "22714": {
    "title": "Challenge: Filter",
    "content": "# ü¶∏ Your Turn\n\nUse `semi_join` to filter `x` by `y`.\n\n## üéØ Your Task\nFilter x.",
    "starter_code": "",
    "solution_code": "semi_join(x, y)",
    "expected_output": "[Tibble]",
    "id": 22714,
    "chapter_title": "Ch 19: Joins",
    "chapter_id": 19
  },
  "23001": {
    "title": "Analogy: Reading Spreadsheets",
    "content": "# üìó Reading Spreadsheets\n\n`read_excel()` reads .xlsx files. It's like opening Excel but in R.\n\n## üéØ Your Task\nRead \"data.xlsx\".",
    "starter_code": "read_excel(\"...\")",
    "solution_code": "read_excel(\"data.xlsx\")",
    "expected_output": "[Tibble]",
    "id": 23001,
    "chapter_title": "Ch 20: Spreadsheets",
    "chapter_id": 20
  },
  "23002": {
    "title": "Variation: Specifying Sheet",
    "content": "# üìë Sheets\n\nExcel files have tabs. Use `sheet = \"SheetName\"`.\n\n## üéØ Your Task\nRead the \"Sales\" sheet.",
    "starter_code": "read_excel(\"data.xlsx\", sheet = ...)",
    "solution_code": "read_excel(\"data.xlsx\", sheet = \"Sales\")",
    "expected_output": "[Tibble]",
    "id": 23002,
    "chapter_title": "Ch 20: Spreadsheets",
    "chapter_id": 20
  },
  "23003": {
    "title": "Fix the Code: Extension",
    "content": "# üîß Fix the Code\n\nFile paths must be exact.\n\n## üéØ Your Task\nFix `.xls` vs `.xlsx`.",
    "starter_code": "read_excel(\"data.xslx\")",
    "solution_code": "read_excel(\"data.xlsx\")",
    "expected_output": "[Tibble]",
    "id": 23003,
    "chapter_title": "Ch 20: Spreadsheets",
    "chapter_id": 20
  },
  "23004": {
    "title": "Challenge: Range",
    "content": "# ü¶∏ Your Turn\n\nRead a specific range `range = \"A1:B10\"`.\n\n## üéØ Your Task\nRead range.",
    "starter_code": "",
    "solution_code": "read_excel(\"data.xlsx\", range = \"A1:B10\")",
    "expected_output": "[Tibble]",
    "id": 23004,
    "chapter_title": "Ch 20: Spreadsheets",
    "chapter_id": 20
  },
  "23101": {
    "title": "Analogy: Remote Control",
    "content": "# üéÆ Remote Control\n\n`dbplyr` lets you control a database (SQL) using R commands. It's like a remote control for a TV.\n\n## üéØ Your Task\nConnect to table \"flights\".",
    "starter_code": "tbl(con, \"...\")",
    "solution_code": "tbl(con, \"flights\")",
    "expected_output": "[Lazy Tibble]",
    "id": 23101,
    "chapter_title": "Ch 21: Databases",
    "chapter_id": 21
  },
  "23102": {
    "title": "Variation: SQL Translation",
    "content": "# üó£Ô∏è Translator\n\nUse `show_query()` to see the SQL R generates.\n\n## üéØ Your Task\nShow query.",
    "starter_code": "flights_db |> show_query()",
    "solution_code": "flights_db |> show_query()",
    "expected_output": "SELECT * FROM ...",
    "id": 23102,
    "chapter_title": "Ch 21: Databases",
    "chapter_id": 21
  },
  "23103": {
    "title": "Fix the Code: Collect",
    "content": "# üîß Fix the Code\n\nData stays on the server until you `collect()` it.\n\n## üéØ Your Task\nCollect data.",
    "starter_code": "flights_db \n# Data is remote",
    "solution_code": "flights_db |> collect()",
    "expected_output": "[Local Tibble]",
    "id": 23103,
    "chapter_title": "Ch 21: Databases",
    "chapter_id": 21
  },
  "23104": {
    "title": "Challenge: Filter DB",
    "content": "# ü¶∏ Your Turn\n\nFilter `year == 2013` on the database.\n\n## üéØ Your Task\nFilter remote.",
    "starter_code": "",
    "solution_code": "flights_db |> filter(year == 2013)",
    "expected_output": "[Lazy Tibble]",
    "id": 23104,
    "chapter_title": "Ch 21: Databases",
    "chapter_id": 21
  },
  "23201": {
    "title": "Analogy: Fast Lane",
    "content": "# üèéÔ∏è Fast Lane\n\nParquet files are super fast compressed data files. Reading them is instant.\n\n## üéØ Your Task\nOpen dataset.",
    "starter_code": "open_dataset(\"data.parquet\")",
    "solution_code": "open_dataset(\"data.parquet\")",
    "expected_output": "[Dataset]",
    "id": 23201,
    "chapter_title": "Ch 22: Arrow",
    "chapter_id": 22
  },
  "23202": {
    "title": "Variation: Write",
    "content": "# üíæ Write\n\nSave as parquet for speed.\n\n## üéØ Your Task\nWrite dataset.",
    "starter_code": "write_dataset(df, \"folder\")",
    "solution_code": "write_dataset(df, \"folder\")",
    "expected_output": "[Files Written]",
    "id": 23202,
    "chapter_title": "Ch 22: Arrow",
    "chapter_id": 22
  },
  "23203": {
    "title": "Fix the Code: Function",
    "content": "# üîß Fix the Code\n\nIt's `open_dataset`, not `read_parquet` (usually).\n\n## üéØ Your Task\nFix function.",
    "starter_code": "open_dataset(\"file\")",
    "solution_code": "open_dataset(\"file\")",
    "expected_output": "[Dataset]",
    "id": 23203,
    "chapter_title": "Ch 22: Arrow",
    "chapter_id": 22
  },
  "23204": {
    "title": "Challenge: Filter",
    "content": "# ü¶∏ Your Turn\n\nFilter before collecting.\n\n## üéØ Your Task\nFilter.",
    "starter_code": "",
    "solution_code": "ds |> filter(x > 0) |> collect()",
    "expected_output": "[Tibble]",
    "id": 23204,
    "chapter_title": "Ch 22: Arrow",
    "chapter_id": 22
  },
  "23301": {
    "title": "Analogy: Unpacking Suitcase",
    "content": "# üß≥ Unpacking Suitcase\n\nList-columns are like suitcases inside your data row. `unnest()` unpacks them so you can see the clothes.\n\n## üéØ Your Task\nUnnest `data` column.",
    "starter_code": "unnest(df, cols = ...)",
    "solution_code": "unnest(df, cols = data)",
    "expected_output": "[Expanded Tibble]",
    "id": 23301,
    "chapter_title": "Ch 23: Hierarchical Data",
    "chapter_id": 23
  },
  "23302": {
    "title": "Variation: Wider",
    "content": "# ‚ÜîÔ∏è Wider\n\n`unnest_wider` puts items into columns.\n\n## üéØ Your Task\nUnnest wider.",
    "starter_code": "unnest_wider(df, metadata)",
    "solution_code": "unnest_wider(df, metadata)",
    "expected_output": "[Wide Tibble]",
    "id": 23302,
    "chapter_title": "Ch 23: Hierarchical Data",
    "chapter_id": 23
  },
  "23303": {
    "title": "Fix the Code: Plural",
    "content": "# üîß Fix the Code\n\nIt's `cols`, plural.\n\n## üéØ Your Task\nFix `col`.",
    "starter_code": "unnest(df, col = data)",
    "solution_code": "unnest(df, cols = data)",
    "expected_output": "[Tibble]",
    "id": 23303,
    "chapter_title": "Ch 23: Hierarchical Data",
    "chapter_id": 23
  },
  "23304": {
    "title": "Challenge: Rectangling",
    "content": "# ü¶∏ Your Turn\n\nRectangling means turning nested lists into a rectangle (table).\n\n## üéØ Your Task\nJust run `unnest()`.",
    "starter_code": "",
    "solution_code": "unnest(df, cols = c(x))",
    "expected_output": "[Tibble]",
    "id": 23304,
    "chapter_title": "Ch 23: Hierarchical Data",
    "chapter_id": 23
  },
  "23401": {
    "title": "Analogy: Mining",
    "content": "# ‚õèÔ∏è Mining\n\nWeb scraping is mining the web for data. `read_html` grabs the site.\n\n## üéØ Your Task\nRead example.com.",
    "starter_code": "read_html(\"http://example.com\")",
    "solution_code": "read_html(\"http://example.com\")",
    "expected_output": "[HTML]",
    "id": 23401,
    "chapter_title": "Ch 24: Web Scraping",
    "chapter_id": 24
  },
  "23402": {
    "title": "Variation: Selectors",
    "content": "# üéØ Selectors\n\nUse CSS selectors to find specific elements (like `.title`).\n\n## üéØ Your Task\nFind `h1`.",
    "starter_code": "html_elements(html, \"...\")",
    "solution_code": "html_elements(html, \"h1\")",
    "expected_output": "[Nodeset]",
    "id": 23402,
    "chapter_title": "Ch 24: Web Scraping",
    "chapter_id": 24
  },
  "23403": {
    "title": "Fix the Code: Text",
    "content": "# üîß Fix the Code\n\nYou need `html_text2()` to get the text out of the tag.\n\n## üéØ Your Task\nGet text.",
    "starter_code": "html_element(html, \"p\")",
    "solution_code": "html_text2(html_element(html, \"p\"))",
    "expected_output": "Paragraph text",
    "id": 23403,
    "chapter_title": "Ch 24: Web Scraping",
    "chapter_id": 24
  },
  "23404": {
    "title": "Challenge: Table",
    "content": "# ü¶∏ Your Turn\n\n`html_table()` turns HTML tables into R Data Frames.\n\n## üéØ Your Task\nParse table.",
    "starter_code": "",
    "solution_code": "html_table(element)",
    "expected_output": "[Tibble]",
    "id": 23404,
    "chapter_title": "Ch 24: Web Scraping",
    "chapter_id": 24
  },
  "24001": {
    "title": "Analogy: Teaching Tricks",
    "content": "# üêï Teaching Tricks\n\nWriting a function is teaching R a new trick. You say \"Sit\" (call function) and it sits.\n\n```r\nsit <- function() { print(\"Sat down\") }\n```\n\n## üéØ Your Task\nWrite a function `say_hi`.",
    "starter_code": "say_hi <- function() { print(\"Hi\") }",
    "solution_code": "say_hi <- function() { print(\"Hi\") }",
    "expected_output": "\"Hi\"",
    "id": 24001,
    "chapter_title": "Ch 25: Functions",
    "chapter_id": 25
  },
  "24002": {
    "title": "Variation: Arguments",
    "content": "# ü¶¥ Arguments\n\nGive the dog a bone. Arguments passed to function.\n\n## üéØ Your Task\nFunction taking `name`.",
    "starter_code": "greet <- function(name) { ... }",
    "solution_code": "greet <- function(name) { paste(\"Hi\", name) }",
    "expected_output": "\"Hi Name\"",
    "id": 24002,
    "chapter_title": "Ch 25: Functions",
    "chapter_id": 25
  },
  "24003": {
    "title": "Fix the Code: Braces",
    "content": "# üîß Fix the Code\n\nCode body goes in `{}`.\n\n## üéØ Your Task\nAdd braces.",
    "starter_code": "f <- function() print(\"hi\")",
    "solution_code": "f <- function() { print(\"hi\") }",
    "expected_output": "\"hi\"",
    "id": 24003,
    "chapter_title": "Ch 25: Functions",
    "chapter_id": 25
  },
  "24004": {
    "title": "Challenge: Return",
    "content": "# ü¶∏ Your Turn\n\nFunctions return the last value. Write `add_one(x)`.\n\n## üéØ Your Task\nWrite function.",
    "starter_code": "",
    "solution_code": "add_one <- function(x) { x + 1 }",
    "expected_output": "2",
    "id": 24004,
    "chapter_title": "Ch 25: Functions",
    "chapter_id": 25
  },
  "24101": {
    "title": "Analogy: Assembly Line",
    "content": "# üè≠ Assembly Line\n\n`map()` applies a function to every item in a list, like a robot on an assembly line painting every car.\n\n## üéØ Your Task\nMap `mean` to list.",
    "starter_code": "map(list(a=1:10, b=10:20), mean)",
    "solution_code": "map(list(a=1:10, b=10:20), mean)",
    "expected_output": "[List of means]",
    "id": 24101,
    "chapter_title": "Ch 26: Iteration",
    "chapter_id": 26
  },
  "24102": {
    "title": "Variation: Double",
    "content": "# 2Ô∏è‚É£ Double\n\n`map_dbl` returns a number vector instead of a list.\n\n## ÔøΩÔøΩ Your Task\nUse `map_dbl`.",
    "starter_code": "map_dbl(x, mean)",
    "solution_code": "map_dbl(x, mean)",
    "expected_output": "[Vector]",
    "id": 24102,
    "chapter_title": "Ch 26: Iteration",
    "chapter_id": 26
  },
  "24103": {
    "title": "Fix the Code: Argument",
    "content": "# üîß Fix the Code\n\nPass the function, don't call it `()`.\n\n## üéØ Your Task\nRemove `()`.",
    "starter_code": "map(x, mean())",
    "solution_code": "map(x, mean)",
    "expected_output": "[List]",
    "id": 24103,
    "chapter_title": "Ch 26: Iteration",
    "chapter_id": 26
  },
  "24104": {
    "title": "Challenge: Walk",
    "content": "# ü¶∏ Your Turn\n\n`walk()` is for side effects (like saving files).\n\n## üéØ Your Task\nWalk print.",
    "starter_code": "",
    "solution_code": "walk(x, print)",
    "expected_output": "[Prints]",
    "id": 24104,
    "chapter_title": "Ch 26: Iteration",
    "chapter_id": 26
  },
  "24201": {
    "title": "Analogy: Pepper Shaker",
    "content": "# üßÇ Pepper Shaker\n\nBase R subsetting `$` is like shaking the pepper out. `[[ ]]` acts similarly.\n\n## üéØ Your Task\nGet `mpg` column from `mtcars` using `$`.",
    "starter_code": "mtcars$...",
    "solution_code": "mtcars$mpg",
    "expected_output": "[Vector]",
    "id": 24201,
    "chapter_title": "Ch 27: Base R",
    "chapter_id": 27
  },
  "24202": {
    "title": "Variation: Brackets",
    "content": "# üì¶ Brackets\n\n`[ row, col ]` subsets like coordinates.\n\n## üéØ Your Task\nGet row 1, column 1.",
    "starter_code": "mtcars[1, 1]",
    "solution_code": "mtcars[1, 1]",
    "expected_output": "21",
    "id": 24202,
    "chapter_title": "Ch 27: Base R",
    "chapter_id": 27
  },
  "24203": {
    "title": "Fix the Code: Comma",
    "content": "# üîß Fix the Code\n\n`[1, ]` gets row 1, all columns.\n\n## üéØ Your Task\nGet row 1.",
    "starter_code": "mtcars[1]",
    "solution_code": "mtcars[1, ]",
    "expected_output": "[Data Frame]",
    "id": 24203,
    "chapter_title": "Ch 27: Base R",
    "chapter_id": 27
  },
  "24204": {
    "title": "Challenge: Loop",
    "content": "# ü¶∏ Your Turn\n\nWrite a for loop `for (i in 1:5)`.\n\n## üéØ Your Task\nPrint i.",
    "starter_code": "",
    "solution_code": "for (i in 1:5) print(i)",
    "expected_output": "1 2 3 4 5",
    "id": 24204,
    "chapter_title": "Ch 27: Base R",
    "chapter_id": 27
  },
  "25001": {
    "title": "Analogy: Notebook",
    "content": "# üìì Notebook\n\nQuarto is a lab notebook. It has code AND notes.\n\n## ÔøΩÔøΩ Your Task\nWrite text.",
    "starter_code": "# Just text",
    "solution_code": "Text",
    "expected_output": "Text",
    "id": 25001,
    "chapter_title": "Ch 28: Quarto",
    "chapter_id": 28
  },
  "25002": {
    "title": "Variation: Code Chunk",
    "content": "# üß© Code Chunk\n\nChunks run code. ` ```{r} ` starts one.\n\n## üéØ Your Task\nRun 1+1.",
    "starter_code": "1+1",
    "solution_code": "1+1",
    "expected_output": "2",
    "id": 25002,
    "chapter_title": "Ch 28: Quarto",
    "chapter_id": 28
  },
  "25003": {
    "title": "Fix the Code: Backticks",
    "content": "# üîß Fix the Code\n\nNeed 3 backticks.\n\n## üéØ Your Task\nFix chunk.",
    "starter_code": "``{r}",
    "solution_code": "```{r}",
    "expected_output": "[Chunk]",
    "id": 25003,
    "chapter_title": "Ch 28: Quarto",
    "chapter_id": 28
  },
  "25004": {
    "title": "Challenge: YAML",
    "content": "# ü¶∏ Your Turn\n\nWrite a YAML header title.\n\n## üéØ Your Task\nTitle.",
    "starter_code": "",
    "solution_code": "---\ntitle: \"Hi\"\n---",
    "expected_output": "Title: Hi",
    "id": 25004,
    "chapter_title": "Ch 28: Quarto",
    "chapter_id": 28
  },
  "25101": {
    "title": "Analogy: Transformer",
    "content": "# ü§ñ Transformer\n\nQuarto transforms into HTML, PDF, or Word just by changing one word in the YAML.\n\n## üéØ Your Task\nSet format to html.",
    "starter_code": "format: ...",
    "solution_code": "format: html",
    "expected_output": "HTML",
    "id": 25101,
    "chapter_title": "Ch 29: Quarto Formats",
    "chapter_id": 29
  },
  "25102": {
    "title": "Variation: PDF",
    "content": "# üìÑ PDF\n\nChange to `pdf`.\n\n## üéØ Your Task\nSet format to pdf.",
    "starter_code": "format: pdf",
    "solution_code": "format: pdf",
    "expected_output": "PDF",
    "id": 25102,
    "chapter_title": "Ch 29: Quarto Formats",
    "chapter_id": 29
  },
  "25103": {
    "title": "Fix the Code: Indent",
    "content": "# üîß Fix the Code\n\nYAML cares about indentation.\n\n## üéØ Your Task\nIndent `toc: true`.",
    "starter_code": "format:\nhtml:\ntoc: true",
    "solution_code": "format:\n  html:\n    toc: true",
    "expected_output": "TOC",
    "id": 25103,
    "chapter_title": "Ch 29: Quarto Formats",
    "chapter_id": 29
  },
  "25104": {
    "title": "Challenge: Presentation",
    "content": "# ü¶∏ Your Turn\n\nUse `revealjs` for slides.\n\n## üéØ Your Task\nSlides.",
    "starter_code": "",
    "solution_code": "format: revealjs",
    "expected_output": "Slides",
    "id": 25104,
    "chapter_title": "Ch 29: Quarto Formats",
    "chapter_id": 29
  }
}